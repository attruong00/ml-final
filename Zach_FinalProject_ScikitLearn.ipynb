{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/attruong00/ml-final/blob/main/Zach_FinalProject_ScikitLearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4zhagsD7f4h"
      },
      "outputs": [],
      "source": [
        "# Enter your name(s) here\n",
        "# Abbey Truong att837\n",
        "# Ben Truong bst574\n",
        "# Kamil Kalowski ktk582\n",
        "# Zach Cramer "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJQlHAyV7f4j"
      },
      "source": [
        "# Assignment 4 : Using `scikit-learn`\n",
        "\n",
        "Scikit-learn provides a range of supervised and unsupervised learning algorithms via a consistent interface in Python. In this assigment you'll explore how to train various classifiers using the `scikit-learn` library. The scikit-learn documentation can be found [here](http://scikit-learn.org/stable/documentation.html).\n",
        "\n",
        "In this assignment we'll attempt to classify patients as either having or not having diabetic retinopathy, using the same Diabetic Retinopathy data set from your previous assignments. Recall that this dataset contains 1151 records and 20 attributes (some categorical, some continuous). You can find additional details about the dataset [here](http://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NCCCd4gv7f4l"
      },
      "outputs": [],
      "source": [
        "#You may add additional imports\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "import pandas as pd\n",
        "import statistics\n",
        "import numpy as np\n",
        "import sklearn as sk\n",
        "from sklearn import model_selection, tree, metrics, naive_bayes, preprocessing, decomposition, neighbors, pipeline, svm\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_X-z_l1n7f4l"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ivlcqTtq7f4l",
        "outputId": "697805d0-3e66-4220-8614-c9d7f447823b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Bankrupt?   ROA(C) before interest and depreciation before interest  \\\n",
              "0           1                                           0.370594          \n",
              "1           1                                           0.464291          \n",
              "2           1                                           0.426071          \n",
              "3           1                                           0.399844          \n",
              "4           1                                           0.465022          \n",
              "5           1                                           0.388680          \n",
              "6           0                                           0.390923          \n",
              "7           0                                           0.508361          \n",
              "8           0                                           0.488519          \n",
              "9           0                                           0.495686          \n",
              "10          0                                           0.482475          \n",
              "11          0                                           0.444401          \n",
              "12          0                                           0.491152          \n",
              "13          0                                           0.474041          \n",
              "14          0                                           0.506703          \n",
              "15          0                                           0.513821          \n",
              "16          0                                           0.488909          \n",
              "17          0                                           0.535953          \n",
              "18          0                                           0.504071          \n",
              "19          0                                           0.487398          \n",
              "20          0                                           0.485253          \n",
              "21          0                                           0.504558          \n",
              "22          0                                           0.512017          \n",
              "23          0                                           0.494857          \n",
              "24          0                                           0.509969          \n",
              "25          0                                           0.444986          \n",
              "26          0                                           0.519280          \n",
              "27          0                                           0.547409          \n",
              "28          0                                           0.500853          \n",
              "29          1                                           0.416126          \n",
              "30          1                                           0.462195          \n",
              "31          0                                           0.476088          \n",
              "32          0                                           0.505874          \n",
              "33          0                                           0.486374          \n",
              "34          0                                           0.494369          \n",
              "35          0                                           0.475162          \n",
              "36          0                                           0.462926          \n",
              "37          0                                           0.478916          \n",
              "38          0                                           0.481061          \n",
              "39          0                                           0.486228          \n",
              "40          0                                           0.507824          \n",
              "41          0                                           0.489153          \n",
              "42          0                                           0.511188          \n",
              "43          0                                           0.523034          \n",
              "44          0                                           0.508848          \n",
              "45          0                                           0.488958          \n",
              "46          0                                           0.502121          \n",
              "47          0                                           0.523083          \n",
              "48          0                                           0.493346          \n",
              "49          0                                           0.505631          \n",
              "50          0                                           0.469556          \n",
              "51          0                                           0.523717          \n",
              "52          0                                           0.521913          \n",
              "53          0                                           0.503193          \n",
              "54          1                                           0.453030          \n",
              "55          1                                           0.472091          \n",
              "56          1                                           0.066933          \n",
              "57          1                                           0.406669          \n",
              "58          0                                           0.482231          \n",
              "59          0                                           0.488519          \n",
              "\n",
              "     ROA(A) before interest and % after tax  \\\n",
              "0                                  0.424389   \n",
              "1                                  0.538214   \n",
              "2                                  0.499019   \n",
              "3                                  0.451265   \n",
              "4                                  0.538432   \n",
              "5                                  0.415177   \n",
              "6                                  0.445704   \n",
              "7                                  0.570922   \n",
              "8                                  0.545137   \n",
              "9                                  0.550916   \n",
              "10                                 0.567543   \n",
              "11                                 0.549717   \n",
              "12                                 0.551570   \n",
              "13                                 0.533308   \n",
              "14                                 0.575829   \n",
              "15                                 0.571086   \n",
              "16                                 0.560238   \n",
              "17                                 0.590438   \n",
              "18                                 0.559802   \n",
              "19                                 0.543720   \n",
              "20                                 0.545573   \n",
              "21                                 0.564490   \n",
              "22                                 0.563672   \n",
              "23                                 0.548136   \n",
              "24                                 0.561382   \n",
              "25                                 0.503652   \n",
              "26                                 0.563618   \n",
              "27                                 0.593055   \n",
              "28                                 0.563945   \n",
              "29                                 0.470235   \n",
              "30                                 0.536034   \n",
              "31                                 0.544483   \n",
              "32                                 0.570704   \n",
              "33                                 0.544756   \n",
              "34                                 0.550916   \n",
              "35                                 0.564544   \n",
              "36                                 0.516354   \n",
              "37                                 0.541321   \n",
              "38                                 0.539468   \n",
              "39                                 0.548027   \n",
              "40                                 0.571631   \n",
              "41                                 0.562691   \n",
              "42                                 0.566888   \n",
              "43                                 0.575883   \n",
              "44                                 0.553423   \n",
              "45                                 0.549008   \n",
              "46                                 0.563509   \n",
              "47                                 0.563345   \n",
              "48                                 0.550534   \n",
              "49                                 0.559747   \n",
              "50                                 0.527202   \n",
              "51                                 0.561546   \n",
              "52                                 0.564871   \n",
              "53                                 0.559693   \n",
              "54                                 0.516572   \n",
              "55                                 0.538814   \n",
              "56                                 0.057185   \n",
              "57                                 0.467292   \n",
              "58                                 0.544320   \n",
              "59                                 0.550153   \n",
              "\n",
              "     ROA(B) before interest and depreciation after tax  \\\n",
              "0                                            0.405750    \n",
              "1                                            0.516730    \n",
              "2                                            0.472295    \n",
              "3                                            0.457733    \n",
              "4                                            0.522298    \n",
              "5                                            0.419134    \n",
              "6                                            0.436158    \n",
              "7                                            0.559077    \n",
              "8                                            0.543284    \n",
              "9                                            0.542963    \n",
              "10                                           0.538198    \n",
              "11                                           0.498956    \n",
              "12                                           0.543391    \n",
              "13                                           0.523690    \n",
              "14                                           0.569838    \n",
              "15                                           0.558756    \n",
              "16                                           0.540286    \n",
              "17                                           0.580920    \n",
              "18                                           0.558649    \n",
              "19                                           0.533647    \n",
              "20                                           0.534665    \n",
              "21                                           0.553027    \n",
              "22                                           0.569035    \n",
              "23                                           0.540446    \n",
              "24                                           0.552599    \n",
              "25                                           0.495530    \n",
              "26                                           0.568606    \n",
              "27                                           0.592484    \n",
              "28                                           0.550565    \n",
              "29                                           0.463783    \n",
              "30                                           0.514428    \n",
              "31                                           0.529686    \n",
              "32                                           0.559827    \n",
              "33                                           0.540500    \n",
              "34                                           0.548584    \n",
              "35                                           0.525028    \n",
              "36                                           0.517105    \n",
              "37                                           0.529311    \n",
              "38                                           0.530864    \n",
              "39                                           0.544140    \n",
              "40                                           0.549226    \n",
              "41                                           0.541517    \n",
              "42                                           0.559077    \n",
              "43                                           0.576423    \n",
              "44                                           0.552920    \n",
              "45                                           0.538947    \n",
              "46                                           0.545854    \n",
              "47                                           0.572354    \n",
              "48                                           0.539804    \n",
              "49                                           0.549173    \n",
              "50                                           0.522512    \n",
              "51                                           0.568178    \n",
              "52                                           0.568178    \n",
              "53                                           0.550672    \n",
              "54                                           0.505595    \n",
              "55                                           0.517051    \n",
              "56                                           0.054821    \n",
              "57                                           0.453450    \n",
              "58                                           0.536431    \n",
              "59                                           0.540339    \n",
              "\n",
              "     Operating Gross Margin   Realized Sales Gross Margin  \\\n",
              "0                  0.601457                      0.601457   \n",
              "1                  0.610235                      0.610235   \n",
              "2                  0.601450                      0.601364   \n",
              "3                  0.583541                      0.583541   \n",
              "4                  0.598783                      0.598783   \n",
              "5                  0.590171                      0.590251   \n",
              "6                  0.619950                      0.619950   \n",
              "7                  0.601738                      0.601717   \n",
              "8                  0.603612                      0.603612   \n",
              "9                  0.599209                      0.599209   \n",
              "10                 0.614026                      0.614026   \n",
              "11                 0.623712                      0.623712   \n",
              "12                 0.608131                      0.608138   \n",
              "13                 0.600578                      0.600578   \n",
              "14                 0.604686                      0.604686   \n",
              "15                 0.621773                      0.621773   \n",
              "16                 0.606524                      0.606524   \n",
              "17                 0.618451                      0.618451   \n",
              "18                 0.598344                      0.598344   \n",
              "19                 0.636259                      0.636252   \n",
              "20                 0.622177                      0.622256   \n",
              "21                 0.607446                      0.607446   \n",
              "22                 0.618934                      0.618934   \n",
              "23                 0.609507                      0.609507   \n",
              "24                 0.602956                      0.602956   \n",
              "25                 0.611763                      0.611806   \n",
              "26                 0.626544                      0.626544   \n",
              "27                 0.609738                      0.609738   \n",
              "28                 0.602027                      0.602293   \n",
              "29                 0.599115                      0.599115   \n",
              "30                 0.599987                      0.599987   \n",
              "31                 0.618869                      0.618869   \n",
              "32                 0.601169                      0.601176   \n",
              "33                 0.603475                      0.603475   \n",
              "34                 0.599108                      0.599108   \n",
              "35                 0.614278                      0.614278   \n",
              "36                 0.622703                      0.622530   \n",
              "37                 0.607042                      0.607006   \n",
              "38                 0.600881                      0.600881   \n",
              "39                 0.598445                      0.598445   \n",
              "40                 0.625326                      0.625326   \n",
              "41                 0.604412                      0.604412   \n",
              "42                 0.618177                      0.618177   \n",
              "43                 0.600347                      0.600347   \n",
              "44                 0.635718                      0.635718   \n",
              "45                 0.624317                      0.624288   \n",
              "46                 0.608866                      0.608866   \n",
              "47                 0.620944                      0.620944   \n",
              "48                 0.610206                      0.610206   \n",
              "49                 0.604563                      0.604563   \n",
              "50                 0.619402                      0.619525   \n",
              "51                 0.626414                      0.626414   \n",
              "52                 0.609291                      0.609241   \n",
              "53                 0.603590                      0.603072   \n",
              "54                 0.596946                      0.596953   \n",
              "55                 0.597393                      0.597393   \n",
              "56                 0.601861                      0.601861   \n",
              "57                 0.595994                      0.595994   \n",
              "58                 0.618083                      0.618083   \n",
              "59                 0.600737                      0.600758   \n",
              "\n",
              "     Operating Profit Rate   Pre-tax net Interest Rate  \\\n",
              "0                 0.998969                    0.796887   \n",
              "1                 0.998946                    0.797380   \n",
              "2                 0.998857                    0.796403   \n",
              "3                 0.998700                    0.796967   \n",
              "4                 0.998973                    0.797366   \n",
              "5                 0.998758                    0.796903   \n",
              "6                 0.998993                    0.797012   \n",
              "7                 0.999009                    0.797449   \n",
              "8                 0.998961                    0.797414   \n",
              "9                 0.999001                    0.797404   \n",
              "10                0.998978                    0.797535   \n",
              "11                0.998975                    0.797443   \n",
              "12                0.999045                    0.797429   \n",
              "13                0.998967                    0.797368   \n",
              "14                0.999053                    0.797514   \n",
              "15                0.999097                    0.797507   \n",
              "16                0.998996                    0.797877   \n",
              "17                0.999119                    0.797588   \n",
              "18                0.998989                    0.797412   \n",
              "19                0.999042                    0.797444   \n",
              "20                0.999065                    0.797522   \n",
              "21                0.999030                    0.797466   \n",
              "22                0.999059                    0.797444   \n",
              "23                0.999052                    0.797413   \n",
              "24                0.999025                    0.797417   \n",
              "25                0.998925                    0.797046   \n",
              "26                0.999099                    0.797512   \n",
              "27                0.999112                    0.797571   \n",
              "28                0.998982                    0.797503   \n",
              "29                0.998973                    0.797241   \n",
              "30                0.998909                    0.797291   \n",
              "31                0.999014                    0.797380   \n",
              "32                0.999000                    0.797440   \n",
              "33                0.998944                    0.797415   \n",
              "34                0.998995                    0.797393   \n",
              "35                0.998973                    0.797520   \n",
              "36                0.998997                    0.797245   \n",
              "37                0.999017                    0.797404   \n",
              "38                0.998975                    0.797388   \n",
              "39                0.998953                    0.797404   \n",
              "40                0.999136                    0.797542   \n",
              "41                0.998971                    0.797896   \n",
              "42                0.999077                    0.797522   \n",
              "43                0.999027                    0.797447   \n",
              "44                0.999062                    0.797481   \n",
              "45                0.999098                    0.797547   \n",
              "46                0.999048                    0.797465   \n",
              "47                0.999102                    0.797475   \n",
              "48                0.999023                    0.797429   \n",
              "49                0.999031                    0.797415   \n",
              "50                0.999045                    0.797270   \n",
              "51                0.999133                    0.797526   \n",
              "52                0.999098                    0.797498   \n",
              "53                0.999001                    0.797513   \n",
              "54                0.998940                    0.797084   \n",
              "55                0.998960                    0.797366   \n",
              "56                0.998825                    0.796779   \n",
              "57                0.998831                    0.797191   \n",
              "58                0.999004                    0.797389   \n",
              "59                0.998997                    0.797409   \n",
              "\n",
              "     After-tax net Interest Rate  \\\n",
              "0                       0.808809   \n",
              "1                       0.809301   \n",
              "2                       0.808388   \n",
              "3                       0.808966   \n",
              "4                       0.809304   \n",
              "5                       0.808771   \n",
              "6                       0.808960   \n",
              "7                       0.809362   \n",
              "8                       0.809338   \n",
              "9                       0.809320   \n",
              "10                      0.809460   \n",
              "11                      0.809389   \n",
              "12                      0.809344   \n",
              "13                      0.809299   \n",
              "14                      0.809456   \n",
              "15                      0.809396   \n",
              "16                      0.809761   \n",
              "17                      0.809461   \n",
              "18                      0.809334   \n",
              "19                      0.809340   \n",
              "20                      0.809406   \n",
              "21                      0.809371   \n",
              "22                      0.809367   \n",
              "23                      0.809323   \n",
              "24                      0.809331   \n",
              "25                      0.808993   \n",
              "26                      0.809403   \n",
              "27                      0.809449   \n",
              "28                      0.809400   \n",
              "29                      0.809176   \n",
              "30                      0.809223   \n",
              "31                      0.809307   \n",
              "32                      0.809359   \n",
              "33                      0.809338   \n",
              "34                      0.809318   \n",
              "35                      0.809444   \n",
              "36                      0.809190   \n",
              "37                      0.809317   \n",
              "38                      0.809316   \n",
              "39                      0.809343   \n",
              "40                      0.809413   \n",
              "41                      0.809799   \n",
              "42                      0.809409   \n",
              "43                      0.809362   \n",
              "44                      0.809365   \n",
              "45                      0.809430   \n",
              "46                      0.809364   \n",
              "47                      0.809371   \n",
              "48                      0.809341   \n",
              "49                      0.809332   \n",
              "50                      0.809203   \n",
              "51                      0.809396   \n",
              "52                      0.809387   \n",
              "53                      0.809398   \n",
              "54                      0.809040   \n",
              "55                      0.809290   \n",
              "56                      0.808717   \n",
              "57                      0.809128   \n",
              "58                      0.809315   \n",
              "59                      0.809330   \n",
              "\n",
              "     Non-industry income and expenditure/revenue  ...  \\\n",
              "0                                       0.302646  ...   \n",
              "1                                       0.303556  ...   \n",
              "2                                       0.302035  ...   \n",
              "3                                       0.303350  ...   \n",
              "4                                       0.303475  ...   \n",
              "5                                       0.303116  ...   \n",
              "6                                       0.302814  ...   \n",
              "7                                       0.303545  ...   \n",
              "8                                       0.303584  ...   \n",
              "9                                       0.303483  ...   \n",
              "10                                      0.303759  ...   \n",
              "11                                      0.303605  ...   \n",
              "12                                      0.303435  ...   \n",
              "13                                      0.303492  ...   \n",
              "14                                      0.303566  ...   \n",
              "15                                      0.303462  ...   \n",
              "16                                      0.304319  ...   \n",
              "17                                      0.303559  ...   \n",
              "18                                      0.303523  ...   \n",
              "19                                      0.303467  ...   \n",
              "20                                      0.303554  ...   \n",
              "21                                      0.303531  ...   \n",
              "22                                      0.303433  ...   \n",
              "23                                      0.303392  ...   \n",
              "24                                      0.303456  ...   \n",
              "25                                      0.303017  ...   \n",
              "26                                      0.303466  ...   \n",
              "27                                      0.303543  ...   \n",
              "28                                      0.303695  ...   \n",
              "29                                      0.303258  ...   \n",
              "30                                      0.303477  ...   \n",
              "31                                      0.303415  ...   \n",
              "32                                      0.303547  ...   \n",
              "33                                      0.303622  ...   \n",
              "34                                      0.303475  ...   \n",
              "35                                      0.303743  ...   \n",
              "36                                      0.303212  ...   \n",
              "37                                      0.303449  ...   \n",
              "38                                      0.303508  ...   \n",
              "39                                      0.303584  ...   \n",
              "40                                      0.303442  ...   \n",
              "41                                      0.304406  ...   \n",
              "42                                      0.303529  ...   \n",
              "43                                      0.303503  ...   \n",
              "44                                      0.303490  ...   \n",
              "45                                      0.303529  ...   \n",
              "46                                      0.303491  ...   \n",
              "47                                      0.303396  ...   \n",
              "48                                      0.303480  ...   \n",
              "49                                      0.303439  ...   \n",
              "50                                      0.303157  ...   \n",
              "51                                      0.303420  ...   \n",
              "52                                      0.303445  ...   \n",
              "53                                      0.303672  ...   \n",
              "54                                      0.303052  ...   \n",
              "55                                      0.303501  ...   \n",
              "56                                      0.302760  ...   \n",
              "57                                      0.303466  ...   \n",
              "58                                      0.303451  ...   \n",
              "59                                      0.303502  ...   \n",
              "\n",
              "     Net Income to Total Assets   Total assets to GNP price  \\\n",
              "0                      0.716845                    0.009219   \n",
              "1                      0.795297                    0.008323   \n",
              "2                      0.774670                    0.040003   \n",
              "3                      0.739555                    0.003252   \n",
              "4                      0.795016                    0.003878   \n",
              "5                      0.710420                    0.005278   \n",
              "6                      0.736619                    0.018372   \n",
              "7                      0.815350                    0.010005   \n",
              "8                      0.803647                    0.000824   \n",
              "9                      0.804195                    0.005798   \n",
              "10                     0.814111                    0.076972   \n",
              "11                     0.804887                    0.007318   \n",
              "12                     0.803260                    0.008232   \n",
              "13                     0.794158                    0.005262   \n",
              "14                     0.819715                    0.003954   \n",
              "15                     0.815419                    0.006590   \n",
              "16                     0.810421                    0.013611   \n",
              "17                     0.826642                    0.002746   \n",
              "18                     0.806264                    0.004238   \n",
              "19                     0.802552                    0.001521   \n",
              "20                     0.804639                    0.014039   \n",
              "21                     0.814012                    0.000935   \n",
              "22                     0.808976                    0.001830   \n",
              "23                     0.798104                    0.018753   \n",
              "24                     0.803445                    0.000830   \n",
              "25                     0.772909                    0.005887   \n",
              "26                     0.812974                    0.001193   \n",
              "27                     0.830176                    0.000703   \n",
              "28                     0.812100                    0.001225   \n",
              "29                     0.750431                    0.003011   \n",
              "30                     0.787082                    0.001631   \n",
              "31                     0.796023                    0.015252   \n",
              "32                     0.814102                    0.009774   \n",
              "33                     0.802923                    0.000785   \n",
              "34                     0.803066                    0.005124   \n",
              "35                     0.811257                    0.080006   \n",
              "36                     0.784385                    0.007563   \n",
              "37                     0.797744                    0.007308   \n",
              "38                     0.798174                    0.004593   \n",
              "39                     0.801372                    0.003425   \n",
              "40                     0.815731                    0.007173   \n",
              "41                     0.811426                    0.013698   \n",
              "42                     0.813568                    0.002623   \n",
              "43                     0.816257                    0.004076   \n",
              "44                     0.807573                    0.001521   \n",
              "45                     0.806667                    0.012883   \n",
              "46                     0.813580                    0.000887   \n",
              "47                     0.809900                    0.001675   \n",
              "48                     0.799842                    0.018859   \n",
              "49                     0.802638                    0.000717   \n",
              "50                     0.788126                    0.005376   \n",
              "51                     0.811329                    0.001088   \n",
              "52                     0.812826                    0.000729   \n",
              "53                     0.809661                    0.001210   \n",
              "54                     0.777094                    0.040161   \n",
              "55                     0.791982                    0.007233   \n",
              "56                     0.525651                    0.005803   \n",
              "57                     0.746586                    0.001053   \n",
              "58                     0.797585                    0.014207   \n",
              "59                     0.804706                    0.011073   \n",
              "\n",
              "     No-credit Interval   Gross Profit to Sales  \\\n",
              "0              0.622879                0.601453   \n",
              "1              0.623652                0.610237   \n",
              "2              0.623841                0.601449   \n",
              "3              0.622929                0.583538   \n",
              "4              0.623521                0.598782   \n",
              "5              0.622605                0.590172   \n",
              "6              0.623655                0.619949   \n",
              "7              0.623843                0.601739   \n",
              "8              0.623977                0.603613   \n",
              "9              0.623865                0.599205   \n",
              "10             0.623687                0.614021   \n",
              "11             0.623724                0.623709   \n",
              "12             0.623578                0.608125   \n",
              "13             0.623777                0.600578   \n",
              "14             0.624946                0.604683   \n",
              "15             0.623764                0.621769   \n",
              "16             0.623325                0.606524   \n",
              "17             0.623825                0.618452   \n",
              "18             0.623339                0.598345   \n",
              "19             0.623763                0.636256   \n",
              "20             0.623922                0.622177   \n",
              "21             0.623996                0.607441   \n",
              "22             0.623718                0.618933   \n",
              "23             0.623486                0.609508   \n",
              "24             0.623482                0.602952   \n",
              "25             0.623655                0.611761   \n",
              "26             0.623712                0.626543   \n",
              "27             0.624308                0.609733   \n",
              "28             0.623910                0.602027   \n",
              "29             0.623183                0.599110   \n",
              "30             0.621876                0.599984   \n",
              "31             0.623562                0.618864   \n",
              "32             0.623810                0.601169   \n",
              "33             0.623908                0.603474   \n",
              "34             0.623898                0.599107   \n",
              "35             0.623709                0.614277   \n",
              "36             0.623631                0.622698   \n",
              "37             0.623731                0.607042   \n",
              "38             0.623962                0.600876   \n",
              "39             0.623089                0.598440   \n",
              "40             0.623729                0.625327   \n",
              "41             0.622974                0.604410   \n",
              "42             0.623780                0.618178   \n",
              "43             0.622494                0.600343   \n",
              "44             0.623752                0.635718   \n",
              "45             0.623855                0.624318   \n",
              "46             0.624017                0.608864   \n",
              "47             0.623711                0.620941   \n",
              "48             0.623536                0.610202   \n",
              "49             0.623489                0.604564   \n",
              "50             0.623540                0.619403   \n",
              "51             0.623689                0.626413   \n",
              "52             0.624307                0.609288   \n",
              "53             0.623755                0.603586   \n",
              "54             0.617349                0.596946   \n",
              "55             0.622323                0.597394   \n",
              "56             0.623648                0.601857   \n",
              "57             0.623323                0.595994   \n",
              "58             0.623644                0.618084   \n",
              "59             0.623561                0.600737   \n",
              "\n",
              "     Net Income to Stockholder's Equity   Liability to Equity  \\\n",
              "0                              0.827890              0.290202   \n",
              "1                              0.839969              0.283846   \n",
              "2                              0.836774              0.290189   \n",
              "3                              0.834697              0.281721   \n",
              "4                              0.839973              0.278514   \n",
              "5                              0.829939              0.285087   \n",
              "6                              0.829980              0.292504   \n",
              "7                              0.841459              0.278607   \n",
              "8                              0.840487              0.276423   \n",
              "9                              0.840688              0.279388   \n",
              "10                             0.841337              0.278356   \n",
              "11                             0.840650              0.277892   \n",
              "12                             0.840702              0.281113   \n",
              "13                             0.839910              0.278518   \n",
              "14                             0.841624              0.277668   \n",
              "15                             0.841399              0.278124   \n",
              "16                             0.840889              0.276518   \n",
              "17                             0.841983              0.277190   \n",
              "18                             0.840955              0.280839   \n",
              "19                             0.840456              0.277139   \n",
              "20                             0.840639              0.277988   \n",
              "21                             0.841311              0.278208   \n",
              "22                             0.841234              0.281274   \n",
              "23                             0.840326              0.287134   \n",
              "24                             0.841047              0.287406   \n",
              "25                             0.837926              0.281295   \n",
              "26                             0.841188              0.277781   \n",
              "27                             0.841854              0.275704   \n",
              "28                             0.841116              0.277664   \n",
              "29                             0.831976              0.293599   \n",
              "30                             0.838259              0.297038   \n",
              "31                             0.840052              0.290365   \n",
              "32                             0.841432              0.279111   \n",
              "33                             0.840451              0.276555   \n",
              "34                             0.840609              0.279572   \n",
              "35                             0.841252              0.279474   \n",
              "36                             0.839029              0.280673   \n",
              "37                             0.840206              0.281335   \n",
              "38                             0.840193              0.277940   \n",
              "39                             0.840386              0.277247   \n",
              "40                             0.841574              0.279236   \n",
              "41                             0.840979              0.276803   \n",
              "42                             0.841197              0.277532   \n",
              "43                             0.841802              0.280569   \n",
              "44                             0.840820              0.277738   \n",
              "45                             0.840771              0.277885   \n",
              "46                             0.841275              0.278154   \n",
              "47                             0.841282              0.280888   \n",
              "48                             0.840599              0.288724   \n",
              "49                             0.840860              0.285724   \n",
              "50                             0.839285              0.281982   \n",
              "51                             0.841075              0.277759   \n",
              "52                             0.840992              0.276193   \n",
              "53                             0.840973              0.277869   \n",
              "54                             0.837053              0.290904   \n",
              "55                             0.839162              0.300005   \n",
              "56                             1.000000              0.182790   \n",
              "57                             0.831082              0.294222   \n",
              "58                             0.840280              0.289105   \n",
              "59                             0.840818              0.280855   \n",
              "\n",
              "     Degree of Financial Leverage (DFL)  \\\n",
              "0                              0.026601   \n",
              "1                              0.264577   \n",
              "2                              0.026555   \n",
              "3                              0.026697   \n",
              "4                              0.024752   \n",
              "5                              0.026675   \n",
              "6                              0.026622   \n",
              "7                              0.027031   \n",
              "8                              0.026891   \n",
              "9                              0.027243   \n",
              "10                             0.026971   \n",
              "11                             0.027391   \n",
              "12                             0.027480   \n",
              "13                             0.025000   \n",
              "14                             0.026896   \n",
              "15                             0.026897   \n",
              "16                             0.027003   \n",
              "17                             0.026908   \n",
              "18                             0.027637   \n",
              "19                             0.026924   \n",
              "20                             0.026794   \n",
              "21                             0.026890   \n",
              "22                             0.027565   \n",
              "23                             0.029364   \n",
              "24                             0.028206   \n",
              "25                             0.026415   \n",
              "26                             0.026924   \n",
              "27                             0.026852   \n",
              "28                             0.027067   \n",
              "29                             0.026472   \n",
              "30                             0.024886   \n",
              "31                             0.174091   \n",
              "32                             0.027119   \n",
              "33                             0.027003   \n",
              "34                             0.027677   \n",
              "35                             0.027115   \n",
              "36                             0.026385   \n",
              "37                             0.028426   \n",
              "38                             0.029088   \n",
              "39                             0.028873   \n",
              "40                             0.026908   \n",
              "41                             0.027017   \n",
              "42                             0.026986   \n",
              "43                             0.027131   \n",
              "44                             0.026877   \n",
              "45                             0.026792   \n",
              "46                             0.026862   \n",
              "47                             0.027170   \n",
              "48                             0.028759   \n",
              "49                             0.028520   \n",
              "50                             0.025749   \n",
              "51                             0.026954   \n",
              "52                             0.026912   \n",
              "53                             0.026991   \n",
              "54                             0.026050   \n",
              "55                             0.022762   \n",
              "56                             0.026763   \n",
              "57                             0.026590   \n",
              "58                             0.032353   \n",
              "59                             0.027157   \n",
              "\n",
              "     Interest Coverage Ratio (Interest expense to EBIT)   Net Income Flag  \\\n",
              "0                                            0.564050                   1   \n",
              "1                                            0.570175                   1   \n",
              "2                                            0.563706                   1   \n",
              "3                                            0.564663                   1   \n",
              "4                                            0.575617                   1   \n",
              "5                                            0.564538                   1   \n",
              "6                                            0.564200                   1   \n",
              "7                                            0.566089                   1   \n",
              "8                                            0.565592                   1   \n",
              "9                                            0.566668                   1   \n",
              "10                                           0.565892                   1   \n",
              "11                                           0.566983                   1   \n",
              "12                                           0.567146                   1   \n",
              "13                                           0.577445                   1   \n",
              "14                                           0.565614                   1   \n",
              "15                                           0.565618                   1   \n",
              "16                                           0.565999                   1   \n",
              "17                                           0.565659                   1   \n",
              "18                                           0.567399                   1   \n",
              "19                                           0.565722                   1   \n",
              "20                                           0.565171                   1   \n",
              "21                                           0.565589                   1   \n",
              "22                                           0.567289                   1   \n",
              "23                                           0.568730                   1   \n",
              "24                                           0.568043                   1   \n",
              "25                                           0.562377                   1   \n",
              "26                                           0.565723                   1   \n",
              "27                                           0.565433                   1   \n",
              "28                                           0.566201                   1   \n",
              "29                                           0.562977                   1   \n",
              "30                                           0.576473                   1   \n",
              "31                                           0.570161                   1   \n",
              "32                                           0.566351                   1   \n",
              "33                                           0.566000                   1   \n",
              "34                                           0.567456                   1   \n",
              "35                                           0.566339                   1   \n",
              "36                                           0.562016                   1   \n",
              "37                                           0.568219                   1   \n",
              "38                                           0.568609                   1   \n",
              "39                                           0.568501                   1   \n",
              "40                                           0.565659                   1   \n",
              "41                                           0.566045                   1   \n",
              "42                                           0.565941                   1   \n",
              "43                                           0.566383                   1   \n",
              "44                                           0.565538                   1   \n",
              "45                                           0.565164                   1   \n",
              "46                                           0.565476                   1   \n",
              "47                                           0.566487                   1   \n",
              "48                                           0.568437                   1   \n",
              "49                                           0.568286                   1   \n",
              "50                                           0.198632                   1   \n",
              "51                                           0.565831                   1   \n",
              "52                                           0.565675                   1   \n",
              "53                                           0.565961                   1   \n",
              "54                                           0.553328                   1   \n",
              "55                                           0.571989                   1   \n",
              "56                                           0.565021                   1   \n",
              "57                                           0.563976                   1   \n",
              "58                                           0.569393                   1   \n",
              "59                                           0.566455                   1   \n",
              "\n",
              "     Equity to Liability  \n",
              "0               0.016469  \n",
              "1               0.020794  \n",
              "2               0.016474  \n",
              "3               0.023982  \n",
              "4               0.035490  \n",
              "5               0.019534  \n",
              "6               0.015663  \n",
              "7               0.034889  \n",
              "8               0.065826  \n",
              "9               0.030801  \n",
              "10              0.036572  \n",
              "11              0.040381  \n",
              "12              0.025282  \n",
              "13              0.035464  \n",
              "14              0.042646  \n",
              "15              0.038354  \n",
              "16              0.062940  \n",
              "17              0.048822  \n",
              "18              0.025953  \n",
              "19              0.049622  \n",
              "20              0.039507  \n",
              "21              0.037680  \n",
              "22              0.024915  \n",
              "23              0.018005  \n",
              "24              0.017839  \n",
              "25              0.024869  \n",
              "26              0.041465  \n",
              "27              0.104842  \n",
              "28              0.042682  \n",
              "29              0.015349  \n",
              "30              0.014562  \n",
              "31              0.016404  \n",
              "32              0.032086  \n",
              "33              0.061887  \n",
              "34              0.030024  \n",
              "35              0.030431  \n",
              "36              0.026387  \n",
              "37              0.024781  \n",
              "38              0.039935  \n",
              "39              0.047960  \n",
              "40              0.031485  \n",
              "41              0.055858  \n",
              "42              0.044195  \n",
              "43              0.026674  \n",
              "44              0.041906  \n",
              "45              0.040450  \n",
              "46              0.038105  \n",
              "47              0.025829  \n",
              "48              0.017125  \n",
              "49              0.018998  \n",
              "50              0.023491  \n",
              "51              0.041680  \n",
              "52              0.074266  \n",
              "53              0.040601  \n",
              "54              0.016199  \n",
              "55              0.014054  \n",
              "56              0.009178  \n",
              "57              0.015186  \n",
              "58              0.016943  \n",
              "59              0.025913  \n",
              "\n",
              "[60 rows x 96 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-be041c1c-868a-47ff-9b12-93cefee5d1de\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Bankrupt?</th>\n",
              "      <th>ROA(C) before interest and depreciation before interest</th>\n",
              "      <th>ROA(A) before interest and % after tax</th>\n",
              "      <th>ROA(B) before interest and depreciation after tax</th>\n",
              "      <th>Operating Gross Margin</th>\n",
              "      <th>Realized Sales Gross Margin</th>\n",
              "      <th>Operating Profit Rate</th>\n",
              "      <th>Pre-tax net Interest Rate</th>\n",
              "      <th>After-tax net Interest Rate</th>\n",
              "      <th>Non-industry income and expenditure/revenue</th>\n",
              "      <th>...</th>\n",
              "      <th>Net Income to Total Assets</th>\n",
              "      <th>Total assets to GNP price</th>\n",
              "      <th>No-credit Interval</th>\n",
              "      <th>Gross Profit to Sales</th>\n",
              "      <th>Net Income to Stockholder's Equity</th>\n",
              "      <th>Liability to Equity</th>\n",
              "      <th>Degree of Financial Leverage (DFL)</th>\n",
              "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
              "      <th>Net Income Flag</th>\n",
              "      <th>Equity to Liability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.370594</td>\n",
              "      <td>0.424389</td>\n",
              "      <td>0.405750</td>\n",
              "      <td>0.601457</td>\n",
              "      <td>0.601457</td>\n",
              "      <td>0.998969</td>\n",
              "      <td>0.796887</td>\n",
              "      <td>0.808809</td>\n",
              "      <td>0.302646</td>\n",
              "      <td>...</td>\n",
              "      <td>0.716845</td>\n",
              "      <td>0.009219</td>\n",
              "      <td>0.622879</td>\n",
              "      <td>0.601453</td>\n",
              "      <td>0.827890</td>\n",
              "      <td>0.290202</td>\n",
              "      <td>0.026601</td>\n",
              "      <td>0.564050</td>\n",
              "      <td>1</td>\n",
              "      <td>0.016469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.464291</td>\n",
              "      <td>0.538214</td>\n",
              "      <td>0.516730</td>\n",
              "      <td>0.610235</td>\n",
              "      <td>0.610235</td>\n",
              "      <td>0.998946</td>\n",
              "      <td>0.797380</td>\n",
              "      <td>0.809301</td>\n",
              "      <td>0.303556</td>\n",
              "      <td>...</td>\n",
              "      <td>0.795297</td>\n",
              "      <td>0.008323</td>\n",
              "      <td>0.623652</td>\n",
              "      <td>0.610237</td>\n",
              "      <td>0.839969</td>\n",
              "      <td>0.283846</td>\n",
              "      <td>0.264577</td>\n",
              "      <td>0.570175</td>\n",
              "      <td>1</td>\n",
              "      <td>0.020794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0.426071</td>\n",
              "      <td>0.499019</td>\n",
              "      <td>0.472295</td>\n",
              "      <td>0.601450</td>\n",
              "      <td>0.601364</td>\n",
              "      <td>0.998857</td>\n",
              "      <td>0.796403</td>\n",
              "      <td>0.808388</td>\n",
              "      <td>0.302035</td>\n",
              "      <td>...</td>\n",
              "      <td>0.774670</td>\n",
              "      <td>0.040003</td>\n",
              "      <td>0.623841</td>\n",
              "      <td>0.601449</td>\n",
              "      <td>0.836774</td>\n",
              "      <td>0.290189</td>\n",
              "      <td>0.026555</td>\n",
              "      <td>0.563706</td>\n",
              "      <td>1</td>\n",
              "      <td>0.016474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0.399844</td>\n",
              "      <td>0.451265</td>\n",
              "      <td>0.457733</td>\n",
              "      <td>0.583541</td>\n",
              "      <td>0.583541</td>\n",
              "      <td>0.998700</td>\n",
              "      <td>0.796967</td>\n",
              "      <td>0.808966</td>\n",
              "      <td>0.303350</td>\n",
              "      <td>...</td>\n",
              "      <td>0.739555</td>\n",
              "      <td>0.003252</td>\n",
              "      <td>0.622929</td>\n",
              "      <td>0.583538</td>\n",
              "      <td>0.834697</td>\n",
              "      <td>0.281721</td>\n",
              "      <td>0.026697</td>\n",
              "      <td>0.564663</td>\n",
              "      <td>1</td>\n",
              "      <td>0.023982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0.465022</td>\n",
              "      <td>0.538432</td>\n",
              "      <td>0.522298</td>\n",
              "      <td>0.598783</td>\n",
              "      <td>0.598783</td>\n",
              "      <td>0.998973</td>\n",
              "      <td>0.797366</td>\n",
              "      <td>0.809304</td>\n",
              "      <td>0.303475</td>\n",
              "      <td>...</td>\n",
              "      <td>0.795016</td>\n",
              "      <td>0.003878</td>\n",
              "      <td>0.623521</td>\n",
              "      <td>0.598782</td>\n",
              "      <td>0.839973</td>\n",
              "      <td>0.278514</td>\n",
              "      <td>0.024752</td>\n",
              "      <td>0.575617</td>\n",
              "      <td>1</td>\n",
              "      <td>0.035490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0.388680</td>\n",
              "      <td>0.415177</td>\n",
              "      <td>0.419134</td>\n",
              "      <td>0.590171</td>\n",
              "      <td>0.590251</td>\n",
              "      <td>0.998758</td>\n",
              "      <td>0.796903</td>\n",
              "      <td>0.808771</td>\n",
              "      <td>0.303116</td>\n",
              "      <td>...</td>\n",
              "      <td>0.710420</td>\n",
              "      <td>0.005278</td>\n",
              "      <td>0.622605</td>\n",
              "      <td>0.590172</td>\n",
              "      <td>0.829939</td>\n",
              "      <td>0.285087</td>\n",
              "      <td>0.026675</td>\n",
              "      <td>0.564538</td>\n",
              "      <td>1</td>\n",
              "      <td>0.019534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0.390923</td>\n",
              "      <td>0.445704</td>\n",
              "      <td>0.436158</td>\n",
              "      <td>0.619950</td>\n",
              "      <td>0.619950</td>\n",
              "      <td>0.998993</td>\n",
              "      <td>0.797012</td>\n",
              "      <td>0.808960</td>\n",
              "      <td>0.302814</td>\n",
              "      <td>...</td>\n",
              "      <td>0.736619</td>\n",
              "      <td>0.018372</td>\n",
              "      <td>0.623655</td>\n",
              "      <td>0.619949</td>\n",
              "      <td>0.829980</td>\n",
              "      <td>0.292504</td>\n",
              "      <td>0.026622</td>\n",
              "      <td>0.564200</td>\n",
              "      <td>1</td>\n",
              "      <td>0.015663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0.508361</td>\n",
              "      <td>0.570922</td>\n",
              "      <td>0.559077</td>\n",
              "      <td>0.601738</td>\n",
              "      <td>0.601717</td>\n",
              "      <td>0.999009</td>\n",
              "      <td>0.797449</td>\n",
              "      <td>0.809362</td>\n",
              "      <td>0.303545</td>\n",
              "      <td>...</td>\n",
              "      <td>0.815350</td>\n",
              "      <td>0.010005</td>\n",
              "      <td>0.623843</td>\n",
              "      <td>0.601739</td>\n",
              "      <td>0.841459</td>\n",
              "      <td>0.278607</td>\n",
              "      <td>0.027031</td>\n",
              "      <td>0.566089</td>\n",
              "      <td>1</td>\n",
              "      <td>0.034889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0.488519</td>\n",
              "      <td>0.545137</td>\n",
              "      <td>0.543284</td>\n",
              "      <td>0.603612</td>\n",
              "      <td>0.603612</td>\n",
              "      <td>0.998961</td>\n",
              "      <td>0.797414</td>\n",
              "      <td>0.809338</td>\n",
              "      <td>0.303584</td>\n",
              "      <td>...</td>\n",
              "      <td>0.803647</td>\n",
              "      <td>0.000824</td>\n",
              "      <td>0.623977</td>\n",
              "      <td>0.603613</td>\n",
              "      <td>0.840487</td>\n",
              "      <td>0.276423</td>\n",
              "      <td>0.026891</td>\n",
              "      <td>0.565592</td>\n",
              "      <td>1</td>\n",
              "      <td>0.065826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0.495686</td>\n",
              "      <td>0.550916</td>\n",
              "      <td>0.542963</td>\n",
              "      <td>0.599209</td>\n",
              "      <td>0.599209</td>\n",
              "      <td>0.999001</td>\n",
              "      <td>0.797404</td>\n",
              "      <td>0.809320</td>\n",
              "      <td>0.303483</td>\n",
              "      <td>...</td>\n",
              "      <td>0.804195</td>\n",
              "      <td>0.005798</td>\n",
              "      <td>0.623865</td>\n",
              "      <td>0.599205</td>\n",
              "      <td>0.840688</td>\n",
              "      <td>0.279388</td>\n",
              "      <td>0.027243</td>\n",
              "      <td>0.566668</td>\n",
              "      <td>1</td>\n",
              "      <td>0.030801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0.482475</td>\n",
              "      <td>0.567543</td>\n",
              "      <td>0.538198</td>\n",
              "      <td>0.614026</td>\n",
              "      <td>0.614026</td>\n",
              "      <td>0.998978</td>\n",
              "      <td>0.797535</td>\n",
              "      <td>0.809460</td>\n",
              "      <td>0.303759</td>\n",
              "      <td>...</td>\n",
              "      <td>0.814111</td>\n",
              "      <td>0.076972</td>\n",
              "      <td>0.623687</td>\n",
              "      <td>0.614021</td>\n",
              "      <td>0.841337</td>\n",
              "      <td>0.278356</td>\n",
              "      <td>0.026971</td>\n",
              "      <td>0.565892</td>\n",
              "      <td>1</td>\n",
              "      <td>0.036572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0.444401</td>\n",
              "      <td>0.549717</td>\n",
              "      <td>0.498956</td>\n",
              "      <td>0.623712</td>\n",
              "      <td>0.623712</td>\n",
              "      <td>0.998975</td>\n",
              "      <td>0.797443</td>\n",
              "      <td>0.809389</td>\n",
              "      <td>0.303605</td>\n",
              "      <td>...</td>\n",
              "      <td>0.804887</td>\n",
              "      <td>0.007318</td>\n",
              "      <td>0.623724</td>\n",
              "      <td>0.623709</td>\n",
              "      <td>0.840650</td>\n",
              "      <td>0.277892</td>\n",
              "      <td>0.027391</td>\n",
              "      <td>0.566983</td>\n",
              "      <td>1</td>\n",
              "      <td>0.040381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>0.491152</td>\n",
              "      <td>0.551570</td>\n",
              "      <td>0.543391</td>\n",
              "      <td>0.608131</td>\n",
              "      <td>0.608138</td>\n",
              "      <td>0.999045</td>\n",
              "      <td>0.797429</td>\n",
              "      <td>0.809344</td>\n",
              "      <td>0.303435</td>\n",
              "      <td>...</td>\n",
              "      <td>0.803260</td>\n",
              "      <td>0.008232</td>\n",
              "      <td>0.623578</td>\n",
              "      <td>0.608125</td>\n",
              "      <td>0.840702</td>\n",
              "      <td>0.281113</td>\n",
              "      <td>0.027480</td>\n",
              "      <td>0.567146</td>\n",
              "      <td>1</td>\n",
              "      <td>0.025282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>0.474041</td>\n",
              "      <td>0.533308</td>\n",
              "      <td>0.523690</td>\n",
              "      <td>0.600578</td>\n",
              "      <td>0.600578</td>\n",
              "      <td>0.998967</td>\n",
              "      <td>0.797368</td>\n",
              "      <td>0.809299</td>\n",
              "      <td>0.303492</td>\n",
              "      <td>...</td>\n",
              "      <td>0.794158</td>\n",
              "      <td>0.005262</td>\n",
              "      <td>0.623777</td>\n",
              "      <td>0.600578</td>\n",
              "      <td>0.839910</td>\n",
              "      <td>0.278518</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.577445</td>\n",
              "      <td>1</td>\n",
              "      <td>0.035464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>0.506703</td>\n",
              "      <td>0.575829</td>\n",
              "      <td>0.569838</td>\n",
              "      <td>0.604686</td>\n",
              "      <td>0.604686</td>\n",
              "      <td>0.999053</td>\n",
              "      <td>0.797514</td>\n",
              "      <td>0.809456</td>\n",
              "      <td>0.303566</td>\n",
              "      <td>...</td>\n",
              "      <td>0.819715</td>\n",
              "      <td>0.003954</td>\n",
              "      <td>0.624946</td>\n",
              "      <td>0.604683</td>\n",
              "      <td>0.841624</td>\n",
              "      <td>0.277668</td>\n",
              "      <td>0.026896</td>\n",
              "      <td>0.565614</td>\n",
              "      <td>1</td>\n",
              "      <td>0.042646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>0.513821</td>\n",
              "      <td>0.571086</td>\n",
              "      <td>0.558756</td>\n",
              "      <td>0.621773</td>\n",
              "      <td>0.621773</td>\n",
              "      <td>0.999097</td>\n",
              "      <td>0.797507</td>\n",
              "      <td>0.809396</td>\n",
              "      <td>0.303462</td>\n",
              "      <td>...</td>\n",
              "      <td>0.815419</td>\n",
              "      <td>0.006590</td>\n",
              "      <td>0.623764</td>\n",
              "      <td>0.621769</td>\n",
              "      <td>0.841399</td>\n",
              "      <td>0.278124</td>\n",
              "      <td>0.026897</td>\n",
              "      <td>0.565618</td>\n",
              "      <td>1</td>\n",
              "      <td>0.038354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>0.488909</td>\n",
              "      <td>0.560238</td>\n",
              "      <td>0.540286</td>\n",
              "      <td>0.606524</td>\n",
              "      <td>0.606524</td>\n",
              "      <td>0.998996</td>\n",
              "      <td>0.797877</td>\n",
              "      <td>0.809761</td>\n",
              "      <td>0.304319</td>\n",
              "      <td>...</td>\n",
              "      <td>0.810421</td>\n",
              "      <td>0.013611</td>\n",
              "      <td>0.623325</td>\n",
              "      <td>0.606524</td>\n",
              "      <td>0.840889</td>\n",
              "      <td>0.276518</td>\n",
              "      <td>0.027003</td>\n",
              "      <td>0.565999</td>\n",
              "      <td>1</td>\n",
              "      <td>0.062940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>0.535953</td>\n",
              "      <td>0.590438</td>\n",
              "      <td>0.580920</td>\n",
              "      <td>0.618451</td>\n",
              "      <td>0.618451</td>\n",
              "      <td>0.999119</td>\n",
              "      <td>0.797588</td>\n",
              "      <td>0.809461</td>\n",
              "      <td>0.303559</td>\n",
              "      <td>...</td>\n",
              "      <td>0.826642</td>\n",
              "      <td>0.002746</td>\n",
              "      <td>0.623825</td>\n",
              "      <td>0.618452</td>\n",
              "      <td>0.841983</td>\n",
              "      <td>0.277190</td>\n",
              "      <td>0.026908</td>\n",
              "      <td>0.565659</td>\n",
              "      <td>1</td>\n",
              "      <td>0.048822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>0.504071</td>\n",
              "      <td>0.559802</td>\n",
              "      <td>0.558649</td>\n",
              "      <td>0.598344</td>\n",
              "      <td>0.598344</td>\n",
              "      <td>0.998989</td>\n",
              "      <td>0.797412</td>\n",
              "      <td>0.809334</td>\n",
              "      <td>0.303523</td>\n",
              "      <td>...</td>\n",
              "      <td>0.806264</td>\n",
              "      <td>0.004238</td>\n",
              "      <td>0.623339</td>\n",
              "      <td>0.598345</td>\n",
              "      <td>0.840955</td>\n",
              "      <td>0.280839</td>\n",
              "      <td>0.027637</td>\n",
              "      <td>0.567399</td>\n",
              "      <td>1</td>\n",
              "      <td>0.025953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>0.487398</td>\n",
              "      <td>0.543720</td>\n",
              "      <td>0.533647</td>\n",
              "      <td>0.636259</td>\n",
              "      <td>0.636252</td>\n",
              "      <td>0.999042</td>\n",
              "      <td>0.797444</td>\n",
              "      <td>0.809340</td>\n",
              "      <td>0.303467</td>\n",
              "      <td>...</td>\n",
              "      <td>0.802552</td>\n",
              "      <td>0.001521</td>\n",
              "      <td>0.623763</td>\n",
              "      <td>0.636256</td>\n",
              "      <td>0.840456</td>\n",
              "      <td>0.277139</td>\n",
              "      <td>0.026924</td>\n",
              "      <td>0.565722</td>\n",
              "      <td>1</td>\n",
              "      <td>0.049622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0</td>\n",
              "      <td>0.485253</td>\n",
              "      <td>0.545573</td>\n",
              "      <td>0.534665</td>\n",
              "      <td>0.622177</td>\n",
              "      <td>0.622256</td>\n",
              "      <td>0.999065</td>\n",
              "      <td>0.797522</td>\n",
              "      <td>0.809406</td>\n",
              "      <td>0.303554</td>\n",
              "      <td>...</td>\n",
              "      <td>0.804639</td>\n",
              "      <td>0.014039</td>\n",
              "      <td>0.623922</td>\n",
              "      <td>0.622177</td>\n",
              "      <td>0.840639</td>\n",
              "      <td>0.277988</td>\n",
              "      <td>0.026794</td>\n",
              "      <td>0.565171</td>\n",
              "      <td>1</td>\n",
              "      <td>0.039507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0</td>\n",
              "      <td>0.504558</td>\n",
              "      <td>0.564490</td>\n",
              "      <td>0.553027</td>\n",
              "      <td>0.607446</td>\n",
              "      <td>0.607446</td>\n",
              "      <td>0.999030</td>\n",
              "      <td>0.797466</td>\n",
              "      <td>0.809371</td>\n",
              "      <td>0.303531</td>\n",
              "      <td>...</td>\n",
              "      <td>0.814012</td>\n",
              "      <td>0.000935</td>\n",
              "      <td>0.623996</td>\n",
              "      <td>0.607441</td>\n",
              "      <td>0.841311</td>\n",
              "      <td>0.278208</td>\n",
              "      <td>0.026890</td>\n",
              "      <td>0.565589</td>\n",
              "      <td>1</td>\n",
              "      <td>0.037680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0</td>\n",
              "      <td>0.512017</td>\n",
              "      <td>0.563672</td>\n",
              "      <td>0.569035</td>\n",
              "      <td>0.618934</td>\n",
              "      <td>0.618934</td>\n",
              "      <td>0.999059</td>\n",
              "      <td>0.797444</td>\n",
              "      <td>0.809367</td>\n",
              "      <td>0.303433</td>\n",
              "      <td>...</td>\n",
              "      <td>0.808976</td>\n",
              "      <td>0.001830</td>\n",
              "      <td>0.623718</td>\n",
              "      <td>0.618933</td>\n",
              "      <td>0.841234</td>\n",
              "      <td>0.281274</td>\n",
              "      <td>0.027565</td>\n",
              "      <td>0.567289</td>\n",
              "      <td>1</td>\n",
              "      <td>0.024915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0</td>\n",
              "      <td>0.494857</td>\n",
              "      <td>0.548136</td>\n",
              "      <td>0.540446</td>\n",
              "      <td>0.609507</td>\n",
              "      <td>0.609507</td>\n",
              "      <td>0.999052</td>\n",
              "      <td>0.797413</td>\n",
              "      <td>0.809323</td>\n",
              "      <td>0.303392</td>\n",
              "      <td>...</td>\n",
              "      <td>0.798104</td>\n",
              "      <td>0.018753</td>\n",
              "      <td>0.623486</td>\n",
              "      <td>0.609508</td>\n",
              "      <td>0.840326</td>\n",
              "      <td>0.287134</td>\n",
              "      <td>0.029364</td>\n",
              "      <td>0.568730</td>\n",
              "      <td>1</td>\n",
              "      <td>0.018005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0</td>\n",
              "      <td>0.509969</td>\n",
              "      <td>0.561382</td>\n",
              "      <td>0.552599</td>\n",
              "      <td>0.602956</td>\n",
              "      <td>0.602956</td>\n",
              "      <td>0.999025</td>\n",
              "      <td>0.797417</td>\n",
              "      <td>0.809331</td>\n",
              "      <td>0.303456</td>\n",
              "      <td>...</td>\n",
              "      <td>0.803445</td>\n",
              "      <td>0.000830</td>\n",
              "      <td>0.623482</td>\n",
              "      <td>0.602952</td>\n",
              "      <td>0.841047</td>\n",
              "      <td>0.287406</td>\n",
              "      <td>0.028206</td>\n",
              "      <td>0.568043</td>\n",
              "      <td>1</td>\n",
              "      <td>0.017839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0</td>\n",
              "      <td>0.444986</td>\n",
              "      <td>0.503652</td>\n",
              "      <td>0.495530</td>\n",
              "      <td>0.611763</td>\n",
              "      <td>0.611806</td>\n",
              "      <td>0.998925</td>\n",
              "      <td>0.797046</td>\n",
              "      <td>0.808993</td>\n",
              "      <td>0.303017</td>\n",
              "      <td>...</td>\n",
              "      <td>0.772909</td>\n",
              "      <td>0.005887</td>\n",
              "      <td>0.623655</td>\n",
              "      <td>0.611761</td>\n",
              "      <td>0.837926</td>\n",
              "      <td>0.281295</td>\n",
              "      <td>0.026415</td>\n",
              "      <td>0.562377</td>\n",
              "      <td>1</td>\n",
              "      <td>0.024869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0</td>\n",
              "      <td>0.519280</td>\n",
              "      <td>0.563618</td>\n",
              "      <td>0.568606</td>\n",
              "      <td>0.626544</td>\n",
              "      <td>0.626544</td>\n",
              "      <td>0.999099</td>\n",
              "      <td>0.797512</td>\n",
              "      <td>0.809403</td>\n",
              "      <td>0.303466</td>\n",
              "      <td>...</td>\n",
              "      <td>0.812974</td>\n",
              "      <td>0.001193</td>\n",
              "      <td>0.623712</td>\n",
              "      <td>0.626543</td>\n",
              "      <td>0.841188</td>\n",
              "      <td>0.277781</td>\n",
              "      <td>0.026924</td>\n",
              "      <td>0.565723</td>\n",
              "      <td>1</td>\n",
              "      <td>0.041465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0</td>\n",
              "      <td>0.547409</td>\n",
              "      <td>0.593055</td>\n",
              "      <td>0.592484</td>\n",
              "      <td>0.609738</td>\n",
              "      <td>0.609738</td>\n",
              "      <td>0.999112</td>\n",
              "      <td>0.797571</td>\n",
              "      <td>0.809449</td>\n",
              "      <td>0.303543</td>\n",
              "      <td>...</td>\n",
              "      <td>0.830176</td>\n",
              "      <td>0.000703</td>\n",
              "      <td>0.624308</td>\n",
              "      <td>0.609733</td>\n",
              "      <td>0.841854</td>\n",
              "      <td>0.275704</td>\n",
              "      <td>0.026852</td>\n",
              "      <td>0.565433</td>\n",
              "      <td>1</td>\n",
              "      <td>0.104842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0</td>\n",
              "      <td>0.500853</td>\n",
              "      <td>0.563945</td>\n",
              "      <td>0.550565</td>\n",
              "      <td>0.602027</td>\n",
              "      <td>0.602293</td>\n",
              "      <td>0.998982</td>\n",
              "      <td>0.797503</td>\n",
              "      <td>0.809400</td>\n",
              "      <td>0.303695</td>\n",
              "      <td>...</td>\n",
              "      <td>0.812100</td>\n",
              "      <td>0.001225</td>\n",
              "      <td>0.623910</td>\n",
              "      <td>0.602027</td>\n",
              "      <td>0.841116</td>\n",
              "      <td>0.277664</td>\n",
              "      <td>0.027067</td>\n",
              "      <td>0.566201</td>\n",
              "      <td>1</td>\n",
              "      <td>0.042682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1</td>\n",
              "      <td>0.416126</td>\n",
              "      <td>0.470235</td>\n",
              "      <td>0.463783</td>\n",
              "      <td>0.599115</td>\n",
              "      <td>0.599115</td>\n",
              "      <td>0.998973</td>\n",
              "      <td>0.797241</td>\n",
              "      <td>0.809176</td>\n",
              "      <td>0.303258</td>\n",
              "      <td>...</td>\n",
              "      <td>0.750431</td>\n",
              "      <td>0.003011</td>\n",
              "      <td>0.623183</td>\n",
              "      <td>0.599110</td>\n",
              "      <td>0.831976</td>\n",
              "      <td>0.293599</td>\n",
              "      <td>0.026472</td>\n",
              "      <td>0.562977</td>\n",
              "      <td>1</td>\n",
              "      <td>0.015349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1</td>\n",
              "      <td>0.462195</td>\n",
              "      <td>0.536034</td>\n",
              "      <td>0.514428</td>\n",
              "      <td>0.599987</td>\n",
              "      <td>0.599987</td>\n",
              "      <td>0.998909</td>\n",
              "      <td>0.797291</td>\n",
              "      <td>0.809223</td>\n",
              "      <td>0.303477</td>\n",
              "      <td>...</td>\n",
              "      <td>0.787082</td>\n",
              "      <td>0.001631</td>\n",
              "      <td>0.621876</td>\n",
              "      <td>0.599984</td>\n",
              "      <td>0.838259</td>\n",
              "      <td>0.297038</td>\n",
              "      <td>0.024886</td>\n",
              "      <td>0.576473</td>\n",
              "      <td>1</td>\n",
              "      <td>0.014562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0</td>\n",
              "      <td>0.476088</td>\n",
              "      <td>0.544483</td>\n",
              "      <td>0.529686</td>\n",
              "      <td>0.618869</td>\n",
              "      <td>0.618869</td>\n",
              "      <td>0.999014</td>\n",
              "      <td>0.797380</td>\n",
              "      <td>0.809307</td>\n",
              "      <td>0.303415</td>\n",
              "      <td>...</td>\n",
              "      <td>0.796023</td>\n",
              "      <td>0.015252</td>\n",
              "      <td>0.623562</td>\n",
              "      <td>0.618864</td>\n",
              "      <td>0.840052</td>\n",
              "      <td>0.290365</td>\n",
              "      <td>0.174091</td>\n",
              "      <td>0.570161</td>\n",
              "      <td>1</td>\n",
              "      <td>0.016404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0</td>\n",
              "      <td>0.505874</td>\n",
              "      <td>0.570704</td>\n",
              "      <td>0.559827</td>\n",
              "      <td>0.601169</td>\n",
              "      <td>0.601176</td>\n",
              "      <td>0.999000</td>\n",
              "      <td>0.797440</td>\n",
              "      <td>0.809359</td>\n",
              "      <td>0.303547</td>\n",
              "      <td>...</td>\n",
              "      <td>0.814102</td>\n",
              "      <td>0.009774</td>\n",
              "      <td>0.623810</td>\n",
              "      <td>0.601169</td>\n",
              "      <td>0.841432</td>\n",
              "      <td>0.279111</td>\n",
              "      <td>0.027119</td>\n",
              "      <td>0.566351</td>\n",
              "      <td>1</td>\n",
              "      <td>0.032086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0</td>\n",
              "      <td>0.486374</td>\n",
              "      <td>0.544756</td>\n",
              "      <td>0.540500</td>\n",
              "      <td>0.603475</td>\n",
              "      <td>0.603475</td>\n",
              "      <td>0.998944</td>\n",
              "      <td>0.797415</td>\n",
              "      <td>0.809338</td>\n",
              "      <td>0.303622</td>\n",
              "      <td>...</td>\n",
              "      <td>0.802923</td>\n",
              "      <td>0.000785</td>\n",
              "      <td>0.623908</td>\n",
              "      <td>0.603474</td>\n",
              "      <td>0.840451</td>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.027003</td>\n",
              "      <td>0.566000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.061887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0</td>\n",
              "      <td>0.494369</td>\n",
              "      <td>0.550916</td>\n",
              "      <td>0.548584</td>\n",
              "      <td>0.599108</td>\n",
              "      <td>0.599108</td>\n",
              "      <td>0.998995</td>\n",
              "      <td>0.797393</td>\n",
              "      <td>0.809318</td>\n",
              "      <td>0.303475</td>\n",
              "      <td>...</td>\n",
              "      <td>0.803066</td>\n",
              "      <td>0.005124</td>\n",
              "      <td>0.623898</td>\n",
              "      <td>0.599107</td>\n",
              "      <td>0.840609</td>\n",
              "      <td>0.279572</td>\n",
              "      <td>0.027677</td>\n",
              "      <td>0.567456</td>\n",
              "      <td>1</td>\n",
              "      <td>0.030024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0</td>\n",
              "      <td>0.475162</td>\n",
              "      <td>0.564544</td>\n",
              "      <td>0.525028</td>\n",
              "      <td>0.614278</td>\n",
              "      <td>0.614278</td>\n",
              "      <td>0.998973</td>\n",
              "      <td>0.797520</td>\n",
              "      <td>0.809444</td>\n",
              "      <td>0.303743</td>\n",
              "      <td>...</td>\n",
              "      <td>0.811257</td>\n",
              "      <td>0.080006</td>\n",
              "      <td>0.623709</td>\n",
              "      <td>0.614277</td>\n",
              "      <td>0.841252</td>\n",
              "      <td>0.279474</td>\n",
              "      <td>0.027115</td>\n",
              "      <td>0.566339</td>\n",
              "      <td>1</td>\n",
              "      <td>0.030431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0</td>\n",
              "      <td>0.462926</td>\n",
              "      <td>0.516354</td>\n",
              "      <td>0.517105</td>\n",
              "      <td>0.622703</td>\n",
              "      <td>0.622530</td>\n",
              "      <td>0.998997</td>\n",
              "      <td>0.797245</td>\n",
              "      <td>0.809190</td>\n",
              "      <td>0.303212</td>\n",
              "      <td>...</td>\n",
              "      <td>0.784385</td>\n",
              "      <td>0.007563</td>\n",
              "      <td>0.623631</td>\n",
              "      <td>0.622698</td>\n",
              "      <td>0.839029</td>\n",
              "      <td>0.280673</td>\n",
              "      <td>0.026385</td>\n",
              "      <td>0.562016</td>\n",
              "      <td>1</td>\n",
              "      <td>0.026387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0</td>\n",
              "      <td>0.478916</td>\n",
              "      <td>0.541321</td>\n",
              "      <td>0.529311</td>\n",
              "      <td>0.607042</td>\n",
              "      <td>0.607006</td>\n",
              "      <td>0.999017</td>\n",
              "      <td>0.797404</td>\n",
              "      <td>0.809317</td>\n",
              "      <td>0.303449</td>\n",
              "      <td>...</td>\n",
              "      <td>0.797744</td>\n",
              "      <td>0.007308</td>\n",
              "      <td>0.623731</td>\n",
              "      <td>0.607042</td>\n",
              "      <td>0.840206</td>\n",
              "      <td>0.281335</td>\n",
              "      <td>0.028426</td>\n",
              "      <td>0.568219</td>\n",
              "      <td>1</td>\n",
              "      <td>0.024781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0</td>\n",
              "      <td>0.481061</td>\n",
              "      <td>0.539468</td>\n",
              "      <td>0.530864</td>\n",
              "      <td>0.600881</td>\n",
              "      <td>0.600881</td>\n",
              "      <td>0.998975</td>\n",
              "      <td>0.797388</td>\n",
              "      <td>0.809316</td>\n",
              "      <td>0.303508</td>\n",
              "      <td>...</td>\n",
              "      <td>0.798174</td>\n",
              "      <td>0.004593</td>\n",
              "      <td>0.623962</td>\n",
              "      <td>0.600876</td>\n",
              "      <td>0.840193</td>\n",
              "      <td>0.277940</td>\n",
              "      <td>0.029088</td>\n",
              "      <td>0.568609</td>\n",
              "      <td>1</td>\n",
              "      <td>0.039935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0</td>\n",
              "      <td>0.486228</td>\n",
              "      <td>0.548027</td>\n",
              "      <td>0.544140</td>\n",
              "      <td>0.598445</td>\n",
              "      <td>0.598445</td>\n",
              "      <td>0.998953</td>\n",
              "      <td>0.797404</td>\n",
              "      <td>0.809343</td>\n",
              "      <td>0.303584</td>\n",
              "      <td>...</td>\n",
              "      <td>0.801372</td>\n",
              "      <td>0.003425</td>\n",
              "      <td>0.623089</td>\n",
              "      <td>0.598440</td>\n",
              "      <td>0.840386</td>\n",
              "      <td>0.277247</td>\n",
              "      <td>0.028873</td>\n",
              "      <td>0.568501</td>\n",
              "      <td>1</td>\n",
              "      <td>0.047960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0</td>\n",
              "      <td>0.507824</td>\n",
              "      <td>0.571631</td>\n",
              "      <td>0.549226</td>\n",
              "      <td>0.625326</td>\n",
              "      <td>0.625326</td>\n",
              "      <td>0.999136</td>\n",
              "      <td>0.797542</td>\n",
              "      <td>0.809413</td>\n",
              "      <td>0.303442</td>\n",
              "      <td>...</td>\n",
              "      <td>0.815731</td>\n",
              "      <td>0.007173</td>\n",
              "      <td>0.623729</td>\n",
              "      <td>0.625327</td>\n",
              "      <td>0.841574</td>\n",
              "      <td>0.279236</td>\n",
              "      <td>0.026908</td>\n",
              "      <td>0.565659</td>\n",
              "      <td>1</td>\n",
              "      <td>0.031485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0</td>\n",
              "      <td>0.489153</td>\n",
              "      <td>0.562691</td>\n",
              "      <td>0.541517</td>\n",
              "      <td>0.604412</td>\n",
              "      <td>0.604412</td>\n",
              "      <td>0.998971</td>\n",
              "      <td>0.797896</td>\n",
              "      <td>0.809799</td>\n",
              "      <td>0.304406</td>\n",
              "      <td>...</td>\n",
              "      <td>0.811426</td>\n",
              "      <td>0.013698</td>\n",
              "      <td>0.622974</td>\n",
              "      <td>0.604410</td>\n",
              "      <td>0.840979</td>\n",
              "      <td>0.276803</td>\n",
              "      <td>0.027017</td>\n",
              "      <td>0.566045</td>\n",
              "      <td>1</td>\n",
              "      <td>0.055858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0</td>\n",
              "      <td>0.511188</td>\n",
              "      <td>0.566888</td>\n",
              "      <td>0.559077</td>\n",
              "      <td>0.618177</td>\n",
              "      <td>0.618177</td>\n",
              "      <td>0.999077</td>\n",
              "      <td>0.797522</td>\n",
              "      <td>0.809409</td>\n",
              "      <td>0.303529</td>\n",
              "      <td>...</td>\n",
              "      <td>0.813568</td>\n",
              "      <td>0.002623</td>\n",
              "      <td>0.623780</td>\n",
              "      <td>0.618178</td>\n",
              "      <td>0.841197</td>\n",
              "      <td>0.277532</td>\n",
              "      <td>0.026986</td>\n",
              "      <td>0.565941</td>\n",
              "      <td>1</td>\n",
              "      <td>0.044195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0</td>\n",
              "      <td>0.523034</td>\n",
              "      <td>0.575883</td>\n",
              "      <td>0.576423</td>\n",
              "      <td>0.600347</td>\n",
              "      <td>0.600347</td>\n",
              "      <td>0.999027</td>\n",
              "      <td>0.797447</td>\n",
              "      <td>0.809362</td>\n",
              "      <td>0.303503</td>\n",
              "      <td>...</td>\n",
              "      <td>0.816257</td>\n",
              "      <td>0.004076</td>\n",
              "      <td>0.622494</td>\n",
              "      <td>0.600343</td>\n",
              "      <td>0.841802</td>\n",
              "      <td>0.280569</td>\n",
              "      <td>0.027131</td>\n",
              "      <td>0.566383</td>\n",
              "      <td>1</td>\n",
              "      <td>0.026674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0</td>\n",
              "      <td>0.508848</td>\n",
              "      <td>0.553423</td>\n",
              "      <td>0.552920</td>\n",
              "      <td>0.635718</td>\n",
              "      <td>0.635718</td>\n",
              "      <td>0.999062</td>\n",
              "      <td>0.797481</td>\n",
              "      <td>0.809365</td>\n",
              "      <td>0.303490</td>\n",
              "      <td>...</td>\n",
              "      <td>0.807573</td>\n",
              "      <td>0.001521</td>\n",
              "      <td>0.623752</td>\n",
              "      <td>0.635718</td>\n",
              "      <td>0.840820</td>\n",
              "      <td>0.277738</td>\n",
              "      <td>0.026877</td>\n",
              "      <td>0.565538</td>\n",
              "      <td>1</td>\n",
              "      <td>0.041906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0</td>\n",
              "      <td>0.488958</td>\n",
              "      <td>0.549008</td>\n",
              "      <td>0.538947</td>\n",
              "      <td>0.624317</td>\n",
              "      <td>0.624288</td>\n",
              "      <td>0.999098</td>\n",
              "      <td>0.797547</td>\n",
              "      <td>0.809430</td>\n",
              "      <td>0.303529</td>\n",
              "      <td>...</td>\n",
              "      <td>0.806667</td>\n",
              "      <td>0.012883</td>\n",
              "      <td>0.623855</td>\n",
              "      <td>0.624318</td>\n",
              "      <td>0.840771</td>\n",
              "      <td>0.277885</td>\n",
              "      <td>0.026792</td>\n",
              "      <td>0.565164</td>\n",
              "      <td>1</td>\n",
              "      <td>0.040450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0</td>\n",
              "      <td>0.502121</td>\n",
              "      <td>0.563509</td>\n",
              "      <td>0.545854</td>\n",
              "      <td>0.608866</td>\n",
              "      <td>0.608866</td>\n",
              "      <td>0.999048</td>\n",
              "      <td>0.797465</td>\n",
              "      <td>0.809364</td>\n",
              "      <td>0.303491</td>\n",
              "      <td>...</td>\n",
              "      <td>0.813580</td>\n",
              "      <td>0.000887</td>\n",
              "      <td>0.624017</td>\n",
              "      <td>0.608864</td>\n",
              "      <td>0.841275</td>\n",
              "      <td>0.278154</td>\n",
              "      <td>0.026862</td>\n",
              "      <td>0.565476</td>\n",
              "      <td>1</td>\n",
              "      <td>0.038105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0</td>\n",
              "      <td>0.523083</td>\n",
              "      <td>0.563345</td>\n",
              "      <td>0.572354</td>\n",
              "      <td>0.620944</td>\n",
              "      <td>0.620944</td>\n",
              "      <td>0.999102</td>\n",
              "      <td>0.797475</td>\n",
              "      <td>0.809371</td>\n",
              "      <td>0.303396</td>\n",
              "      <td>...</td>\n",
              "      <td>0.809900</td>\n",
              "      <td>0.001675</td>\n",
              "      <td>0.623711</td>\n",
              "      <td>0.620941</td>\n",
              "      <td>0.841282</td>\n",
              "      <td>0.280888</td>\n",
              "      <td>0.027170</td>\n",
              "      <td>0.566487</td>\n",
              "      <td>1</td>\n",
              "      <td>0.025829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0</td>\n",
              "      <td>0.493346</td>\n",
              "      <td>0.550534</td>\n",
              "      <td>0.539804</td>\n",
              "      <td>0.610206</td>\n",
              "      <td>0.610206</td>\n",
              "      <td>0.999023</td>\n",
              "      <td>0.797429</td>\n",
              "      <td>0.809341</td>\n",
              "      <td>0.303480</td>\n",
              "      <td>...</td>\n",
              "      <td>0.799842</td>\n",
              "      <td>0.018859</td>\n",
              "      <td>0.623536</td>\n",
              "      <td>0.610202</td>\n",
              "      <td>0.840599</td>\n",
              "      <td>0.288724</td>\n",
              "      <td>0.028759</td>\n",
              "      <td>0.568437</td>\n",
              "      <td>1</td>\n",
              "      <td>0.017125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0</td>\n",
              "      <td>0.505631</td>\n",
              "      <td>0.559747</td>\n",
              "      <td>0.549173</td>\n",
              "      <td>0.604563</td>\n",
              "      <td>0.604563</td>\n",
              "      <td>0.999031</td>\n",
              "      <td>0.797415</td>\n",
              "      <td>0.809332</td>\n",
              "      <td>0.303439</td>\n",
              "      <td>...</td>\n",
              "      <td>0.802638</td>\n",
              "      <td>0.000717</td>\n",
              "      <td>0.623489</td>\n",
              "      <td>0.604564</td>\n",
              "      <td>0.840860</td>\n",
              "      <td>0.285724</td>\n",
              "      <td>0.028520</td>\n",
              "      <td>0.568286</td>\n",
              "      <td>1</td>\n",
              "      <td>0.018998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0</td>\n",
              "      <td>0.469556</td>\n",
              "      <td>0.527202</td>\n",
              "      <td>0.522512</td>\n",
              "      <td>0.619402</td>\n",
              "      <td>0.619525</td>\n",
              "      <td>0.999045</td>\n",
              "      <td>0.797270</td>\n",
              "      <td>0.809203</td>\n",
              "      <td>0.303157</td>\n",
              "      <td>...</td>\n",
              "      <td>0.788126</td>\n",
              "      <td>0.005376</td>\n",
              "      <td>0.623540</td>\n",
              "      <td>0.619403</td>\n",
              "      <td>0.839285</td>\n",
              "      <td>0.281982</td>\n",
              "      <td>0.025749</td>\n",
              "      <td>0.198632</td>\n",
              "      <td>1</td>\n",
              "      <td>0.023491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>0</td>\n",
              "      <td>0.523717</td>\n",
              "      <td>0.561546</td>\n",
              "      <td>0.568178</td>\n",
              "      <td>0.626414</td>\n",
              "      <td>0.626414</td>\n",
              "      <td>0.999133</td>\n",
              "      <td>0.797526</td>\n",
              "      <td>0.809396</td>\n",
              "      <td>0.303420</td>\n",
              "      <td>...</td>\n",
              "      <td>0.811329</td>\n",
              "      <td>0.001088</td>\n",
              "      <td>0.623689</td>\n",
              "      <td>0.626413</td>\n",
              "      <td>0.841075</td>\n",
              "      <td>0.277759</td>\n",
              "      <td>0.026954</td>\n",
              "      <td>0.565831</td>\n",
              "      <td>1</td>\n",
              "      <td>0.041680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0</td>\n",
              "      <td>0.521913</td>\n",
              "      <td>0.564871</td>\n",
              "      <td>0.568178</td>\n",
              "      <td>0.609291</td>\n",
              "      <td>0.609241</td>\n",
              "      <td>0.999098</td>\n",
              "      <td>0.797498</td>\n",
              "      <td>0.809387</td>\n",
              "      <td>0.303445</td>\n",
              "      <td>...</td>\n",
              "      <td>0.812826</td>\n",
              "      <td>0.000729</td>\n",
              "      <td>0.624307</td>\n",
              "      <td>0.609288</td>\n",
              "      <td>0.840992</td>\n",
              "      <td>0.276193</td>\n",
              "      <td>0.026912</td>\n",
              "      <td>0.565675</td>\n",
              "      <td>1</td>\n",
              "      <td>0.074266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>0</td>\n",
              "      <td>0.503193</td>\n",
              "      <td>0.559693</td>\n",
              "      <td>0.550672</td>\n",
              "      <td>0.603590</td>\n",
              "      <td>0.603072</td>\n",
              "      <td>0.999001</td>\n",
              "      <td>0.797513</td>\n",
              "      <td>0.809398</td>\n",
              "      <td>0.303672</td>\n",
              "      <td>...</td>\n",
              "      <td>0.809661</td>\n",
              "      <td>0.001210</td>\n",
              "      <td>0.623755</td>\n",
              "      <td>0.603586</td>\n",
              "      <td>0.840973</td>\n",
              "      <td>0.277869</td>\n",
              "      <td>0.026991</td>\n",
              "      <td>0.565961</td>\n",
              "      <td>1</td>\n",
              "      <td>0.040601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>1</td>\n",
              "      <td>0.453030</td>\n",
              "      <td>0.516572</td>\n",
              "      <td>0.505595</td>\n",
              "      <td>0.596946</td>\n",
              "      <td>0.596953</td>\n",
              "      <td>0.998940</td>\n",
              "      <td>0.797084</td>\n",
              "      <td>0.809040</td>\n",
              "      <td>0.303052</td>\n",
              "      <td>...</td>\n",
              "      <td>0.777094</td>\n",
              "      <td>0.040161</td>\n",
              "      <td>0.617349</td>\n",
              "      <td>0.596946</td>\n",
              "      <td>0.837053</td>\n",
              "      <td>0.290904</td>\n",
              "      <td>0.026050</td>\n",
              "      <td>0.553328</td>\n",
              "      <td>1</td>\n",
              "      <td>0.016199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>1</td>\n",
              "      <td>0.472091</td>\n",
              "      <td>0.538814</td>\n",
              "      <td>0.517051</td>\n",
              "      <td>0.597393</td>\n",
              "      <td>0.597393</td>\n",
              "      <td>0.998960</td>\n",
              "      <td>0.797366</td>\n",
              "      <td>0.809290</td>\n",
              "      <td>0.303501</td>\n",
              "      <td>...</td>\n",
              "      <td>0.791982</td>\n",
              "      <td>0.007233</td>\n",
              "      <td>0.622323</td>\n",
              "      <td>0.597394</td>\n",
              "      <td>0.839162</td>\n",
              "      <td>0.300005</td>\n",
              "      <td>0.022762</td>\n",
              "      <td>0.571989</td>\n",
              "      <td>1</td>\n",
              "      <td>0.014054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>1</td>\n",
              "      <td>0.066933</td>\n",
              "      <td>0.057185</td>\n",
              "      <td>0.054821</td>\n",
              "      <td>0.601861</td>\n",
              "      <td>0.601861</td>\n",
              "      <td>0.998825</td>\n",
              "      <td>0.796779</td>\n",
              "      <td>0.808717</td>\n",
              "      <td>0.302760</td>\n",
              "      <td>...</td>\n",
              "      <td>0.525651</td>\n",
              "      <td>0.005803</td>\n",
              "      <td>0.623648</td>\n",
              "      <td>0.601857</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.182790</td>\n",
              "      <td>0.026763</td>\n",
              "      <td>0.565021</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>1</td>\n",
              "      <td>0.406669</td>\n",
              "      <td>0.467292</td>\n",
              "      <td>0.453450</td>\n",
              "      <td>0.595994</td>\n",
              "      <td>0.595994</td>\n",
              "      <td>0.998831</td>\n",
              "      <td>0.797191</td>\n",
              "      <td>0.809128</td>\n",
              "      <td>0.303466</td>\n",
              "      <td>...</td>\n",
              "      <td>0.746586</td>\n",
              "      <td>0.001053</td>\n",
              "      <td>0.623323</td>\n",
              "      <td>0.595994</td>\n",
              "      <td>0.831082</td>\n",
              "      <td>0.294222</td>\n",
              "      <td>0.026590</td>\n",
              "      <td>0.563976</td>\n",
              "      <td>1</td>\n",
              "      <td>0.015186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>0</td>\n",
              "      <td>0.482231</td>\n",
              "      <td>0.544320</td>\n",
              "      <td>0.536431</td>\n",
              "      <td>0.618083</td>\n",
              "      <td>0.618083</td>\n",
              "      <td>0.999004</td>\n",
              "      <td>0.797389</td>\n",
              "      <td>0.809315</td>\n",
              "      <td>0.303451</td>\n",
              "      <td>...</td>\n",
              "      <td>0.797585</td>\n",
              "      <td>0.014207</td>\n",
              "      <td>0.623644</td>\n",
              "      <td>0.618084</td>\n",
              "      <td>0.840280</td>\n",
              "      <td>0.289105</td>\n",
              "      <td>0.032353</td>\n",
              "      <td>0.569393</td>\n",
              "      <td>1</td>\n",
              "      <td>0.016943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>0</td>\n",
              "      <td>0.488519</td>\n",
              "      <td>0.550153</td>\n",
              "      <td>0.540339</td>\n",
              "      <td>0.600737</td>\n",
              "      <td>0.600758</td>\n",
              "      <td>0.998997</td>\n",
              "      <td>0.797409</td>\n",
              "      <td>0.809330</td>\n",
              "      <td>0.303502</td>\n",
              "      <td>...</td>\n",
              "      <td>0.804706</td>\n",
              "      <td>0.011073</td>\n",
              "      <td>0.623561</td>\n",
              "      <td>0.600737</td>\n",
              "      <td>0.840818</td>\n",
              "      <td>0.280855</td>\n",
              "      <td>0.027157</td>\n",
              "      <td>0.566455</td>\n",
              "      <td>1</td>\n",
              "      <td>0.025913</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60 rows  96 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be041c1c-868a-47ff-9b12-93cefee5d1de')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-be041c1c-868a-47ff-9b12-93cefee5d1de button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-be041c1c-868a-47ff-9b12-93cefee5d1de');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# open file\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "# drop any duplicate records\n",
        "data.drop_duplicates(inplace=True)\n",
        "# fill in missing values with mean values\n",
        "data.fillna(data.mean(), inplace=True)\n",
        "# remove outliers \n",
        "# Q1 = data.quantile(0.25)\n",
        "# Q3 = data.quantile(0.75)\n",
        "# IQR = Q3 - Q1\n",
        "# filtered_entries = ~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)\n",
        "# data = data[filtered_entries]\n",
        "# print(data.shape)\n",
        "(data.head(60))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use decision tree to determine the best number of features\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "features = data.iloc[:,:95]\n",
        "labels = data.iloc[:, 0:1]\n",
        "labels = labels.values.ravel()\n",
        "\n",
        "trainFeatures = []\n",
        "trainLabels = []\n",
        "testFeatures = []\n",
        "testLabels = []\n",
        "\n",
        "trainFeatures, testFeatures, trainLabels, testLabels = sk.model_selection.train_test_split(data.iloc[:, 0:95], data.iloc[:, 0:1], train_size = 0.8, test_size = 0.2, random_state = 42)\n",
        "\n",
        "\n",
        "decTreeTest = sk.tree.DecisionTreeClassifier(criterion = \"entropy\")\n",
        "decTreeTest = decTreeTest.fit(trainFeatures, trainLabels)\n",
        "\n",
        "param_grid = {\n",
        "              'max_features': [10, 20, 30, 40, 50, 60, 70, 80, 90, 95 ]\n",
        "              }\n",
        "\n",
        "# Create GridSearchCV object with 5-fold CV and 'accuracy' scoring\n",
        "grid_search = GridSearchCV(decTreeTest, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(data.iloc[:,0:95], labels)\n",
        "\n",
        "print(\"Best max_features: \", grid_search.best_params_['max_features'])\n"
      ],
      "metadata": {
        "id": "qLv6MPUyNCaK",
        "outputId": "c41ee5ae-144c-4b8f-96b4-fd6381a40273",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best max_features:  50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI-PFuN97f4m"
      },
      "source": [
        "### A. Data prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsKBB9Ge7f4n"
      },
      "source": [
        "Q1. All of the classifiers in `scikit-learn` require that you separate the feature columns from the class label column, so go ahead and do that first. You should end up with two separate data frames: one that contains all of the feature values and one that contains the class labels. \n",
        "\n",
        "Note: Later in this assignment, you may get a warning stating \"a column-vector was passed when a 1d array was expected.\" This indicates that some function wants a _flat array_ of labels, rather than a 2D DataFrame of labels. You can go ahead and transform the labels into a flat array here by doing either `labels.values.ravel()` or `labels.iloc[:,0]`. And you can just use that flat array for everything.\n",
        "\n",
        "Print the `shape` of your features data frame, the shape or len of your labels dataframe or array, and the `head` of the features data frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Kk4DCbz27f4n",
        "outputId": "25dcc388-735e-45ca-f457-6af9551a509c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of features data frame:  (1150, 19)\n",
            "shape of labels array:  (1150,)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>quality</th>\n",
              "      <th>prescreen</th>\n",
              "      <th>ma2</th>\n",
              "      <th>ma3</th>\n",
              "      <th>ma4</th>\n",
              "      <th>ma5</th>\n",
              "      <th>ma6</th>\n",
              "      <th>ma7</th>\n",
              "      <th>exudate8</th>\n",
              "      <th>exudate9</th>\n",
              "      <th>exudate10</th>\n",
              "      <th>exudate11</th>\n",
              "      <th>exudate12</th>\n",
              "      <th>exudate13</th>\n",
              "      <th>exudate14</th>\n",
              "      <th>exudate15</th>\n",
              "      <th>euDist</th>\n",
              "      <th>diameter</th>\n",
              "      <th>amfm_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>19</td>\n",
              "      <td>18</td>\n",
              "      <td>14</td>\n",
              "      <td>49.895756</td>\n",
              "      <td>17.775994</td>\n",
              "      <td>5.270920</td>\n",
              "      <td>0.771761</td>\n",
              "      <td>0.018632</td>\n",
              "      <td>0.006864</td>\n",
              "      <td>0.003923</td>\n",
              "      <td>0.003923</td>\n",
              "      <td>0.486903</td>\n",
              "      <td>0.100025</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>22</td>\n",
              "      <td>18</td>\n",
              "      <td>16</td>\n",
              "      <td>13</td>\n",
              "      <td>57.709936</td>\n",
              "      <td>23.799994</td>\n",
              "      <td>3.325423</td>\n",
              "      <td>0.234185</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.520908</td>\n",
              "      <td>0.144414</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>62</td>\n",
              "      <td>60</td>\n",
              "      <td>59</td>\n",
              "      <td>54</td>\n",
              "      <td>47</td>\n",
              "      <td>33</td>\n",
              "      <td>55.831441</td>\n",
              "      <td>27.993933</td>\n",
              "      <td>12.687485</td>\n",
              "      <td>4.852282</td>\n",
              "      <td>1.393889</td>\n",
              "      <td>0.373252</td>\n",
              "      <td>0.041817</td>\n",
              "      <td>0.007744</td>\n",
              "      <td>0.530904</td>\n",
              "      <td>0.128548</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>55</td>\n",
              "      <td>53</td>\n",
              "      <td>53</td>\n",
              "      <td>50</td>\n",
              "      <td>43</td>\n",
              "      <td>31</td>\n",
              "      <td>40.467228</td>\n",
              "      <td>18.445954</td>\n",
              "      <td>9.118901</td>\n",
              "      <td>3.079428</td>\n",
              "      <td>0.840261</td>\n",
              "      <td>0.272434</td>\n",
              "      <td>0.007653</td>\n",
              "      <td>0.001531</td>\n",
              "      <td>0.483284</td>\n",
              "      <td>0.114790</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>41</td>\n",
              "      <td>39</td>\n",
              "      <td>27</td>\n",
              "      <td>18.026254</td>\n",
              "      <td>8.570709</td>\n",
              "      <td>0.410381</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.475935</td>\n",
              "      <td>0.123572</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>44</td>\n",
              "      <td>43</td>\n",
              "      <td>41</td>\n",
              "      <td>41</td>\n",
              "      <td>37</td>\n",
              "      <td>29</td>\n",
              "      <td>28.356400</td>\n",
              "      <td>6.935636</td>\n",
              "      <td>2.305771</td>\n",
              "      <td>0.323724</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.502831</td>\n",
              "      <td>0.126741</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>29</td>\n",
              "      <td>29</td>\n",
              "      <td>27</td>\n",
              "      <td>25</td>\n",
              "      <td>16</td>\n",
              "      <td>15.448398</td>\n",
              "      <td>9.113819</td>\n",
              "      <td>1.633493</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.541743</td>\n",
              "      <td>0.139575</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>20.679649</td>\n",
              "      <td>9.497786</td>\n",
              "      <td>1.223660</td>\n",
              "      <td>0.150382</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.576318</td>\n",
              "      <td>0.071071</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>21</td>\n",
              "      <td>18</td>\n",
              "      <td>15</td>\n",
              "      <td>13</td>\n",
              "      <td>10</td>\n",
              "      <td>66.691933</td>\n",
              "      <td>23.545543</td>\n",
              "      <td>6.151117</td>\n",
              "      <td>0.496372</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500073</td>\n",
              "      <td>0.116793</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>79</td>\n",
              "      <td>75</td>\n",
              "      <td>73</td>\n",
              "      <td>71</td>\n",
              "      <td>64</td>\n",
              "      <td>47</td>\n",
              "      <td>22.141784</td>\n",
              "      <td>10.054384</td>\n",
              "      <td>0.874633</td>\n",
              "      <td>0.099780</td>\n",
              "      <td>0.023386</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.560959</td>\n",
              "      <td>0.109134</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   quality  prescreen  ma2  ma3  ma4  ma5  ma6  ma7   exudate8   exudate9  \\\n",
              "0        1          1   22   22   22   19   18   14  49.895756  17.775994   \n",
              "1        1          1   24   24   22   18   16   13  57.709936  23.799994   \n",
              "2        1          1   62   60   59   54   47   33  55.831441  27.993933   \n",
              "3        1          1   55   53   53   50   43   31  40.467228  18.445954   \n",
              "4        1          1   44   44   44   41   39   27  18.026254   8.570709   \n",
              "5        1          1   44   43   41   41   37   29  28.356400   6.935636   \n",
              "6        1          0   29   29   29   27   25   16  15.448398   9.113819   \n",
              "7        1          1    6    6    6    6    2    1  20.679649   9.497786   \n",
              "8        1          1   22   21   18   15   13   10  66.691933  23.545543   \n",
              "9        1          1   79   75   73   71   64   47  22.141784  10.054384   \n",
              "\n",
              "   exudate10  exudate11  exudate12  exudate13  exudate14  exudate15    euDist  \\\n",
              "0   5.270920   0.771761   0.018632   0.006864   0.003923   0.003923  0.486903   \n",
              "1   3.325423   0.234185   0.003903   0.003903   0.003903   0.003903  0.520908   \n",
              "2  12.687485   4.852282   1.393889   0.373252   0.041817   0.007744  0.530904   \n",
              "3   9.118901   3.079428   0.840261   0.272434   0.007653   0.001531  0.483284   \n",
              "4   0.410381   0.000000   0.000000   0.000000   0.000000   0.000000  0.475935   \n",
              "5   2.305771   0.323724   0.000000   0.000000   0.000000   0.000000  0.502831   \n",
              "6   1.633493   0.000000   0.000000   0.000000   0.000000   0.000000  0.541743   \n",
              "7   1.223660   0.150382   0.000000   0.000000   0.000000   0.000000  0.576318   \n",
              "8   6.151117   0.496372   0.000000   0.000000   0.000000   0.000000  0.500073   \n",
              "9   0.874633   0.099780   0.023386   0.000000   0.000000   0.000000  0.560959   \n",
              "\n",
              "   diameter  amfm_class  \n",
              "0  0.100025           1  \n",
              "1  0.144414           0  \n",
              "2  0.128548           0  \n",
              "3  0.114790           0  \n",
              "4  0.123572           0  \n",
              "5  0.126741           0  \n",
              "6  0.139575           0  \n",
              "7  0.071071           1  \n",
              "8  0.116793           0  \n",
              "9  0.109134           0  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# your code goes here\n",
        "class_labels = data[data.columns[19]]\n",
        "class_labels = class_labels.values.ravel()\n",
        "feature_vals = data[data.columns[0:19]]\n",
        "\n",
        "print(\"shape of features data frame: \", feature_vals.shape)\n",
        "print(\"shape of labels array: \", class_labels.shape)\n",
        "feature_vals.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eudijX_g7f4o"
      },
      "source": [
        "### B. Decision Trees (DT) & Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOcM224l7f4o"
      },
      "source": [
        "**Train/Test Split**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zozBELAD7f4o"
      },
      "source": [
        "Q2. You can train a classifier using the holdout method by splitting your data into a  training set and a  test set, then you can evaluate the classifier on the held-out test set. \n",
        "\n",
        "Let's try this with a decision tree classifier. \n",
        "\n",
        "* Use `sklearn.model_selection.train_test_split` to split your dataset into training and test sets (do an 80%-20% split). Display how many records are in the training set and how many are in the test set.\n",
        "* Use `sklearn.tree.DecisionTreeClassifier` to fit a decision tree classifier on the training set. Use entropy as the split criterion. \n",
        "* Now that the tree has been learned from the training data, we can run the test data through and predict classes for the test data. Use the `predict` method of `DecisionTreeClassifier` to classify the test data. \n",
        "* Then use `sklearn.metrics.accuracy_score` to print out the accuracy of the classifier on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHXn3BqK7f4o",
        "outputId": "5f6eaee5-291a-46e7-e1e4-84bd2e87f91a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "records in training set:  920\n",
            "records in test set:  230\n",
            "accuracy:  0.6173913043478261\n",
            "max_depth = \n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "features_train, features_test, labels_train, labels_test = model_selection.train_test_split(\n",
        "    feature_vals, class_labels, test_size=0.2)\n",
        "print(\"records in training set: \", len(features_train))\n",
        "print(\"records in test set: \", len(features_test))\n",
        "\n",
        "classifier = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
        "classifier.fit(features_train, labels_train)\n",
        "\n",
        "preds = classifier.predict(features_test)\n",
        "acc = metrics.accuracy_score(y_true=labels_test, y_pred=preds)\n",
        "print(\"accuracy: \", acc)\n",
        "\n",
        "print(\"max_depth = \", )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUPTeJ7a7f4p"
      },
      "source": [
        "Q3. Note that the DecisionTree classifier has many parameters that can be set. Try tweaking parameters like split criterion, max_depth, min_impurity_decrease, min_samples_leaf, min_samples_split, etc. to see how they affect accuracy. Print the accuracy of a few different variations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cETc8uxK7f4p",
        "outputId": "6ec48222-60ab-45ee-9222-58f7791242d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gini, random, max_depth=5 , min_samples_split=5 , accuracy:  0.6173913043478261\n",
            "entropy, best, max_depth=5 , min_samples_split=5 , accuracy:  0.6347826086956522\n",
            "entropy, best, max_depth=20 , min_samples_split=5 , accuracy:  0.6130434782608696\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "classifier.set_params(\n",
        "    criterion=\"gini\",\n",
        "    splitter=\"random\",\n",
        "    max_depth=5,\n",
        "    min_samples_split=5\n",
        ")\n",
        "classifier.fit(features_train, labels_train)\n",
        "preds = classifier.predict(features_test)\n",
        "acc = metrics.accuracy_score(y_true=labels_test, y_pred=preds)\n",
        "print(\"gini, random, max_depth=5 , min_samples_split=5 , accuracy: \", acc)\n",
        "\n",
        "classifier.set_params(\n",
        "    criterion=\"entropy\",\n",
        "    splitter=\"best\",\n",
        "    max_depth=5,\n",
        "    min_samples_split=5\n",
        ")\n",
        "classifier.fit(features_train, labels_train)\n",
        "preds = classifier.predict(features_test)\n",
        "acc = metrics.accuracy_score(y_true=labels_test, y_pred=preds)\n",
        "print(\"entropy, best, max_depth=5 , min_samples_split=5 , accuracy: \", acc)\n",
        "\n",
        "classifier.set_params(\n",
        "    criterion=\"entropy\",\n",
        "    splitter=\"best\",\n",
        "    max_depth=20,\n",
        "    min_samples_split=5\n",
        ")\n",
        "classifier.fit(features_train, labels_train)\n",
        "preds = classifier.predict(features_test)\n",
        "acc = metrics.accuracy_score(y_true=labels_test, y_pred=preds)\n",
        "print(\"entropy, best, max_depth=20 , min_samples_split=5 , accuracy: \", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFhluJzO7f4q"
      },
      "source": [
        "**Cross Validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akO_PpYF7f4q"
      },
      "source": [
        "Q4. You have now built a decision tree and tested it's accuracy using the \"holdout\" method. But as discussed in class, this is not sufficient for estimating generalization accuracy. Instead, we should use Cross Validation to get a better estimate of accuracy. \n",
        "\n",
        "Use `sklearn.model_selection.cross_val_score` to perform 10-fold cross validation on a decision tree. You will pass the FULL dataset into `cross_val_score` which will automatically divide it into the number of folds you tell it to, train a decision tree model on the training set for each fold, and test it on the test set for each fold. It will return a numpy array with the accuracy out of each fold. Average these accuracies to print out the generalization accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "0hweuAef7f4q",
        "outputId": "c99c0d58-cc7a-477a-f93a-03ec6a4d0792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.6313043478260869\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "accs = model_selection.cross_val_score(\n",
        "    estimator=classifier,\n",
        "    X=feature_vals,\n",
        "    y=class_labels,\n",
        "    cv=10\n",
        "    \n",
        ")\n",
        "acc = np.mean(accs)\n",
        "print(\"accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Q0F9_J7f4q"
      },
      "source": [
        "**Nested Cross Validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQq0TAli7f4r"
      },
      "source": [
        "Q5. Now we want to tune our model to use the best parameters to avoid overfitting to our training data. Grid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters (hyperparameters) specified in a grid. \n",
        "* Use `sklearn.model_selection.GridSearchCV` to find the best `max_depth`, `max_features`, and `min_samples_leaf` for your tree. Use a 5-fold-CV and 'accuracy' for the scoring criteria.\n",
        "* Try the values [5,10,15,20] for `max_depth` and `min_samples_leaf`. Try [5,10,15] for `max_features`. \n",
        "* Print out the best value for each of the tested parameters (`best_params_`).\n",
        "* Print out the accuracy of the model with these best values (`best_score_`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMFgq1kh7f4r",
        "outputId": "2c703957-e318-4a51-96ec-e562accbc0e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'max_depth': 20, 'max_features': 15, 'min_samples_leaf': 20}\n",
            "0.6504347826086956\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "grid = model_selection.GridSearchCV(\n",
        "    estimator=classifier,\n",
        "    param_grid={\n",
        "        \"max_depth\": [5,10,15,20],\n",
        "        \"min_samples_leaf\": [5,10,15,20],\n",
        "        \"max_features\":  [5,10,15]\n",
        "    },\n",
        "    scoring=\"accuracy\",\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "grid.fit(feature_vals, class_labels)\n",
        "\n",
        "print(grid.best_params_)\n",
        "print(grid.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izMZ0lA-7f4r"
      },
      "source": [
        "Q6. What you did in Q5 performed the _inner_ loop of a nested CV (no test set was held out). What you did in Q4 performed an _outer_ loop of CV (holds out a test set). Now we need to combine them to perform the nested cross-validation that we discussed in class. To do this, you'll need to pass the a `GridSearchCV` into a `cross_val_score`. \n",
        "\n",
        "What this does is: the `cross_val_score` splits the data in to train and test sets for the first outer fold, and it passes the train set into `GridSearchCV`. `GridSearchCV` then splits that set into train and validation sets for k number of folds (the inner CV loop). The hyper-parameters for which the average score over all inner iterations is best, is reported as the `best_params_`, `best_score_`, and `best_estimator_`(best decision tree). This best decision tree is then evaluated with the test set from the `cross_val_score` (the outer CV loop). And this whole thing is repeated for the remaining k folds of the `cross_val_score` (the outer CV loop). \n",
        "\n",
        "That is a lot of explanation for a very complex (but IMPORTANT) process, which can all be performed with a single line of code!\n",
        "\n",
        "Be patient for this one to run. The nested cross-validation loop can take some time. A [ * ] next to the cell indicates that it is still running.\n",
        "\n",
        "Print the accuracy of your tuned, cross-validated model. This is the official accuracy that you would report for your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBT2kSAW7f4r",
        "outputId": "74831478-7632-4dfd-ae87-38e8512d45e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cross_val_score:  0.6078260869565216\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "acc = model_selection.cross_val_score(\n",
        "    estimator=grid,\n",
        "    X=feature_vals,\n",
        "    y=class_labels,\n",
        "    cv=10\n",
        ")\n",
        "\n",
        "print(\"cross_val_score: \", np.mean(acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEukPPgq7f4s"
      },
      "source": [
        "### C. Naive Bayes (NB) & Evaluation Metrics\n",
        "\n",
        "`sklearn.naive_bayes.GaussianNB` implements the Gaussian Naive Bayes algorithm for classification. This means that the liklihood of continuous features is estimated using a Gaussian distribution. (Refer to slide 13 of the Naive Bayes powerpoint notes.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEyo34kr7f4s"
      },
      "source": [
        "Q7. Create a `sklearn.naive_bayes.GaussianNB` classifier. Use `sklearn.model_selection.cross_val_score` to do a 10-fold cross validation on the classifier. Display the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY8tkteY7f4s",
        "outputId": "c2b07c1e-b37f-4951-8ebc-64c7854220b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy:  0.5947826086956522\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "nb_clf = naive_bayes.GaussianNB()\n",
        "# nb_clf.fit(feature_vals, class_labels)\n",
        "accs = model_selection.cross_val_score(\n",
        "    estimator=nb_clf,\n",
        "    X=feature_vals,\n",
        "    y=class_labels,\n",
        "    cv=10\n",
        "    \n",
        ")\n",
        "\n",
        "acc = np.mean(accs)\n",
        "print(\"accuracy: \", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ6O9h-F7f4t"
      },
      "source": [
        "Q8. `cross_val_score` returns the scores of every test fold. There is another function called `cross_val_predict` that returns predicted y values for every record in the test fold. In other words, for each element in the input, `cross_val_predict` returns the prediction that was obtained for that element when it was in the test set. \n",
        "\n",
        "* Use `cross_val_predict` and `sklearn.metrics.confusion_matrix` to print the confusion matrix for the classifier.\n",
        "\n",
        "* Sckit-learn also provides a useful function `sklearn.metrics.classification_report` for evaluating the classifier on a per-class basis. It is a summary of the precision, recall, and F1 score for each class (and support is just the actual class count). Display the classification report for your Naive Bayes classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWoYN4QB7f4t",
        "outputId": "cc75aa57-9543-42c9-f787-b9b4cb056025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix:\n",
            " [[500  39]\n",
            " [427 184]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.93      0.68       539\n",
            "           1       0.83      0.30      0.44       611\n",
            "\n",
            "    accuracy                           0.59      1150\n",
            "   macro avg       0.68      0.61      0.56      1150\n",
            "weighted avg       0.69      0.59      0.55      1150\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "cross_val_preds = model_selection.cross_val_predict(\n",
        "    estimator=nb_clf, \n",
        "    X=feature_vals, \n",
        "    y=class_labels, \n",
        "    cv=10\n",
        ")\n",
        "confusion = metrics.confusion_matrix(y_true=class_labels, y_pred=cross_val_preds)\n",
        "\n",
        "report = metrics.classification_report(\n",
        "    y_true=class_labels,\n",
        "    y_pred=cross_val_preds,\n",
        ")\n",
        "\n",
        "print(\"confusion matrix:\\n\", confusion)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W1aQQdy7f4t"
      },
      "source": [
        "Q9. Using `sklearn.metrics.roc_curve` plot a ROC curve for the Naive Bayes classifier. Also calculate the area under the curve (AUC) using `sklearn.metrics.roc_auc_score`.\n",
        "\n",
        "* We will just do this on a single holdout test set (because it gets more complicated to put this inside of a cross-validation). So, split your data into training and test sets using `sklearn.model_selection.train_test_split`. Do an 80/20 split.\n",
        "* Fit the Naive Bayes classifier to the training data by calling the `fit` method on the trainng data.\n",
        "* Now call the `predict_proba` method on your classifier and pass in the test data. This will return a 2D numpy array with one row for each datapoint in the test set and 2 columns. Column index 0 is the probability that this datapoint is in class 0, and column index 1 is the probability that this datapoint is in class 1.\n",
        "* We are going to say that class 1 (having the disease) is the rare/positive class. To create a ROC curve, pass the actual Y labels and the probabilites of class 1 (column index 1 out of your predict_proba result) into `sklearn.metrics.roc_curve`\n",
        "* Pass the FPR and TPR that `roc_curve` returns into the plotting code that we have provided you.\n",
        "* Print the AUC (area under the curve) by using `sklearn.metrics.roc_auc_score`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqzdNCxN7f4u",
        "outputId": "fed18b4d-d1b9-4ac7-9fe7-433c9479905f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "auc score:  0.6464153228859112\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# your code goes here\n",
        "# using features_{train, test} and labels_{train, test} from previous part\n",
        "nb_clf.fit(X=features_train, y=labels_train)\n",
        "\n",
        "probas = nb_clf.predict_proba(features_test)\n",
        "\n",
        "#replace these fpr and tpr with the results of your roc_curve\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_true=labels_test, y_score=probas[:,1])\n",
        "\n",
        "auc_score = metrics.roc_auc_score(y_true=labels_test, y_score=probas[:,1])\n",
        "print(\"auc score: \",  auc_score)\n",
        "\n",
        "# Do not change this code! This plots the ROC curve.\n",
        "# Just replace the fpr and tpr above with the values from your roc_curve\n",
        "plt.plot([0,1],[0,1],'k--') #plot the diagonal line\n",
        "plt.plot(fpr, tpr, label='NB') #plot the ROC curve\n",
        "plt.xlabel('fpr')\n",
        "plt.ylabel('tpr')\n",
        "plt.title('ROC Curve Naive Bayes')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqnIKNX77f4u"
      },
      "source": [
        "### D. k-Nearest Neighbor (KNN) & Pipelines "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwW3ewrB7f4u"
      },
      "source": [
        "For some classification algorithms, scaling of the data is critical (like KNN, SVM, Neural Nets). For other classification algorithms, data scaling is not necessary (like Naive Bayes and Decision Trees). _Take a minute to think about why this is the case!!_ But using scaled data with an algorithm that doesn't explicitly need it to be scaled does not hurt the results of that algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVX-sGHI7f4u"
      },
      "source": [
        "Q10. The distance calculation method is central to the KNN algorithm. By default, `KNeighborsClassifier` uses  Euclidean distance as its metric (but this can be changed). Because of the distance calculations, it is critical to scale the data before running Nearest Neighbor!\n",
        "\n",
        "We discussed why dimensionality reduction may also be needed with KNN because of the curse of dimensionality. So we may want to also perform a dimensionality reduction with PCA before running KNN. PCA should only be performed on scaled data! (Remember that you can also reduce dimensionality by performing feature selection and feature engineering.) \n",
        "\n",
        "An important note about scaling data and dimensionality reduction is that they should only be performed on the **training** data, then you transform the test data into the scaled, PCA space that was found on the training data. (Refer to the concept of [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/).)\n",
        "\n",
        "So when you are doing cross-validation, the scaling and PCA needs to happen *inside of your CV loop*. This way, it is performed on the training set for the first fold, then the test set is put into that space. On the second fold, it is performed on the trainng set for the second fold, and the test set is put into that space. And so on for the remaining folds. \n",
        "\n",
        "In order to do this with scikit-learn, you must create what's called a `Pipeline` and pass that in to the cross validation. This is a very important concept for Data Mining and Machine Learning, so let's practice it here.\n",
        "\n",
        "Do the following:\n",
        "* Create a `sklearn.preprocessing.StandardScaler` object to standardize the datasets features (mean = 0 and variance = 1). (Do not call `fit` on it yet. Just create the `StandardScaler` object.)\n",
        "* Create a `sklearn.decomposition.PCA` object to perform PCA dimensionality reduction. (Do not call `fit` on it yet. Just create the `PCA` object.)\n",
        "* Create a `sklearn.neighbors.KNeighborsClassifier`. The number of neighbors defaults to 5 (k=5). Go ahead and change it to 7. (Do not call `fit` on it yet. Just create the `KNeighborsClassifier` object.)\n",
        "* Create a `sklearn.pipeline.Pipeline` object and set the `steps` to the scaler, the PCA, and the KNN objects that you just created. \n",
        "* Pass the `pipeline` object in to a `cross_val_score` as the estimator, along with the features and the labels, and use a 5-fold-CV. \n",
        "\n",
        "In each fold of the cross validation, the training phase will use _only_ the training data for scaling, PCA, and training the model. Then the testing phase will scale & transform the test data into the PCA space (found on the training data) and run the test data through the trained classifier, to return an accuracy measurement for each fold. Print the average accuracy across all 5 folds. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljGI2QSw7f4v",
        "outputId": "2e7fd7c9-c60f-4229-c12f-af297b240190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.62608696 0.64782609 0.64347826 0.59130435 0.5826087 ]\n",
            "average accuracy:  0.6182608695652174\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "scaler = preprocessing.StandardScaler()\n",
        "pca = decomposition.PCA()\n",
        "knn_clf = neighbors.KNeighborsClassifier(n_neighbors=7)\n",
        "pipe = pipeline.Pipeline(steps=[('scaler', scaler), ('pca', pca), ('knn_classifier', knn_clf)])\n",
        "accs = model_selection.cross_val_score(pipe, feature_vals, class_labels, cv=5)\n",
        "\n",
        "print(accs)\n",
        "print(\"average accuracy: \", np.mean(accs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1ow7eM57f4v"
      },
      "source": [
        "Q11. Another important part of KNN is choosing the best number of neighbors (tuning the hyperparameter, k). We can use nested cross validation to do this. Let's try k values from 1-25 to find the best one. \n",
        "\n",
        "We _also_ want to find the best number of dimensions to project down onto using PCA. We can use nested cross validation to do this as well. Let's try from 5-19 dimensions.\n",
        "\n",
        "* Starter code is provided to create the \"parameter grid\" to search. You will need to change this code! Where I have \"knn__n_neighbors\", this indicates that I want to tune the \"n_neighbors\" parameter in the \"knn\" part of the pipeline. When you created your pipeline above, you named the KNN part of the pipeline with a string. You should replace \"knn\" in the param_grid below with whatever you named your KNN part of the pipeline: **<replace_this>__n_neighbors.** Do the same for the PCA part of the pipeline.\n",
        "* Create a `sklearn.model_selection.GridSearchCV` and pass in the pipeline, the param_grid, and set it to a 5-fold-CV.\n",
        "* Now, on that `GridSearchCV` object, call `fit` and pass in the features and labels.\n",
        "* Show the best number of dimensions and best number of neighbors for this dataset by printing the `best_params_` from the `GridSearchCV`.\n",
        "* Also print the accuracy when using this best number of dimensions and neighbors by printing the `best_score_` from the `GridSearchCV`.\n",
        "\n",
        "Be patient, this can take some time to run. It is trying every combination of dimensions from 5-19 with every k from 1-25! A [ * ] next to the cell indicates that it is still running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bNTI7aN7f4v",
        "outputId": "d3f473bb-fa73-4e4b-ed63-ee5f7d6dc795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best params:  {'knn_classifier__n_neighbors': 23, 'pca__n_components': 14}\n",
            "best score:  0.6617391304347826\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "On the \"pca\" part of the pipeline, \n",
        "tune the n_components parameter,\n",
        "by trying the values 1-19.\n",
        "\n",
        "On the \"knn\" part of the pipeline, \n",
        "tune the n_neighbors parameter,\n",
        "by trying the values 1-25.\n",
        "'''\n",
        "param_grid = {\n",
        "    'pca__n_components': list(range(5, 19)),\n",
        "    'knn_classifier__n_neighbors': list(range(1, 25))\n",
        "}\n",
        "\n",
        "# your code goes here\n",
        "grid = model_selection.GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=5\n",
        ")\n",
        "grid.fit(feature_vals, class_labels)\n",
        "\n",
        "print(\"best params: \", grid.best_params_)\n",
        "print(\"best score: \", grid.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc6d_8147f4w"
      },
      "source": [
        "Q12. In Q11, we did not hold out a test set. The accuracy reported out is on the _validation_ set. So now we need to wrap the whole process in another cross-validation to perform a nested cross-validation and report the accuarcy of this KNN model on unseen test data. This is the official accuracy you would report on this model.\n",
        "\n",
        "You'll need to pass the `GridSearchCV` into a `cross_val_score`, just as you did with the decision tree. Use a 5-fold-CV for the outer loop. \n",
        "\n",
        "Again, be patient for this one to run. The nested cross-validation loop can take some time. It is doing what it did above in Q11 five times. A [ * ] next to the cell indicates that it is still running. (Just for comparison, mine takes about 2 mins to run and the fan revs up so it sounds like my computer is going to explode. All computers are different, so yours could take shorter or longer...)\n",
        "\n",
        "<img src=\"https://github.com/attruong00/ml-final/blob/main/model_is_training.png?raw=1\" width=\"250\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm9HU0437f4w",
        "outputId": "74908952-d51b-4992-fc60-e9eff8041751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.59130435 0.68695652 0.66521739 0.61304348 0.65217391]\n",
            "average accuracy on test data:  0.6417391304347827\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "official_accuracy = model_selection.cross_val_score(grid, feature_vals, class_labels, cv=5)\n",
        "print(official_accuracy)\n",
        "print(\"average accuracy on test data: \", np.mean(official_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAexCNdW7f4w"
      },
      "source": [
        "### E. Support Vector Machines (SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbkUKT2A7f4w"
      },
      "source": [
        "Q13. Now put it all together with an SVM. \n",
        "* Create a `pipeline` that includes scaling, PCA, and an `sklearn.svm.SVC`.\n",
        "* Create a parameter grid that tries number of dimensions from 5-19 and SVM kernels `linear`, `rbf` and `poly`.\n",
        "* Create a `GridSearchCV` for the inner CV loop. Use a 5-fold CV.\n",
        "* Run a `cross_val_predict` with a 10-fold CV for the outer loop. \n",
        "* Print out the accuracy and the classification report of using an SVM classifier on this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGsrEuvl7f4x",
        "outputId": "946ea778-be74-475b-ae7e-696a08f87394"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy:  0.72\n",
            "report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.84      0.74       539\n",
            "           1       0.81      0.62      0.70       611\n",
            "\n",
            "    accuracy                           0.72      1150\n",
            "   macro avg       0.74      0.73      0.72      1150\n",
            "weighted avg       0.74      0.72      0.72      1150\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "pipe = pipeline.Pipeline(steps=[\n",
        "    (\"scaler\", preprocessing.StandardScaler()), \n",
        "    (\"pca\", decomposition.PCA()),\n",
        "    (\"svc\", svm.SVC())\n",
        "])\n",
        "param_grid = {\n",
        "    \"pca__n_components\": list(range(5, 19)),\n",
        "    \"svc__kernel\": [\"linear\", \"rbf\", \"poly\"]\n",
        "}\n",
        "\n",
        "grid = model_selection.GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "cross_val_preds = model_selection.cross_val_predict(grid, feature_vals, class_labels,cv=10)\n",
        "accuracy = metrics.accuracy_score(y_true=class_labels, y_pred=cross_val_preds)\n",
        "print(\"accuracy: \", accuracy)\n",
        "\n",
        "report = report = metrics.classification_report(\n",
        "    y_true=class_labels,\n",
        "    y_pred=cross_val_preds,\n",
        ")\n",
        "print(\"report:\\n\", report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qosyoO5a7f4x"
      },
      "source": [
        "### F. Neural Networks (NN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cih316ki7f4x"
      },
      "source": [
        "Q14. Train a multi-layer perceptron with a single hidden layer using `sklearn.neural_network.MLPClassifier`. \n",
        "* Create a pipeline with scaling and a neural net. (No PCA on this one. But scaling is critical to neural nets.)\n",
        "* Use `GridSearchCV` with 5 fold cross validation to find the best hidden layer size and the best activation function. \n",
        "* Try values of `hidden_layer_sizes` ranging from `(30,)` to `(60,)` by increments of 10.\n",
        "* Try activation functions `logistic`, `tanh`, `relu`.\n",
        "* Wrap your `GridSearchCV` in a 5-fold `cross_val_score` and report the accuracy of your neural net.\n",
        "\n",
        "Be patient, as this can take a few minutes to run. You may get ConvergenceWarnings as it runs - that is fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "et6NXbz17f4x"
      },
      "outputs": [],
      "source": [
        "# your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe8U5AlP7f4y"
      },
      "source": [
        "### G. Ensemble Classifiers\n",
        "\n",
        "Ensemble classifiers combine the predictions of multiple base estimators to improve the accuracy of the predictions. One of the key assumptions that ensemble classifiers make is that the base estimators are built independently (so they are diverse)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_LeUwU67f4y"
      },
      "source": [
        "**Random Forests**\n",
        "\n",
        "Q15. Use `sklearn.ensemble.RandomForestClassifier` to classify the data. Scaling the data is not necessary for Decision Trees (take a minute to think about why). So, no need for a pipeline here.\n",
        "\n",
        "The default for RandomForest is to use 100 fully-grown decision trees. Let's use a `GridSearchCV` with a 5-fold CV to try various numbers of base classifiers and select the one with the best results. \n",
        "\n",
        "Try `n_estimators` of 50, 100, and 150 - this is the number of base classifiers in the ensemble. Wrap your GridSearchCV in a cross_val_score with 5-fold CV. Display the classification report. \n",
        "\n",
        "Note that this does get a higher accuracy than a single decision tree!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRq62ewJ7f4y"
      },
      "outputs": [],
      "source": [
        "# your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txjepgms7f4y"
      },
      "source": [
        "**AdaBoost**\n",
        "\n",
        "Random Forests builds its base estimators independently, using bagging. There is another method of training ensemble classifiers called boosting. Here the classifiers are trained sequentially and each time the sampling of the training set depends on the performance of previously generated models.\n",
        "\n",
        "Q16. Evaluate a `sklearn.ensemble.AdaBoostClassifier` classifier on the data. By default, `AdaBoostClassifier` uses 50 decision stumps as the base classifiers. Let's again use a `GridSearchCV` with a 5-fold CV to try various numbers of base classifiers.\n",
        "\n",
        "Try `n_estimators` of 50, 100, and 150 - this is the number of base classifiers in the ensemble. Wrap your GridSearchCV in a cross_val_score with 5-fold CV. Display the classification report.\n",
        "\n",
        "Note that even when using decision stumps as the base classifier, this gets a higher accuracy than a single decision tree!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6hDdCHz7f4y"
      },
      "outputs": [],
      "source": [
        "# your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14fIzV8K7f4y"
      },
      "source": [
        "### H. Build your final model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h423gikj7f4z"
      },
      "source": [
        "Now you have tested all kinds of classifiers on this data. Some have performed better than others. \n",
        "\n",
        "Q17. We may not want to deploy any of these models in the real world to actually diagnose patients because the accuracies are not high enough. What can we do to improve the accuracy rates? Answer as a comment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfiNUb1T7f4z"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Answer here as a comment.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83-hbXfz7f4z"
      },
      "source": [
        "Q18. Let's say we *did* get to the point where we had a model with high enough accuracy and we want to deploy that model and use it for real-world predictions.\n",
        "\n",
        "* Let's say we're going to deploy our SVM classifier.\n",
        "* We need to make one final version of this model, where we use ALL of our available data for training (we do not hold out a test set this time, so no outer cross-validation loop). \n",
        "* We need to tune the parameters of the model on the FULL dataset, so copy the code you entered for Q13, but remove the outer cross validation loop (remove `cross_val_score`). Just run the `GridSearchCV` by calling `fit` on it and passing in the full dataset. This results in the final trained model with the best parameters for the full dataset. You can print out `best_params_` to see what they are.\n",
        "* The accuracy of this model is what you assessed and reported in Q13.\n",
        "\n",
        "\n",
        "* Use the `pickle` package to save your model. We have provided the lines of code for you, just make sure your final model gets passed in to `pickle.dump()`. This will save your model to a file called finalized_model.sav in your current working directory. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DTnEiwH7f4z"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# your code goes here\n",
        "\n",
        "#replace this final_model with your final model\n",
        "final_model = None\n",
        "\n",
        "filename = 'finalized_model.sav'\n",
        "pickle.dump(final_model, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmbORF2a7f4z"
      },
      "source": [
        "Q19. Now if someone wants to use your trained, saved classifier to classify a new record, they can load the saved model and just call `predict` on it. \n",
        "* Given this new record, classify it with your saved model and print out either \"Negative for disease\" or \"Positive for disease.\"\n",
        "* Note that `predict` is expecting a list of lists (a list of records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM0oXqZ47f40"
      },
      "outputs": [],
      "source": [
        "# some time later...\n",
        "\n",
        "# use this as the new record to classify\n",
        "record = [ 0.05905386, 0.2982129, 0.68613149, 0.75078865, 0.87119216, 0.88615694,\n",
        "  0.93600623, 0.98369184, -0.47426472, -0.57642756, -0.53115361, -0.42789774,\n",
        " -0.21907738, -0.20090532, -0.21496782, -0.2080998, 0.06692373, -2.81681183,\n",
        " -0.7117194 ]\n",
        "\n",
        " \n",
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "# your code goes here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}