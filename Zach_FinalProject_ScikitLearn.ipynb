{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/attruong00/ml-final/blob/main/Zach_FinalProject_ScikitLearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4zhagsD7f4h"
      },
      "outputs": [],
      "source": [
        "# Enter your name(s) here\n",
        "# Abbey Truong att837\n",
        "# Ben Truong bst574\n",
        "# Kamil Kalowski ktk582\n",
        "# Zach Cramer zc5455"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJQlHAyV7f4j"
      },
      "source": [
        "# Using Machine Learning to Predict Company Bankruptcy  \n",
        "\n",
        "In this project we employ a variety of statistical methods in an effort to predict whether or not a company will go bankrupt. The data was collected from the Taiwann Economic Journal between 1999 and 2009. Bankruptcy was determined on metrics outlined by the business regulations of the Taiwan Stock Exchange.\n",
        "\n",
        "Dataset: https://www.kaggle.com/datasets/fedesoriano/company-bankruptcy-prediction?resource=download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NCCCd4gv7f4l"
      },
      "outputs": [],
      "source": [
        "#You may add additional imports\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "import pandas as pd\n",
        "import statistics\n",
        "import numpy as np\n",
        "import sklearn as sk\n",
        "from sklearn import model_selection, tree, metrics, naive_bayes, preprocessing, decomposition, neighbors, pipeline, svm\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8-fNADJzzy9"
      },
      "source": [
        "# Data Preparation \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_X-z_l1n7f4l"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ivlcqTtq7f4l",
        "outputId": "761a087f-3505-4551-ccf6-4f270b9dfb07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Bankrupt?   ROA(C) before interest and depreciation before interest  \\\n",
              "0           1                                           0.370594          \n",
              "1           1                                           0.464291          \n",
              "2           1                                           0.426071          \n",
              "3           1                                           0.399844          \n",
              "4           1                                           0.465022          \n",
              "5           1                                           0.388680          \n",
              "6           0                                           0.390923          \n",
              "7           0                                           0.508361          \n",
              "8           0                                           0.488519          \n",
              "9           0                                           0.495686          \n",
              "10          0                                           0.482475          \n",
              "11          0                                           0.444401          \n",
              "12          0                                           0.491152          \n",
              "13          0                                           0.474041          \n",
              "14          0                                           0.506703          \n",
              "15          0                                           0.513821          \n",
              "16          0                                           0.488909          \n",
              "17          0                                           0.535953          \n",
              "18          0                                           0.504071          \n",
              "19          0                                           0.487398          \n",
              "20          0                                           0.485253          \n",
              "21          0                                           0.504558          \n",
              "22          0                                           0.512017          \n",
              "23          0                                           0.494857          \n",
              "24          0                                           0.509969          \n",
              "25          0                                           0.444986          \n",
              "26          0                                           0.519280          \n",
              "27          0                                           0.547409          \n",
              "28          0                                           0.500853          \n",
              "29          1                                           0.416126          \n",
              "30          1                                           0.462195          \n",
              "31          0                                           0.476088          \n",
              "32          0                                           0.505874          \n",
              "33          0                                           0.486374          \n",
              "34          0                                           0.494369          \n",
              "35          0                                           0.475162          \n",
              "36          0                                           0.462926          \n",
              "37          0                                           0.478916          \n",
              "38          0                                           0.481061          \n",
              "39          0                                           0.486228          \n",
              "40          0                                           0.507824          \n",
              "41          0                                           0.489153          \n",
              "42          0                                           0.511188          \n",
              "43          0                                           0.523034          \n",
              "44          0                                           0.508848          \n",
              "45          0                                           0.488958          \n",
              "46          0                                           0.502121          \n",
              "47          0                                           0.523083          \n",
              "48          0                                           0.493346          \n",
              "49          0                                           0.505631          \n",
              "50          0                                           0.469556          \n",
              "51          0                                           0.523717          \n",
              "52          0                                           0.521913          \n",
              "53          0                                           0.503193          \n",
              "54          1                                           0.453030          \n",
              "55          1                                           0.472091          \n",
              "56          1                                           0.066933          \n",
              "57          1                                           0.406669          \n",
              "58          0                                           0.482231          \n",
              "59          0                                           0.488519          \n",
              "\n",
              "     ROA(A) before interest and % after tax  \\\n",
              "0                                  0.424389   \n",
              "1                                  0.538214   \n",
              "2                                  0.499019   \n",
              "3                                  0.451265   \n",
              "4                                  0.538432   \n",
              "5                                  0.415177   \n",
              "6                                  0.445704   \n",
              "7                                  0.570922   \n",
              "8                                  0.545137   \n",
              "9                                  0.550916   \n",
              "10                                 0.567543   \n",
              "11                                 0.549717   \n",
              "12                                 0.551570   \n",
              "13                                 0.533308   \n",
              "14                                 0.575829   \n",
              "15                                 0.571086   \n",
              "16                                 0.560238   \n",
              "17                                 0.590438   \n",
              "18                                 0.559802   \n",
              "19                                 0.543720   \n",
              "20                                 0.545573   \n",
              "21                                 0.564490   \n",
              "22                                 0.563672   \n",
              "23                                 0.548136   \n",
              "24                                 0.561382   \n",
              "25                                 0.503652   \n",
              "26                                 0.563618   \n",
              "27                                 0.593055   \n",
              "28                                 0.563945   \n",
              "29                                 0.470235   \n",
              "30                                 0.536034   \n",
              "31                                 0.544483   \n",
              "32                                 0.570704   \n",
              "33                                 0.544756   \n",
              "34                                 0.550916   \n",
              "35                                 0.564544   \n",
              "36                                 0.516354   \n",
              "37                                 0.541321   \n",
              "38                                 0.539468   \n",
              "39                                 0.548027   \n",
              "40                                 0.571631   \n",
              "41                                 0.562691   \n",
              "42                                 0.566888   \n",
              "43                                 0.575883   \n",
              "44                                 0.553423   \n",
              "45                                 0.549008   \n",
              "46                                 0.563509   \n",
              "47                                 0.563345   \n",
              "48                                 0.550534   \n",
              "49                                 0.559747   \n",
              "50                                 0.527202   \n",
              "51                                 0.561546   \n",
              "52                                 0.564871   \n",
              "53                                 0.559693   \n",
              "54                                 0.516572   \n",
              "55                                 0.538814   \n",
              "56                                 0.057185   \n",
              "57                                 0.467292   \n",
              "58                                 0.544320   \n",
              "59                                 0.550153   \n",
              "\n",
              "     ROA(B) before interest and depreciation after tax  \\\n",
              "0                                            0.405750    \n",
              "1                                            0.516730    \n",
              "2                                            0.472295    \n",
              "3                                            0.457733    \n",
              "4                                            0.522298    \n",
              "5                                            0.419134    \n",
              "6                                            0.436158    \n",
              "7                                            0.559077    \n",
              "8                                            0.543284    \n",
              "9                                            0.542963    \n",
              "10                                           0.538198    \n",
              "11                                           0.498956    \n",
              "12                                           0.543391    \n",
              "13                                           0.523690    \n",
              "14                                           0.569838    \n",
              "15                                           0.558756    \n",
              "16                                           0.540286    \n",
              "17                                           0.580920    \n",
              "18                                           0.558649    \n",
              "19                                           0.533647    \n",
              "20                                           0.534665    \n",
              "21                                           0.553027    \n",
              "22                                           0.569035    \n",
              "23                                           0.540446    \n",
              "24                                           0.552599    \n",
              "25                                           0.495530    \n",
              "26                                           0.568606    \n",
              "27                                           0.592484    \n",
              "28                                           0.550565    \n",
              "29                                           0.463783    \n",
              "30                                           0.514428    \n",
              "31                                           0.529686    \n",
              "32                                           0.559827    \n",
              "33                                           0.540500    \n",
              "34                                           0.548584    \n",
              "35                                           0.525028    \n",
              "36                                           0.517105    \n",
              "37                                           0.529311    \n",
              "38                                           0.530864    \n",
              "39                                           0.544140    \n",
              "40                                           0.549226    \n",
              "41                                           0.541517    \n",
              "42                                           0.559077    \n",
              "43                                           0.576423    \n",
              "44                                           0.552920    \n",
              "45                                           0.538947    \n",
              "46                                           0.545854    \n",
              "47                                           0.572354    \n",
              "48                                           0.539804    \n",
              "49                                           0.549173    \n",
              "50                                           0.522512    \n",
              "51                                           0.568178    \n",
              "52                                           0.568178    \n",
              "53                                           0.550672    \n",
              "54                                           0.505595    \n",
              "55                                           0.517051    \n",
              "56                                           0.054821    \n",
              "57                                           0.453450    \n",
              "58                                           0.536431    \n",
              "59                                           0.540339    \n",
              "\n",
              "     Operating Gross Margin   Realized Sales Gross Margin  \\\n",
              "0                  0.601457                      0.601457   \n",
              "1                  0.610235                      0.610235   \n",
              "2                  0.601450                      0.601364   \n",
              "3                  0.583541                      0.583541   \n",
              "4                  0.598783                      0.598783   \n",
              "5                  0.590171                      0.590251   \n",
              "6                  0.619950                      0.619950   \n",
              "7                  0.601738                      0.601717   \n",
              "8                  0.603612                      0.603612   \n",
              "9                  0.599209                      0.599209   \n",
              "10                 0.614026                      0.614026   \n",
              "11                 0.623712                      0.623712   \n",
              "12                 0.608131                      0.608138   \n",
              "13                 0.600578                      0.600578   \n",
              "14                 0.604686                      0.604686   \n",
              "15                 0.621773                      0.621773   \n",
              "16                 0.606524                      0.606524   \n",
              "17                 0.618451                      0.618451   \n",
              "18                 0.598344                      0.598344   \n",
              "19                 0.636259                      0.636252   \n",
              "20                 0.622177                      0.622256   \n",
              "21                 0.607446                      0.607446   \n",
              "22                 0.618934                      0.618934   \n",
              "23                 0.609507                      0.609507   \n",
              "24                 0.602956                      0.602956   \n",
              "25                 0.611763                      0.611806   \n",
              "26                 0.626544                      0.626544   \n",
              "27                 0.609738                      0.609738   \n",
              "28                 0.602027                      0.602293   \n",
              "29                 0.599115                      0.599115   \n",
              "30                 0.599987                      0.599987   \n",
              "31                 0.618869                      0.618869   \n",
              "32                 0.601169                      0.601176   \n",
              "33                 0.603475                      0.603475   \n",
              "34                 0.599108                      0.599108   \n",
              "35                 0.614278                      0.614278   \n",
              "36                 0.622703                      0.622530   \n",
              "37                 0.607042                      0.607006   \n",
              "38                 0.600881                      0.600881   \n",
              "39                 0.598445                      0.598445   \n",
              "40                 0.625326                      0.625326   \n",
              "41                 0.604412                      0.604412   \n",
              "42                 0.618177                      0.618177   \n",
              "43                 0.600347                      0.600347   \n",
              "44                 0.635718                      0.635718   \n",
              "45                 0.624317                      0.624288   \n",
              "46                 0.608866                      0.608866   \n",
              "47                 0.620944                      0.620944   \n",
              "48                 0.610206                      0.610206   \n",
              "49                 0.604563                      0.604563   \n",
              "50                 0.619402                      0.619525   \n",
              "51                 0.626414                      0.626414   \n",
              "52                 0.609291                      0.609241   \n",
              "53                 0.603590                      0.603072   \n",
              "54                 0.596946                      0.596953   \n",
              "55                 0.597393                      0.597393   \n",
              "56                 0.601861                      0.601861   \n",
              "57                 0.595994                      0.595994   \n",
              "58                 0.618083                      0.618083   \n",
              "59                 0.600737                      0.600758   \n",
              "\n",
              "     Operating Profit Rate   Pre-tax net Interest Rate  \\\n",
              "0                 0.998969                    0.796887   \n",
              "1                 0.998946                    0.797380   \n",
              "2                 0.998857                    0.796403   \n",
              "3                 0.998700                    0.796967   \n",
              "4                 0.998973                    0.797366   \n",
              "5                 0.998758                    0.796903   \n",
              "6                 0.998993                    0.797012   \n",
              "7                 0.999009                    0.797449   \n",
              "8                 0.998961                    0.797414   \n",
              "9                 0.999001                    0.797404   \n",
              "10                0.998978                    0.797535   \n",
              "11                0.998975                    0.797443   \n",
              "12                0.999045                    0.797429   \n",
              "13                0.998967                    0.797368   \n",
              "14                0.999053                    0.797514   \n",
              "15                0.999097                    0.797507   \n",
              "16                0.998996                    0.797877   \n",
              "17                0.999119                    0.797588   \n",
              "18                0.998989                    0.797412   \n",
              "19                0.999042                    0.797444   \n",
              "20                0.999065                    0.797522   \n",
              "21                0.999030                    0.797466   \n",
              "22                0.999059                    0.797444   \n",
              "23                0.999052                    0.797413   \n",
              "24                0.999025                    0.797417   \n",
              "25                0.998925                    0.797046   \n",
              "26                0.999099                    0.797512   \n",
              "27                0.999112                    0.797571   \n",
              "28                0.998982                    0.797503   \n",
              "29                0.998973                    0.797241   \n",
              "30                0.998909                    0.797291   \n",
              "31                0.999014                    0.797380   \n",
              "32                0.999000                    0.797440   \n",
              "33                0.998944                    0.797415   \n",
              "34                0.998995                    0.797393   \n",
              "35                0.998973                    0.797520   \n",
              "36                0.998997                    0.797245   \n",
              "37                0.999017                    0.797404   \n",
              "38                0.998975                    0.797388   \n",
              "39                0.998953                    0.797404   \n",
              "40                0.999136                    0.797542   \n",
              "41                0.998971                    0.797896   \n",
              "42                0.999077                    0.797522   \n",
              "43                0.999027                    0.797447   \n",
              "44                0.999062                    0.797481   \n",
              "45                0.999098                    0.797547   \n",
              "46                0.999048                    0.797465   \n",
              "47                0.999102                    0.797475   \n",
              "48                0.999023                    0.797429   \n",
              "49                0.999031                    0.797415   \n",
              "50                0.999045                    0.797270   \n",
              "51                0.999133                    0.797526   \n",
              "52                0.999098                    0.797498   \n",
              "53                0.999001                    0.797513   \n",
              "54                0.998940                    0.797084   \n",
              "55                0.998960                    0.797366   \n",
              "56                0.998825                    0.796779   \n",
              "57                0.998831                    0.797191   \n",
              "58                0.999004                    0.797389   \n",
              "59                0.998997                    0.797409   \n",
              "\n",
              "     After-tax net Interest Rate  \\\n",
              "0                       0.808809   \n",
              "1                       0.809301   \n",
              "2                       0.808388   \n",
              "3                       0.808966   \n",
              "4                       0.809304   \n",
              "5                       0.808771   \n",
              "6                       0.808960   \n",
              "7                       0.809362   \n",
              "8                       0.809338   \n",
              "9                       0.809320   \n",
              "10                      0.809460   \n",
              "11                      0.809389   \n",
              "12                      0.809344   \n",
              "13                      0.809299   \n",
              "14                      0.809456   \n",
              "15                      0.809396   \n",
              "16                      0.809761   \n",
              "17                      0.809461   \n",
              "18                      0.809334   \n",
              "19                      0.809340   \n",
              "20                      0.809406   \n",
              "21                      0.809371   \n",
              "22                      0.809367   \n",
              "23                      0.809323   \n",
              "24                      0.809331   \n",
              "25                      0.808993   \n",
              "26                      0.809403   \n",
              "27                      0.809449   \n",
              "28                      0.809400   \n",
              "29                      0.809176   \n",
              "30                      0.809223   \n",
              "31                      0.809307   \n",
              "32                      0.809359   \n",
              "33                      0.809338   \n",
              "34                      0.809318   \n",
              "35                      0.809444   \n",
              "36                      0.809190   \n",
              "37                      0.809317   \n",
              "38                      0.809316   \n",
              "39                      0.809343   \n",
              "40                      0.809413   \n",
              "41                      0.809799   \n",
              "42                      0.809409   \n",
              "43                      0.809362   \n",
              "44                      0.809365   \n",
              "45                      0.809430   \n",
              "46                      0.809364   \n",
              "47                      0.809371   \n",
              "48                      0.809341   \n",
              "49                      0.809332   \n",
              "50                      0.809203   \n",
              "51                      0.809396   \n",
              "52                      0.809387   \n",
              "53                      0.809398   \n",
              "54                      0.809040   \n",
              "55                      0.809290   \n",
              "56                      0.808717   \n",
              "57                      0.809128   \n",
              "58                      0.809315   \n",
              "59                      0.809330   \n",
              "\n",
              "     Non-industry income and expenditure/revenue  ...  \\\n",
              "0                                       0.302646  ...   \n",
              "1                                       0.303556  ...   \n",
              "2                                       0.302035  ...   \n",
              "3                                       0.303350  ...   \n",
              "4                                       0.303475  ...   \n",
              "5                                       0.303116  ...   \n",
              "6                                       0.302814  ...   \n",
              "7                                       0.303545  ...   \n",
              "8                                       0.303584  ...   \n",
              "9                                       0.303483  ...   \n",
              "10                                      0.303759  ...   \n",
              "11                                      0.303605  ...   \n",
              "12                                      0.303435  ...   \n",
              "13                                      0.303492  ...   \n",
              "14                                      0.303566  ...   \n",
              "15                                      0.303462  ...   \n",
              "16                                      0.304319  ...   \n",
              "17                                      0.303559  ...   \n",
              "18                                      0.303523  ...   \n",
              "19                                      0.303467  ...   \n",
              "20                                      0.303554  ...   \n",
              "21                                      0.303531  ...   \n",
              "22                                      0.303433  ...   \n",
              "23                                      0.303392  ...   \n",
              "24                                      0.303456  ...   \n",
              "25                                      0.303017  ...   \n",
              "26                                      0.303466  ...   \n",
              "27                                      0.303543  ...   \n",
              "28                                      0.303695  ...   \n",
              "29                                      0.303258  ...   \n",
              "30                                      0.303477  ...   \n",
              "31                                      0.303415  ...   \n",
              "32                                      0.303547  ...   \n",
              "33                                      0.303622  ...   \n",
              "34                                      0.303475  ...   \n",
              "35                                      0.303743  ...   \n",
              "36                                      0.303212  ...   \n",
              "37                                      0.303449  ...   \n",
              "38                                      0.303508  ...   \n",
              "39                                      0.303584  ...   \n",
              "40                                      0.303442  ...   \n",
              "41                                      0.304406  ...   \n",
              "42                                      0.303529  ...   \n",
              "43                                      0.303503  ...   \n",
              "44                                      0.303490  ...   \n",
              "45                                      0.303529  ...   \n",
              "46                                      0.303491  ...   \n",
              "47                                      0.303396  ...   \n",
              "48                                      0.303480  ...   \n",
              "49                                      0.303439  ...   \n",
              "50                                      0.303157  ...   \n",
              "51                                      0.303420  ...   \n",
              "52                                      0.303445  ...   \n",
              "53                                      0.303672  ...   \n",
              "54                                      0.303052  ...   \n",
              "55                                      0.303501  ...   \n",
              "56                                      0.302760  ...   \n",
              "57                                      0.303466  ...   \n",
              "58                                      0.303451  ...   \n",
              "59                                      0.303502  ...   \n",
              "\n",
              "     Net Income to Total Assets   Total assets to GNP price  \\\n",
              "0                      0.716845                    0.009219   \n",
              "1                      0.795297                    0.008323   \n",
              "2                      0.774670                    0.040003   \n",
              "3                      0.739555                    0.003252   \n",
              "4                      0.795016                    0.003878   \n",
              "5                      0.710420                    0.005278   \n",
              "6                      0.736619                    0.018372   \n",
              "7                      0.815350                    0.010005   \n",
              "8                      0.803647                    0.000824   \n",
              "9                      0.804195                    0.005798   \n",
              "10                     0.814111                    0.076972   \n",
              "11                     0.804887                    0.007318   \n",
              "12                     0.803260                    0.008232   \n",
              "13                     0.794158                    0.005262   \n",
              "14                     0.819715                    0.003954   \n",
              "15                     0.815419                    0.006590   \n",
              "16                     0.810421                    0.013611   \n",
              "17                     0.826642                    0.002746   \n",
              "18                     0.806264                    0.004238   \n",
              "19                     0.802552                    0.001521   \n",
              "20                     0.804639                    0.014039   \n",
              "21                     0.814012                    0.000935   \n",
              "22                     0.808976                    0.001830   \n",
              "23                     0.798104                    0.018753   \n",
              "24                     0.803445                    0.000830   \n",
              "25                     0.772909                    0.005887   \n",
              "26                     0.812974                    0.001193   \n",
              "27                     0.830176                    0.000703   \n",
              "28                     0.812100                    0.001225   \n",
              "29                     0.750431                    0.003011   \n",
              "30                     0.787082                    0.001631   \n",
              "31                     0.796023                    0.015252   \n",
              "32                     0.814102                    0.009774   \n",
              "33                     0.802923                    0.000785   \n",
              "34                     0.803066                    0.005124   \n",
              "35                     0.811257                    0.080006   \n",
              "36                     0.784385                    0.007563   \n",
              "37                     0.797744                    0.007308   \n",
              "38                     0.798174                    0.004593   \n",
              "39                     0.801372                    0.003425   \n",
              "40                     0.815731                    0.007173   \n",
              "41                     0.811426                    0.013698   \n",
              "42                     0.813568                    0.002623   \n",
              "43                     0.816257                    0.004076   \n",
              "44                     0.807573                    0.001521   \n",
              "45                     0.806667                    0.012883   \n",
              "46                     0.813580                    0.000887   \n",
              "47                     0.809900                    0.001675   \n",
              "48                     0.799842                    0.018859   \n",
              "49                     0.802638                    0.000717   \n",
              "50                     0.788126                    0.005376   \n",
              "51                     0.811329                    0.001088   \n",
              "52                     0.812826                    0.000729   \n",
              "53                     0.809661                    0.001210   \n",
              "54                     0.777094                    0.040161   \n",
              "55                     0.791982                    0.007233   \n",
              "56                     0.525651                    0.005803   \n",
              "57                     0.746586                    0.001053   \n",
              "58                     0.797585                    0.014207   \n",
              "59                     0.804706                    0.011073   \n",
              "\n",
              "     No-credit Interval   Gross Profit to Sales  \\\n",
              "0              0.622879                0.601453   \n",
              "1              0.623652                0.610237   \n",
              "2              0.623841                0.601449   \n",
              "3              0.622929                0.583538   \n",
              "4              0.623521                0.598782   \n",
              "5              0.622605                0.590172   \n",
              "6              0.623655                0.619949   \n",
              "7              0.623843                0.601739   \n",
              "8              0.623977                0.603613   \n",
              "9              0.623865                0.599205   \n",
              "10             0.623687                0.614021   \n",
              "11             0.623724                0.623709   \n",
              "12             0.623578                0.608125   \n",
              "13             0.623777                0.600578   \n",
              "14             0.624946                0.604683   \n",
              "15             0.623764                0.621769   \n",
              "16             0.623325                0.606524   \n",
              "17             0.623825                0.618452   \n",
              "18             0.623339                0.598345   \n",
              "19             0.623763                0.636256   \n",
              "20             0.623922                0.622177   \n",
              "21             0.623996                0.607441   \n",
              "22             0.623718                0.618933   \n",
              "23             0.623486                0.609508   \n",
              "24             0.623482                0.602952   \n",
              "25             0.623655                0.611761   \n",
              "26             0.623712                0.626543   \n",
              "27             0.624308                0.609733   \n",
              "28             0.623910                0.602027   \n",
              "29             0.623183                0.599110   \n",
              "30             0.621876                0.599984   \n",
              "31             0.623562                0.618864   \n",
              "32             0.623810                0.601169   \n",
              "33             0.623908                0.603474   \n",
              "34             0.623898                0.599107   \n",
              "35             0.623709                0.614277   \n",
              "36             0.623631                0.622698   \n",
              "37             0.623731                0.607042   \n",
              "38             0.623962                0.600876   \n",
              "39             0.623089                0.598440   \n",
              "40             0.623729                0.625327   \n",
              "41             0.622974                0.604410   \n",
              "42             0.623780                0.618178   \n",
              "43             0.622494                0.600343   \n",
              "44             0.623752                0.635718   \n",
              "45             0.623855                0.624318   \n",
              "46             0.624017                0.608864   \n",
              "47             0.623711                0.620941   \n",
              "48             0.623536                0.610202   \n",
              "49             0.623489                0.604564   \n",
              "50             0.623540                0.619403   \n",
              "51             0.623689                0.626413   \n",
              "52             0.624307                0.609288   \n",
              "53             0.623755                0.603586   \n",
              "54             0.617349                0.596946   \n",
              "55             0.622323                0.597394   \n",
              "56             0.623648                0.601857   \n",
              "57             0.623323                0.595994   \n",
              "58             0.623644                0.618084   \n",
              "59             0.623561                0.600737   \n",
              "\n",
              "     Net Income to Stockholder's Equity   Liability to Equity  \\\n",
              "0                              0.827890              0.290202   \n",
              "1                              0.839969              0.283846   \n",
              "2                              0.836774              0.290189   \n",
              "3                              0.834697              0.281721   \n",
              "4                              0.839973              0.278514   \n",
              "5                              0.829939              0.285087   \n",
              "6                              0.829980              0.292504   \n",
              "7                              0.841459              0.278607   \n",
              "8                              0.840487              0.276423   \n",
              "9                              0.840688              0.279388   \n",
              "10                             0.841337              0.278356   \n",
              "11                             0.840650              0.277892   \n",
              "12                             0.840702              0.281113   \n",
              "13                             0.839910              0.278518   \n",
              "14                             0.841624              0.277668   \n",
              "15                             0.841399              0.278124   \n",
              "16                             0.840889              0.276518   \n",
              "17                             0.841983              0.277190   \n",
              "18                             0.840955              0.280839   \n",
              "19                             0.840456              0.277139   \n",
              "20                             0.840639              0.277988   \n",
              "21                             0.841311              0.278208   \n",
              "22                             0.841234              0.281274   \n",
              "23                             0.840326              0.287134   \n",
              "24                             0.841047              0.287406   \n",
              "25                             0.837926              0.281295   \n",
              "26                             0.841188              0.277781   \n",
              "27                             0.841854              0.275704   \n",
              "28                             0.841116              0.277664   \n",
              "29                             0.831976              0.293599   \n",
              "30                             0.838259              0.297038   \n",
              "31                             0.840052              0.290365   \n",
              "32                             0.841432              0.279111   \n",
              "33                             0.840451              0.276555   \n",
              "34                             0.840609              0.279572   \n",
              "35                             0.841252              0.279474   \n",
              "36                             0.839029              0.280673   \n",
              "37                             0.840206              0.281335   \n",
              "38                             0.840193              0.277940   \n",
              "39                             0.840386              0.277247   \n",
              "40                             0.841574              0.279236   \n",
              "41                             0.840979              0.276803   \n",
              "42                             0.841197              0.277532   \n",
              "43                             0.841802              0.280569   \n",
              "44                             0.840820              0.277738   \n",
              "45                             0.840771              0.277885   \n",
              "46                             0.841275              0.278154   \n",
              "47                             0.841282              0.280888   \n",
              "48                             0.840599              0.288724   \n",
              "49                             0.840860              0.285724   \n",
              "50                             0.839285              0.281982   \n",
              "51                             0.841075              0.277759   \n",
              "52                             0.840992              0.276193   \n",
              "53                             0.840973              0.277869   \n",
              "54                             0.837053              0.290904   \n",
              "55                             0.839162              0.300005   \n",
              "56                             1.000000              0.182790   \n",
              "57                             0.831082              0.294222   \n",
              "58                             0.840280              0.289105   \n",
              "59                             0.840818              0.280855   \n",
              "\n",
              "     Degree of Financial Leverage (DFL)  \\\n",
              "0                              0.026601   \n",
              "1                              0.264577   \n",
              "2                              0.026555   \n",
              "3                              0.026697   \n",
              "4                              0.024752   \n",
              "5                              0.026675   \n",
              "6                              0.026622   \n",
              "7                              0.027031   \n",
              "8                              0.026891   \n",
              "9                              0.027243   \n",
              "10                             0.026971   \n",
              "11                             0.027391   \n",
              "12                             0.027480   \n",
              "13                             0.025000   \n",
              "14                             0.026896   \n",
              "15                             0.026897   \n",
              "16                             0.027003   \n",
              "17                             0.026908   \n",
              "18                             0.027637   \n",
              "19                             0.026924   \n",
              "20                             0.026794   \n",
              "21                             0.026890   \n",
              "22                             0.027565   \n",
              "23                             0.029364   \n",
              "24                             0.028206   \n",
              "25                             0.026415   \n",
              "26                             0.026924   \n",
              "27                             0.026852   \n",
              "28                             0.027067   \n",
              "29                             0.026472   \n",
              "30                             0.024886   \n",
              "31                             0.174091   \n",
              "32                             0.027119   \n",
              "33                             0.027003   \n",
              "34                             0.027677   \n",
              "35                             0.027115   \n",
              "36                             0.026385   \n",
              "37                             0.028426   \n",
              "38                             0.029088   \n",
              "39                             0.028873   \n",
              "40                             0.026908   \n",
              "41                             0.027017   \n",
              "42                             0.026986   \n",
              "43                             0.027131   \n",
              "44                             0.026877   \n",
              "45                             0.026792   \n",
              "46                             0.026862   \n",
              "47                             0.027170   \n",
              "48                             0.028759   \n",
              "49                             0.028520   \n",
              "50                             0.025749   \n",
              "51                             0.026954   \n",
              "52                             0.026912   \n",
              "53                             0.026991   \n",
              "54                             0.026050   \n",
              "55                             0.022762   \n",
              "56                             0.026763   \n",
              "57                             0.026590   \n",
              "58                             0.032353   \n",
              "59                             0.027157   \n",
              "\n",
              "     Interest Coverage Ratio (Interest expense to EBIT)   Net Income Flag  \\\n",
              "0                                            0.564050                   1   \n",
              "1                                            0.570175                   1   \n",
              "2                                            0.563706                   1   \n",
              "3                                            0.564663                   1   \n",
              "4                                            0.575617                   1   \n",
              "5                                            0.564538                   1   \n",
              "6                                            0.564200                   1   \n",
              "7                                            0.566089                   1   \n",
              "8                                            0.565592                   1   \n",
              "9                                            0.566668                   1   \n",
              "10                                           0.565892                   1   \n",
              "11                                           0.566983                   1   \n",
              "12                                           0.567146                   1   \n",
              "13                                           0.577445                   1   \n",
              "14                                           0.565614                   1   \n",
              "15                                           0.565618                   1   \n",
              "16                                           0.565999                   1   \n",
              "17                                           0.565659                   1   \n",
              "18                                           0.567399                   1   \n",
              "19                                           0.565722                   1   \n",
              "20                                           0.565171                   1   \n",
              "21                                           0.565589                   1   \n",
              "22                                           0.567289                   1   \n",
              "23                                           0.568730                   1   \n",
              "24                                           0.568043                   1   \n",
              "25                                           0.562377                   1   \n",
              "26                                           0.565723                   1   \n",
              "27                                           0.565433                   1   \n",
              "28                                           0.566201                   1   \n",
              "29                                           0.562977                   1   \n",
              "30                                           0.576473                   1   \n",
              "31                                           0.570161                   1   \n",
              "32                                           0.566351                   1   \n",
              "33                                           0.566000                   1   \n",
              "34                                           0.567456                   1   \n",
              "35                                           0.566339                   1   \n",
              "36                                           0.562016                   1   \n",
              "37                                           0.568219                   1   \n",
              "38                                           0.568609                   1   \n",
              "39                                           0.568501                   1   \n",
              "40                                           0.565659                   1   \n",
              "41                                           0.566045                   1   \n",
              "42                                           0.565941                   1   \n",
              "43                                           0.566383                   1   \n",
              "44                                           0.565538                   1   \n",
              "45                                           0.565164                   1   \n",
              "46                                           0.565476                   1   \n",
              "47                                           0.566487                   1   \n",
              "48                                           0.568437                   1   \n",
              "49                                           0.568286                   1   \n",
              "50                                           0.198632                   1   \n",
              "51                                           0.565831                   1   \n",
              "52                                           0.565675                   1   \n",
              "53                                           0.565961                   1   \n",
              "54                                           0.553328                   1   \n",
              "55                                           0.571989                   1   \n",
              "56                                           0.565021                   1   \n",
              "57                                           0.563976                   1   \n",
              "58                                           0.569393                   1   \n",
              "59                                           0.566455                   1   \n",
              "\n",
              "     Equity to Liability  \n",
              "0               0.016469  \n",
              "1               0.020794  \n",
              "2               0.016474  \n",
              "3               0.023982  \n",
              "4               0.035490  \n",
              "5               0.019534  \n",
              "6               0.015663  \n",
              "7               0.034889  \n",
              "8               0.065826  \n",
              "9               0.030801  \n",
              "10              0.036572  \n",
              "11              0.040381  \n",
              "12              0.025282  \n",
              "13              0.035464  \n",
              "14              0.042646  \n",
              "15              0.038354  \n",
              "16              0.062940  \n",
              "17              0.048822  \n",
              "18              0.025953  \n",
              "19              0.049622  \n",
              "20              0.039507  \n",
              "21              0.037680  \n",
              "22              0.024915  \n",
              "23              0.018005  \n",
              "24              0.017839  \n",
              "25              0.024869  \n",
              "26              0.041465  \n",
              "27              0.104842  \n",
              "28              0.042682  \n",
              "29              0.015349  \n",
              "30              0.014562  \n",
              "31              0.016404  \n",
              "32              0.032086  \n",
              "33              0.061887  \n",
              "34              0.030024  \n",
              "35              0.030431  \n",
              "36              0.026387  \n",
              "37              0.024781  \n",
              "38              0.039935  \n",
              "39              0.047960  \n",
              "40              0.031485  \n",
              "41              0.055858  \n",
              "42              0.044195  \n",
              "43              0.026674  \n",
              "44              0.041906  \n",
              "45              0.040450  \n",
              "46              0.038105  \n",
              "47              0.025829  \n",
              "48              0.017125  \n",
              "49              0.018998  \n",
              "50              0.023491  \n",
              "51              0.041680  \n",
              "52              0.074266  \n",
              "53              0.040601  \n",
              "54              0.016199  \n",
              "55              0.014054  \n",
              "56              0.009178  \n",
              "57              0.015186  \n",
              "58              0.016943  \n",
              "59              0.025913  \n",
              "\n",
              "[60 rows x 96 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-46a19330-515a-4e13-81ea-9342ffd757a4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Bankrupt?</th>\n",
              "      <th>ROA(C) before interest and depreciation before interest</th>\n",
              "      <th>ROA(A) before interest and % after tax</th>\n",
              "      <th>ROA(B) before interest and depreciation after tax</th>\n",
              "      <th>Operating Gross Margin</th>\n",
              "      <th>Realized Sales Gross Margin</th>\n",
              "      <th>Operating Profit Rate</th>\n",
              "      <th>Pre-tax net Interest Rate</th>\n",
              "      <th>After-tax net Interest Rate</th>\n",
              "      <th>Non-industry income and expenditure/revenue</th>\n",
              "      <th>...</th>\n",
              "      <th>Net Income to Total Assets</th>\n",
              "      <th>Total assets to GNP price</th>\n",
              "      <th>No-credit Interval</th>\n",
              "      <th>Gross Profit to Sales</th>\n",
              "      <th>Net Income to Stockholder's Equity</th>\n",
              "      <th>Liability to Equity</th>\n",
              "      <th>Degree of Financial Leverage (DFL)</th>\n",
              "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
              "      <th>Net Income Flag</th>\n",
              "      <th>Equity to Liability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.370594</td>\n",
              "      <td>0.424389</td>\n",
              "      <td>0.405750</td>\n",
              "      <td>0.601457</td>\n",
              "      <td>0.601457</td>\n",
              "      <td>0.998969</td>\n",
              "      <td>0.796887</td>\n",
              "      <td>0.808809</td>\n",
              "      <td>0.302646</td>\n",
              "      <td>...</td>\n",
              "      <td>0.716845</td>\n",
              "      <td>0.009219</td>\n",
              "      <td>0.622879</td>\n",
              "      <td>0.601453</td>\n",
              "      <td>0.827890</td>\n",
              "      <td>0.290202</td>\n",
              "      <td>0.026601</td>\n",
              "      <td>0.564050</td>\n",
              "      <td>1</td>\n",
              "      <td>0.016469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.464291</td>\n",
              "      <td>0.538214</td>\n",
              "      <td>0.516730</td>\n",
              "      <td>0.610235</td>\n",
              "      <td>0.610235</td>\n",
              "      <td>0.998946</td>\n",
              "      <td>0.797380</td>\n",
              "      <td>0.809301</td>\n",
              "      <td>0.303556</td>\n",
              "      <td>...</td>\n",
              "      <td>0.795297</td>\n",
              "      <td>0.008323</td>\n",
              "      <td>0.623652</td>\n",
              "      <td>0.610237</td>\n",
              "      <td>0.839969</td>\n",
              "      <td>0.283846</td>\n",
              "      <td>0.264577</td>\n",
              "      <td>0.570175</td>\n",
              "      <td>1</td>\n",
              "      <td>0.020794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0.426071</td>\n",
              "      <td>0.499019</td>\n",
              "      <td>0.472295</td>\n",
              "      <td>0.601450</td>\n",
              "      <td>0.601364</td>\n",
              "      <td>0.998857</td>\n",
              "      <td>0.796403</td>\n",
              "      <td>0.808388</td>\n",
              "      <td>0.302035</td>\n",
              "      <td>...</td>\n",
              "      <td>0.774670</td>\n",
              "      <td>0.040003</td>\n",
              "      <td>0.623841</td>\n",
              "      <td>0.601449</td>\n",
              "      <td>0.836774</td>\n",
              "      <td>0.290189</td>\n",
              "      <td>0.026555</td>\n",
              "      <td>0.563706</td>\n",
              "      <td>1</td>\n",
              "      <td>0.016474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0.399844</td>\n",
              "      <td>0.451265</td>\n",
              "      <td>0.457733</td>\n",
              "      <td>0.583541</td>\n",
              "      <td>0.583541</td>\n",
              "      <td>0.998700</td>\n",
              "      <td>0.796967</td>\n",
              "      <td>0.808966</td>\n",
              "      <td>0.303350</td>\n",
              "      <td>...</td>\n",
              "      <td>0.739555</td>\n",
              "      <td>0.003252</td>\n",
              "      <td>0.622929</td>\n",
              "      <td>0.583538</td>\n",
              "      <td>0.834697</td>\n",
              "      <td>0.281721</td>\n",
              "      <td>0.026697</td>\n",
              "      <td>0.564663</td>\n",
              "      <td>1</td>\n",
              "      <td>0.023982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0.465022</td>\n",
              "      <td>0.538432</td>\n",
              "      <td>0.522298</td>\n",
              "      <td>0.598783</td>\n",
              "      <td>0.598783</td>\n",
              "      <td>0.998973</td>\n",
              "      <td>0.797366</td>\n",
              "      <td>0.809304</td>\n",
              "      <td>0.303475</td>\n",
              "      <td>...</td>\n",
              "      <td>0.795016</td>\n",
              "      <td>0.003878</td>\n",
              "      <td>0.623521</td>\n",
              "      <td>0.598782</td>\n",
              "      <td>0.839973</td>\n",
              "      <td>0.278514</td>\n",
              "      <td>0.024752</td>\n",
              "      <td>0.575617</td>\n",
              "      <td>1</td>\n",
              "      <td>0.035490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0.388680</td>\n",
              "      <td>0.415177</td>\n",
              "      <td>0.419134</td>\n",
              "      <td>0.590171</td>\n",
              "      <td>0.590251</td>\n",
              "      <td>0.998758</td>\n",
              "      <td>0.796903</td>\n",
              "      <td>0.808771</td>\n",
              "      <td>0.303116</td>\n",
              "      <td>...</td>\n",
              "      <td>0.710420</td>\n",
              "      <td>0.005278</td>\n",
              "      <td>0.622605</td>\n",
              "      <td>0.590172</td>\n",
              "      <td>0.829939</td>\n",
              "      <td>0.285087</td>\n",
              "      <td>0.026675</td>\n",
              "      <td>0.564538</td>\n",
              "      <td>1</td>\n",
              "      <td>0.019534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0.390923</td>\n",
              "      <td>0.445704</td>\n",
              "      <td>0.436158</td>\n",
              "      <td>0.619950</td>\n",
              "      <td>0.619950</td>\n",
              "      <td>0.998993</td>\n",
              "      <td>0.797012</td>\n",
              "      <td>0.808960</td>\n",
              "      <td>0.302814</td>\n",
              "      <td>...</td>\n",
              "      <td>0.736619</td>\n",
              "      <td>0.018372</td>\n",
              "      <td>0.623655</td>\n",
              "      <td>0.619949</td>\n",
              "      <td>0.829980</td>\n",
              "      <td>0.292504</td>\n",
              "      <td>0.026622</td>\n",
              "      <td>0.564200</td>\n",
              "      <td>1</td>\n",
              "      <td>0.015663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0.508361</td>\n",
              "      <td>0.570922</td>\n",
              "      <td>0.559077</td>\n",
              "      <td>0.601738</td>\n",
              "      <td>0.601717</td>\n",
              "      <td>0.999009</td>\n",
              "      <td>0.797449</td>\n",
              "      <td>0.809362</td>\n",
              "      <td>0.303545</td>\n",
              "      <td>...</td>\n",
              "      <td>0.815350</td>\n",
              "      <td>0.010005</td>\n",
              "      <td>0.623843</td>\n",
              "      <td>0.601739</td>\n",
              "      <td>0.841459</td>\n",
              "      <td>0.278607</td>\n",
              "      <td>0.027031</td>\n",
              "      <td>0.566089</td>\n",
              "      <td>1</td>\n",
              "      <td>0.034889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0.488519</td>\n",
              "      <td>0.545137</td>\n",
              "      <td>0.543284</td>\n",
              "      <td>0.603612</td>\n",
              "      <td>0.603612</td>\n",
              "      <td>0.998961</td>\n",
              "      <td>0.797414</td>\n",
              "      <td>0.809338</td>\n",
              "      <td>0.303584</td>\n",
              "      <td>...</td>\n",
              "      <td>0.803647</td>\n",
              "      <td>0.000824</td>\n",
              "      <td>0.623977</td>\n",
              "      <td>0.603613</td>\n",
              "      <td>0.840487</td>\n",
              "      <td>0.276423</td>\n",
              "      <td>0.026891</td>\n",
              "      <td>0.565592</td>\n",
              "      <td>1</td>\n",
              "      <td>0.065826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0.495686</td>\n",
              "      <td>0.550916</td>\n",
              "      <td>0.542963</td>\n",
              "      <td>0.599209</td>\n",
              "      <td>0.599209</td>\n",
              "      <td>0.999001</td>\n",
              "      <td>0.797404</td>\n",
              "      <td>0.809320</td>\n",
              "      <td>0.303483</td>\n",
              "      <td>...</td>\n",
              "      <td>0.804195</td>\n",
              "      <td>0.005798</td>\n",
              "      <td>0.623865</td>\n",
              "      <td>0.599205</td>\n",
              "      <td>0.840688</td>\n",
              "      <td>0.279388</td>\n",
              "      <td>0.027243</td>\n",
              "      <td>0.566668</td>\n",
              "      <td>1</td>\n",
              "      <td>0.030801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0.482475</td>\n",
              "      <td>0.567543</td>\n",
              "      <td>0.538198</td>\n",
              "      <td>0.614026</td>\n",
              "      <td>0.614026</td>\n",
              "      <td>0.998978</td>\n",
              "      <td>0.797535</td>\n",
              "      <td>0.809460</td>\n",
              "      <td>0.303759</td>\n",
              "      <td>...</td>\n",
              "      <td>0.814111</td>\n",
              "      <td>0.076972</td>\n",
              "      <td>0.623687</td>\n",
              "      <td>0.614021</td>\n",
              "      <td>0.841337</td>\n",
              "      <td>0.278356</td>\n",
              "      <td>0.026971</td>\n",
              "      <td>0.565892</td>\n",
              "      <td>1</td>\n",
              "      <td>0.036572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0.444401</td>\n",
              "      <td>0.549717</td>\n",
              "      <td>0.498956</td>\n",
              "      <td>0.623712</td>\n",
              "      <td>0.623712</td>\n",
              "      <td>0.998975</td>\n",
              "      <td>0.797443</td>\n",
              "      <td>0.809389</td>\n",
              "      <td>0.303605</td>\n",
              "      <td>...</td>\n",
              "      <td>0.804887</td>\n",
              "      <td>0.007318</td>\n",
              "      <td>0.623724</td>\n",
              "      <td>0.623709</td>\n",
              "      <td>0.840650</td>\n",
              "      <td>0.277892</td>\n",
              "      <td>0.027391</td>\n",
              "      <td>0.566983</td>\n",
              "      <td>1</td>\n",
              "      <td>0.040381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>0.491152</td>\n",
              "      <td>0.551570</td>\n",
              "      <td>0.543391</td>\n",
              "      <td>0.608131</td>\n",
              "      <td>0.608138</td>\n",
              "      <td>0.999045</td>\n",
              "      <td>0.797429</td>\n",
              "      <td>0.809344</td>\n",
              "      <td>0.303435</td>\n",
              "      <td>...</td>\n",
              "      <td>0.803260</td>\n",
              "      <td>0.008232</td>\n",
              "      <td>0.623578</td>\n",
              "      <td>0.608125</td>\n",
              "      <td>0.840702</td>\n",
              "      <td>0.281113</td>\n",
              "      <td>0.027480</td>\n",
              "      <td>0.567146</td>\n",
              "      <td>1</td>\n",
              "      <td>0.025282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>0.474041</td>\n",
              "      <td>0.533308</td>\n",
              "      <td>0.523690</td>\n",
              "      <td>0.600578</td>\n",
              "      <td>0.600578</td>\n",
              "      <td>0.998967</td>\n",
              "      <td>0.797368</td>\n",
              "      <td>0.809299</td>\n",
              "      <td>0.303492</td>\n",
              "      <td>...</td>\n",
              "      <td>0.794158</td>\n",
              "      <td>0.005262</td>\n",
              "      <td>0.623777</td>\n",
              "      <td>0.600578</td>\n",
              "      <td>0.839910</td>\n",
              "      <td>0.278518</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.577445</td>\n",
              "      <td>1</td>\n",
              "      <td>0.035464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>0.506703</td>\n",
              "      <td>0.575829</td>\n",
              "      <td>0.569838</td>\n",
              "      <td>0.604686</td>\n",
              "      <td>0.604686</td>\n",
              "      <td>0.999053</td>\n",
              "      <td>0.797514</td>\n",
              "      <td>0.809456</td>\n",
              "      <td>0.303566</td>\n",
              "      <td>...</td>\n",
              "      <td>0.819715</td>\n",
              "      <td>0.003954</td>\n",
              "      <td>0.624946</td>\n",
              "      <td>0.604683</td>\n",
              "      <td>0.841624</td>\n",
              "      <td>0.277668</td>\n",
              "      <td>0.026896</td>\n",
              "      <td>0.565614</td>\n",
              "      <td>1</td>\n",
              "      <td>0.042646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>0.513821</td>\n",
              "      <td>0.571086</td>\n",
              "      <td>0.558756</td>\n",
              "      <td>0.621773</td>\n",
              "      <td>0.621773</td>\n",
              "      <td>0.999097</td>\n",
              "      <td>0.797507</td>\n",
              "      <td>0.809396</td>\n",
              "      <td>0.303462</td>\n",
              "      <td>...</td>\n",
              "      <td>0.815419</td>\n",
              "      <td>0.006590</td>\n",
              "      <td>0.623764</td>\n",
              "      <td>0.621769</td>\n",
              "      <td>0.841399</td>\n",
              "      <td>0.278124</td>\n",
              "      <td>0.026897</td>\n",
              "      <td>0.565618</td>\n",
              "      <td>1</td>\n",
              "      <td>0.038354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>0.488909</td>\n",
              "      <td>0.560238</td>\n",
              "      <td>0.540286</td>\n",
              "      <td>0.606524</td>\n",
              "      <td>0.606524</td>\n",
              "      <td>0.998996</td>\n",
              "      <td>0.797877</td>\n",
              "      <td>0.809761</td>\n",
              "      <td>0.304319</td>\n",
              "      <td>...</td>\n",
              "      <td>0.810421</td>\n",
              "      <td>0.013611</td>\n",
              "      <td>0.623325</td>\n",
              "      <td>0.606524</td>\n",
              "      <td>0.840889</td>\n",
              "      <td>0.276518</td>\n",
              "      <td>0.027003</td>\n",
              "      <td>0.565999</td>\n",
              "      <td>1</td>\n",
              "      <td>0.062940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>0.535953</td>\n",
              "      <td>0.590438</td>\n",
              "      <td>0.580920</td>\n",
              "      <td>0.618451</td>\n",
              "      <td>0.618451</td>\n",
              "      <td>0.999119</td>\n",
              "      <td>0.797588</td>\n",
              "      <td>0.809461</td>\n",
              "      <td>0.303559</td>\n",
              "      <td>...</td>\n",
              "      <td>0.826642</td>\n",
              "      <td>0.002746</td>\n",
              "      <td>0.623825</td>\n",
              "      <td>0.618452</td>\n",
              "      <td>0.841983</td>\n",
              "      <td>0.277190</td>\n",
              "      <td>0.026908</td>\n",
              "      <td>0.565659</td>\n",
              "      <td>1</td>\n",
              "      <td>0.048822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>0.504071</td>\n",
              "      <td>0.559802</td>\n",
              "      <td>0.558649</td>\n",
              "      <td>0.598344</td>\n",
              "      <td>0.598344</td>\n",
              "      <td>0.998989</td>\n",
              "      <td>0.797412</td>\n",
              "      <td>0.809334</td>\n",
              "      <td>0.303523</td>\n",
              "      <td>...</td>\n",
              "      <td>0.806264</td>\n",
              "      <td>0.004238</td>\n",
              "      <td>0.623339</td>\n",
              "      <td>0.598345</td>\n",
              "      <td>0.840955</td>\n",
              "      <td>0.280839</td>\n",
              "      <td>0.027637</td>\n",
              "      <td>0.567399</td>\n",
              "      <td>1</td>\n",
              "      <td>0.025953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>0.487398</td>\n",
              "      <td>0.543720</td>\n",
              "      <td>0.533647</td>\n",
              "      <td>0.636259</td>\n",
              "      <td>0.636252</td>\n",
              "      <td>0.999042</td>\n",
              "      <td>0.797444</td>\n",
              "      <td>0.809340</td>\n",
              "      <td>0.303467</td>\n",
              "      <td>...</td>\n",
              "      <td>0.802552</td>\n",
              "      <td>0.001521</td>\n",
              "      <td>0.623763</td>\n",
              "      <td>0.636256</td>\n",
              "      <td>0.840456</td>\n",
              "      <td>0.277139</td>\n",
              "      <td>0.026924</td>\n",
              "      <td>0.565722</td>\n",
              "      <td>1</td>\n",
              "      <td>0.049622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0</td>\n",
              "      <td>0.485253</td>\n",
              "      <td>0.545573</td>\n",
              "      <td>0.534665</td>\n",
              "      <td>0.622177</td>\n",
              "      <td>0.622256</td>\n",
              "      <td>0.999065</td>\n",
              "      <td>0.797522</td>\n",
              "      <td>0.809406</td>\n",
              "      <td>0.303554</td>\n",
              "      <td>...</td>\n",
              "      <td>0.804639</td>\n",
              "      <td>0.014039</td>\n",
              "      <td>0.623922</td>\n",
              "      <td>0.622177</td>\n",
              "      <td>0.840639</td>\n",
              "      <td>0.277988</td>\n",
              "      <td>0.026794</td>\n",
              "      <td>0.565171</td>\n",
              "      <td>1</td>\n",
              "      <td>0.039507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0</td>\n",
              "      <td>0.504558</td>\n",
              "      <td>0.564490</td>\n",
              "      <td>0.553027</td>\n",
              "      <td>0.607446</td>\n",
              "      <td>0.607446</td>\n",
              "      <td>0.999030</td>\n",
              "      <td>0.797466</td>\n",
              "      <td>0.809371</td>\n",
              "      <td>0.303531</td>\n",
              "      <td>...</td>\n",
              "      <td>0.814012</td>\n",
              "      <td>0.000935</td>\n",
              "      <td>0.623996</td>\n",
              "      <td>0.607441</td>\n",
              "      <td>0.841311</td>\n",
              "      <td>0.278208</td>\n",
              "      <td>0.026890</td>\n",
              "      <td>0.565589</td>\n",
              "      <td>1</td>\n",
              "      <td>0.037680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0</td>\n",
              "      <td>0.512017</td>\n",
              "      <td>0.563672</td>\n",
              "      <td>0.569035</td>\n",
              "      <td>0.618934</td>\n",
              "      <td>0.618934</td>\n",
              "      <td>0.999059</td>\n",
              "      <td>0.797444</td>\n",
              "      <td>0.809367</td>\n",
              "      <td>0.303433</td>\n",
              "      <td>...</td>\n",
              "      <td>0.808976</td>\n",
              "      <td>0.001830</td>\n",
              "      <td>0.623718</td>\n",
              "      <td>0.618933</td>\n",
              "      <td>0.841234</td>\n",
              "      <td>0.281274</td>\n",
              "      <td>0.027565</td>\n",
              "      <td>0.567289</td>\n",
              "      <td>1</td>\n",
              "      <td>0.024915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0</td>\n",
              "      <td>0.494857</td>\n",
              "      <td>0.548136</td>\n",
              "      <td>0.540446</td>\n",
              "      <td>0.609507</td>\n",
              "      <td>0.609507</td>\n",
              "      <td>0.999052</td>\n",
              "      <td>0.797413</td>\n",
              "      <td>0.809323</td>\n",
              "      <td>0.303392</td>\n",
              "      <td>...</td>\n",
              "      <td>0.798104</td>\n",
              "      <td>0.018753</td>\n",
              "      <td>0.623486</td>\n",
              "      <td>0.609508</td>\n",
              "      <td>0.840326</td>\n",
              "      <td>0.287134</td>\n",
              "      <td>0.029364</td>\n",
              "      <td>0.568730</td>\n",
              "      <td>1</td>\n",
              "      <td>0.018005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0</td>\n",
              "      <td>0.509969</td>\n",
              "      <td>0.561382</td>\n",
              "      <td>0.552599</td>\n",
              "      <td>0.602956</td>\n",
              "      <td>0.602956</td>\n",
              "      <td>0.999025</td>\n",
              "      <td>0.797417</td>\n",
              "      <td>0.809331</td>\n",
              "      <td>0.303456</td>\n",
              "      <td>...</td>\n",
              "      <td>0.803445</td>\n",
              "      <td>0.000830</td>\n",
              "      <td>0.623482</td>\n",
              "      <td>0.602952</td>\n",
              "      <td>0.841047</td>\n",
              "      <td>0.287406</td>\n",
              "      <td>0.028206</td>\n",
              "      <td>0.568043</td>\n",
              "      <td>1</td>\n",
              "      <td>0.017839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0</td>\n",
              "      <td>0.444986</td>\n",
              "      <td>0.503652</td>\n",
              "      <td>0.495530</td>\n",
              "      <td>0.611763</td>\n",
              "      <td>0.611806</td>\n",
              "      <td>0.998925</td>\n",
              "      <td>0.797046</td>\n",
              "      <td>0.808993</td>\n",
              "      <td>0.303017</td>\n",
              "      <td>...</td>\n",
              "      <td>0.772909</td>\n",
              "      <td>0.005887</td>\n",
              "      <td>0.623655</td>\n",
              "      <td>0.611761</td>\n",
              "      <td>0.837926</td>\n",
              "      <td>0.281295</td>\n",
              "      <td>0.026415</td>\n",
              "      <td>0.562377</td>\n",
              "      <td>1</td>\n",
              "      <td>0.024869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0</td>\n",
              "      <td>0.519280</td>\n",
              "      <td>0.563618</td>\n",
              "      <td>0.568606</td>\n",
              "      <td>0.626544</td>\n",
              "      <td>0.626544</td>\n",
              "      <td>0.999099</td>\n",
              "      <td>0.797512</td>\n",
              "      <td>0.809403</td>\n",
              "      <td>0.303466</td>\n",
              "      <td>...</td>\n",
              "      <td>0.812974</td>\n",
              "      <td>0.001193</td>\n",
              "      <td>0.623712</td>\n",
              "      <td>0.626543</td>\n",
              "      <td>0.841188</td>\n",
              "      <td>0.277781</td>\n",
              "      <td>0.026924</td>\n",
              "      <td>0.565723</td>\n",
              "      <td>1</td>\n",
              "      <td>0.041465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0</td>\n",
              "      <td>0.547409</td>\n",
              "      <td>0.593055</td>\n",
              "      <td>0.592484</td>\n",
              "      <td>0.609738</td>\n",
              "      <td>0.609738</td>\n",
              "      <td>0.999112</td>\n",
              "      <td>0.797571</td>\n",
              "      <td>0.809449</td>\n",
              "      <td>0.303543</td>\n",
              "      <td>...</td>\n",
              "      <td>0.830176</td>\n",
              "      <td>0.000703</td>\n",
              "      <td>0.624308</td>\n",
              "      <td>0.609733</td>\n",
              "      <td>0.841854</td>\n",
              "      <td>0.275704</td>\n",
              "      <td>0.026852</td>\n",
              "      <td>0.565433</td>\n",
              "      <td>1</td>\n",
              "      <td>0.104842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0</td>\n",
              "      <td>0.500853</td>\n",
              "      <td>0.563945</td>\n",
              "      <td>0.550565</td>\n",
              "      <td>0.602027</td>\n",
              "      <td>0.602293</td>\n",
              "      <td>0.998982</td>\n",
              "      <td>0.797503</td>\n",
              "      <td>0.809400</td>\n",
              "      <td>0.303695</td>\n",
              "      <td>...</td>\n",
              "      <td>0.812100</td>\n",
              "      <td>0.001225</td>\n",
              "      <td>0.623910</td>\n",
              "      <td>0.602027</td>\n",
              "      <td>0.841116</td>\n",
              "      <td>0.277664</td>\n",
              "      <td>0.027067</td>\n",
              "      <td>0.566201</td>\n",
              "      <td>1</td>\n",
              "      <td>0.042682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1</td>\n",
              "      <td>0.416126</td>\n",
              "      <td>0.470235</td>\n",
              "      <td>0.463783</td>\n",
              "      <td>0.599115</td>\n",
              "      <td>0.599115</td>\n",
              "      <td>0.998973</td>\n",
              "      <td>0.797241</td>\n",
              "      <td>0.809176</td>\n",
              "      <td>0.303258</td>\n",
              "      <td>...</td>\n",
              "      <td>0.750431</td>\n",
              "      <td>0.003011</td>\n",
              "      <td>0.623183</td>\n",
              "      <td>0.599110</td>\n",
              "      <td>0.831976</td>\n",
              "      <td>0.293599</td>\n",
              "      <td>0.026472</td>\n",
              "      <td>0.562977</td>\n",
              "      <td>1</td>\n",
              "      <td>0.015349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1</td>\n",
              "      <td>0.462195</td>\n",
              "      <td>0.536034</td>\n",
              "      <td>0.514428</td>\n",
              "      <td>0.599987</td>\n",
              "      <td>0.599987</td>\n",
              "      <td>0.998909</td>\n",
              "      <td>0.797291</td>\n",
              "      <td>0.809223</td>\n",
              "      <td>0.303477</td>\n",
              "      <td>...</td>\n",
              "      <td>0.787082</td>\n",
              "      <td>0.001631</td>\n",
              "      <td>0.621876</td>\n",
              "      <td>0.599984</td>\n",
              "      <td>0.838259</td>\n",
              "      <td>0.297038</td>\n",
              "      <td>0.024886</td>\n",
              "      <td>0.576473</td>\n",
              "      <td>1</td>\n",
              "      <td>0.014562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0</td>\n",
              "      <td>0.476088</td>\n",
              "      <td>0.544483</td>\n",
              "      <td>0.529686</td>\n",
              "      <td>0.618869</td>\n",
              "      <td>0.618869</td>\n",
              "      <td>0.999014</td>\n",
              "      <td>0.797380</td>\n",
              "      <td>0.809307</td>\n",
              "      <td>0.303415</td>\n",
              "      <td>...</td>\n",
              "      <td>0.796023</td>\n",
              "      <td>0.015252</td>\n",
              "      <td>0.623562</td>\n",
              "      <td>0.618864</td>\n",
              "      <td>0.840052</td>\n",
              "      <td>0.290365</td>\n",
              "      <td>0.174091</td>\n",
              "      <td>0.570161</td>\n",
              "      <td>1</td>\n",
              "      <td>0.016404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0</td>\n",
              "      <td>0.505874</td>\n",
              "      <td>0.570704</td>\n",
              "      <td>0.559827</td>\n",
              "      <td>0.601169</td>\n",
              "      <td>0.601176</td>\n",
              "      <td>0.999000</td>\n",
              "      <td>0.797440</td>\n",
              "      <td>0.809359</td>\n",
              "      <td>0.303547</td>\n",
              "      <td>...</td>\n",
              "      <td>0.814102</td>\n",
              "      <td>0.009774</td>\n",
              "      <td>0.623810</td>\n",
              "      <td>0.601169</td>\n",
              "      <td>0.841432</td>\n",
              "      <td>0.279111</td>\n",
              "      <td>0.027119</td>\n",
              "      <td>0.566351</td>\n",
              "      <td>1</td>\n",
              "      <td>0.032086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0</td>\n",
              "      <td>0.486374</td>\n",
              "      <td>0.544756</td>\n",
              "      <td>0.540500</td>\n",
              "      <td>0.603475</td>\n",
              "      <td>0.603475</td>\n",
              "      <td>0.998944</td>\n",
              "      <td>0.797415</td>\n",
              "      <td>0.809338</td>\n",
              "      <td>0.303622</td>\n",
              "      <td>...</td>\n",
              "      <td>0.802923</td>\n",
              "      <td>0.000785</td>\n",
              "      <td>0.623908</td>\n",
              "      <td>0.603474</td>\n",
              "      <td>0.840451</td>\n",
              "      <td>0.276555</td>\n",
              "      <td>0.027003</td>\n",
              "      <td>0.566000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.061887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0</td>\n",
              "      <td>0.494369</td>\n",
              "      <td>0.550916</td>\n",
              "      <td>0.548584</td>\n",
              "      <td>0.599108</td>\n",
              "      <td>0.599108</td>\n",
              "      <td>0.998995</td>\n",
              "      <td>0.797393</td>\n",
              "      <td>0.809318</td>\n",
              "      <td>0.303475</td>\n",
              "      <td>...</td>\n",
              "      <td>0.803066</td>\n",
              "      <td>0.005124</td>\n",
              "      <td>0.623898</td>\n",
              "      <td>0.599107</td>\n",
              "      <td>0.840609</td>\n",
              "      <td>0.279572</td>\n",
              "      <td>0.027677</td>\n",
              "      <td>0.567456</td>\n",
              "      <td>1</td>\n",
              "      <td>0.030024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0</td>\n",
              "      <td>0.475162</td>\n",
              "      <td>0.564544</td>\n",
              "      <td>0.525028</td>\n",
              "      <td>0.614278</td>\n",
              "      <td>0.614278</td>\n",
              "      <td>0.998973</td>\n",
              "      <td>0.797520</td>\n",
              "      <td>0.809444</td>\n",
              "      <td>0.303743</td>\n",
              "      <td>...</td>\n",
              "      <td>0.811257</td>\n",
              "      <td>0.080006</td>\n",
              "      <td>0.623709</td>\n",
              "      <td>0.614277</td>\n",
              "      <td>0.841252</td>\n",
              "      <td>0.279474</td>\n",
              "      <td>0.027115</td>\n",
              "      <td>0.566339</td>\n",
              "      <td>1</td>\n",
              "      <td>0.030431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0</td>\n",
              "      <td>0.462926</td>\n",
              "      <td>0.516354</td>\n",
              "      <td>0.517105</td>\n",
              "      <td>0.622703</td>\n",
              "      <td>0.622530</td>\n",
              "      <td>0.998997</td>\n",
              "      <td>0.797245</td>\n",
              "      <td>0.809190</td>\n",
              "      <td>0.303212</td>\n",
              "      <td>...</td>\n",
              "      <td>0.784385</td>\n",
              "      <td>0.007563</td>\n",
              "      <td>0.623631</td>\n",
              "      <td>0.622698</td>\n",
              "      <td>0.839029</td>\n",
              "      <td>0.280673</td>\n",
              "      <td>0.026385</td>\n",
              "      <td>0.562016</td>\n",
              "      <td>1</td>\n",
              "      <td>0.026387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0</td>\n",
              "      <td>0.478916</td>\n",
              "      <td>0.541321</td>\n",
              "      <td>0.529311</td>\n",
              "      <td>0.607042</td>\n",
              "      <td>0.607006</td>\n",
              "      <td>0.999017</td>\n",
              "      <td>0.797404</td>\n",
              "      <td>0.809317</td>\n",
              "      <td>0.303449</td>\n",
              "      <td>...</td>\n",
              "      <td>0.797744</td>\n",
              "      <td>0.007308</td>\n",
              "      <td>0.623731</td>\n",
              "      <td>0.607042</td>\n",
              "      <td>0.840206</td>\n",
              "      <td>0.281335</td>\n",
              "      <td>0.028426</td>\n",
              "      <td>0.568219</td>\n",
              "      <td>1</td>\n",
              "      <td>0.024781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0</td>\n",
              "      <td>0.481061</td>\n",
              "      <td>0.539468</td>\n",
              "      <td>0.530864</td>\n",
              "      <td>0.600881</td>\n",
              "      <td>0.600881</td>\n",
              "      <td>0.998975</td>\n",
              "      <td>0.797388</td>\n",
              "      <td>0.809316</td>\n",
              "      <td>0.303508</td>\n",
              "      <td>...</td>\n",
              "      <td>0.798174</td>\n",
              "      <td>0.004593</td>\n",
              "      <td>0.623962</td>\n",
              "      <td>0.600876</td>\n",
              "      <td>0.840193</td>\n",
              "      <td>0.277940</td>\n",
              "      <td>0.029088</td>\n",
              "      <td>0.568609</td>\n",
              "      <td>1</td>\n",
              "      <td>0.039935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0</td>\n",
              "      <td>0.486228</td>\n",
              "      <td>0.548027</td>\n",
              "      <td>0.544140</td>\n",
              "      <td>0.598445</td>\n",
              "      <td>0.598445</td>\n",
              "      <td>0.998953</td>\n",
              "      <td>0.797404</td>\n",
              "      <td>0.809343</td>\n",
              "      <td>0.303584</td>\n",
              "      <td>...</td>\n",
              "      <td>0.801372</td>\n",
              "      <td>0.003425</td>\n",
              "      <td>0.623089</td>\n",
              "      <td>0.598440</td>\n",
              "      <td>0.840386</td>\n",
              "      <td>0.277247</td>\n",
              "      <td>0.028873</td>\n",
              "      <td>0.568501</td>\n",
              "      <td>1</td>\n",
              "      <td>0.047960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0</td>\n",
              "      <td>0.507824</td>\n",
              "      <td>0.571631</td>\n",
              "      <td>0.549226</td>\n",
              "      <td>0.625326</td>\n",
              "      <td>0.625326</td>\n",
              "      <td>0.999136</td>\n",
              "      <td>0.797542</td>\n",
              "      <td>0.809413</td>\n",
              "      <td>0.303442</td>\n",
              "      <td>...</td>\n",
              "      <td>0.815731</td>\n",
              "      <td>0.007173</td>\n",
              "      <td>0.623729</td>\n",
              "      <td>0.625327</td>\n",
              "      <td>0.841574</td>\n",
              "      <td>0.279236</td>\n",
              "      <td>0.026908</td>\n",
              "      <td>0.565659</td>\n",
              "      <td>1</td>\n",
              "      <td>0.031485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0</td>\n",
              "      <td>0.489153</td>\n",
              "      <td>0.562691</td>\n",
              "      <td>0.541517</td>\n",
              "      <td>0.604412</td>\n",
              "      <td>0.604412</td>\n",
              "      <td>0.998971</td>\n",
              "      <td>0.797896</td>\n",
              "      <td>0.809799</td>\n",
              "      <td>0.304406</td>\n",
              "      <td>...</td>\n",
              "      <td>0.811426</td>\n",
              "      <td>0.013698</td>\n",
              "      <td>0.622974</td>\n",
              "      <td>0.604410</td>\n",
              "      <td>0.840979</td>\n",
              "      <td>0.276803</td>\n",
              "      <td>0.027017</td>\n",
              "      <td>0.566045</td>\n",
              "      <td>1</td>\n",
              "      <td>0.055858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0</td>\n",
              "      <td>0.511188</td>\n",
              "      <td>0.566888</td>\n",
              "      <td>0.559077</td>\n",
              "      <td>0.618177</td>\n",
              "      <td>0.618177</td>\n",
              "      <td>0.999077</td>\n",
              "      <td>0.797522</td>\n",
              "      <td>0.809409</td>\n",
              "      <td>0.303529</td>\n",
              "      <td>...</td>\n",
              "      <td>0.813568</td>\n",
              "      <td>0.002623</td>\n",
              "      <td>0.623780</td>\n",
              "      <td>0.618178</td>\n",
              "      <td>0.841197</td>\n",
              "      <td>0.277532</td>\n",
              "      <td>0.026986</td>\n",
              "      <td>0.565941</td>\n",
              "      <td>1</td>\n",
              "      <td>0.044195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0</td>\n",
              "      <td>0.523034</td>\n",
              "      <td>0.575883</td>\n",
              "      <td>0.576423</td>\n",
              "      <td>0.600347</td>\n",
              "      <td>0.600347</td>\n",
              "      <td>0.999027</td>\n",
              "      <td>0.797447</td>\n",
              "      <td>0.809362</td>\n",
              "      <td>0.303503</td>\n",
              "      <td>...</td>\n",
              "      <td>0.816257</td>\n",
              "      <td>0.004076</td>\n",
              "      <td>0.622494</td>\n",
              "      <td>0.600343</td>\n",
              "      <td>0.841802</td>\n",
              "      <td>0.280569</td>\n",
              "      <td>0.027131</td>\n",
              "      <td>0.566383</td>\n",
              "      <td>1</td>\n",
              "      <td>0.026674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0</td>\n",
              "      <td>0.508848</td>\n",
              "      <td>0.553423</td>\n",
              "      <td>0.552920</td>\n",
              "      <td>0.635718</td>\n",
              "      <td>0.635718</td>\n",
              "      <td>0.999062</td>\n",
              "      <td>0.797481</td>\n",
              "      <td>0.809365</td>\n",
              "      <td>0.303490</td>\n",
              "      <td>...</td>\n",
              "      <td>0.807573</td>\n",
              "      <td>0.001521</td>\n",
              "      <td>0.623752</td>\n",
              "      <td>0.635718</td>\n",
              "      <td>0.840820</td>\n",
              "      <td>0.277738</td>\n",
              "      <td>0.026877</td>\n",
              "      <td>0.565538</td>\n",
              "      <td>1</td>\n",
              "      <td>0.041906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0</td>\n",
              "      <td>0.488958</td>\n",
              "      <td>0.549008</td>\n",
              "      <td>0.538947</td>\n",
              "      <td>0.624317</td>\n",
              "      <td>0.624288</td>\n",
              "      <td>0.999098</td>\n",
              "      <td>0.797547</td>\n",
              "      <td>0.809430</td>\n",
              "      <td>0.303529</td>\n",
              "      <td>...</td>\n",
              "      <td>0.806667</td>\n",
              "      <td>0.012883</td>\n",
              "      <td>0.623855</td>\n",
              "      <td>0.624318</td>\n",
              "      <td>0.840771</td>\n",
              "      <td>0.277885</td>\n",
              "      <td>0.026792</td>\n",
              "      <td>0.565164</td>\n",
              "      <td>1</td>\n",
              "      <td>0.040450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0</td>\n",
              "      <td>0.502121</td>\n",
              "      <td>0.563509</td>\n",
              "      <td>0.545854</td>\n",
              "      <td>0.608866</td>\n",
              "      <td>0.608866</td>\n",
              "      <td>0.999048</td>\n",
              "      <td>0.797465</td>\n",
              "      <td>0.809364</td>\n",
              "      <td>0.303491</td>\n",
              "      <td>...</td>\n",
              "      <td>0.813580</td>\n",
              "      <td>0.000887</td>\n",
              "      <td>0.624017</td>\n",
              "      <td>0.608864</td>\n",
              "      <td>0.841275</td>\n",
              "      <td>0.278154</td>\n",
              "      <td>0.026862</td>\n",
              "      <td>0.565476</td>\n",
              "      <td>1</td>\n",
              "      <td>0.038105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0</td>\n",
              "      <td>0.523083</td>\n",
              "      <td>0.563345</td>\n",
              "      <td>0.572354</td>\n",
              "      <td>0.620944</td>\n",
              "      <td>0.620944</td>\n",
              "      <td>0.999102</td>\n",
              "      <td>0.797475</td>\n",
              "      <td>0.809371</td>\n",
              "      <td>0.303396</td>\n",
              "      <td>...</td>\n",
              "      <td>0.809900</td>\n",
              "      <td>0.001675</td>\n",
              "      <td>0.623711</td>\n",
              "      <td>0.620941</td>\n",
              "      <td>0.841282</td>\n",
              "      <td>0.280888</td>\n",
              "      <td>0.027170</td>\n",
              "      <td>0.566487</td>\n",
              "      <td>1</td>\n",
              "      <td>0.025829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0</td>\n",
              "      <td>0.493346</td>\n",
              "      <td>0.550534</td>\n",
              "      <td>0.539804</td>\n",
              "      <td>0.610206</td>\n",
              "      <td>0.610206</td>\n",
              "      <td>0.999023</td>\n",
              "      <td>0.797429</td>\n",
              "      <td>0.809341</td>\n",
              "      <td>0.303480</td>\n",
              "      <td>...</td>\n",
              "      <td>0.799842</td>\n",
              "      <td>0.018859</td>\n",
              "      <td>0.623536</td>\n",
              "      <td>0.610202</td>\n",
              "      <td>0.840599</td>\n",
              "      <td>0.288724</td>\n",
              "      <td>0.028759</td>\n",
              "      <td>0.568437</td>\n",
              "      <td>1</td>\n",
              "      <td>0.017125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0</td>\n",
              "      <td>0.505631</td>\n",
              "      <td>0.559747</td>\n",
              "      <td>0.549173</td>\n",
              "      <td>0.604563</td>\n",
              "      <td>0.604563</td>\n",
              "      <td>0.999031</td>\n",
              "      <td>0.797415</td>\n",
              "      <td>0.809332</td>\n",
              "      <td>0.303439</td>\n",
              "      <td>...</td>\n",
              "      <td>0.802638</td>\n",
              "      <td>0.000717</td>\n",
              "      <td>0.623489</td>\n",
              "      <td>0.604564</td>\n",
              "      <td>0.840860</td>\n",
              "      <td>0.285724</td>\n",
              "      <td>0.028520</td>\n",
              "      <td>0.568286</td>\n",
              "      <td>1</td>\n",
              "      <td>0.018998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0</td>\n",
              "      <td>0.469556</td>\n",
              "      <td>0.527202</td>\n",
              "      <td>0.522512</td>\n",
              "      <td>0.619402</td>\n",
              "      <td>0.619525</td>\n",
              "      <td>0.999045</td>\n",
              "      <td>0.797270</td>\n",
              "      <td>0.809203</td>\n",
              "      <td>0.303157</td>\n",
              "      <td>...</td>\n",
              "      <td>0.788126</td>\n",
              "      <td>0.005376</td>\n",
              "      <td>0.623540</td>\n",
              "      <td>0.619403</td>\n",
              "      <td>0.839285</td>\n",
              "      <td>0.281982</td>\n",
              "      <td>0.025749</td>\n",
              "      <td>0.198632</td>\n",
              "      <td>1</td>\n",
              "      <td>0.023491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>0</td>\n",
              "      <td>0.523717</td>\n",
              "      <td>0.561546</td>\n",
              "      <td>0.568178</td>\n",
              "      <td>0.626414</td>\n",
              "      <td>0.626414</td>\n",
              "      <td>0.999133</td>\n",
              "      <td>0.797526</td>\n",
              "      <td>0.809396</td>\n",
              "      <td>0.303420</td>\n",
              "      <td>...</td>\n",
              "      <td>0.811329</td>\n",
              "      <td>0.001088</td>\n",
              "      <td>0.623689</td>\n",
              "      <td>0.626413</td>\n",
              "      <td>0.841075</td>\n",
              "      <td>0.277759</td>\n",
              "      <td>0.026954</td>\n",
              "      <td>0.565831</td>\n",
              "      <td>1</td>\n",
              "      <td>0.041680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0</td>\n",
              "      <td>0.521913</td>\n",
              "      <td>0.564871</td>\n",
              "      <td>0.568178</td>\n",
              "      <td>0.609291</td>\n",
              "      <td>0.609241</td>\n",
              "      <td>0.999098</td>\n",
              "      <td>0.797498</td>\n",
              "      <td>0.809387</td>\n",
              "      <td>0.303445</td>\n",
              "      <td>...</td>\n",
              "      <td>0.812826</td>\n",
              "      <td>0.000729</td>\n",
              "      <td>0.624307</td>\n",
              "      <td>0.609288</td>\n",
              "      <td>0.840992</td>\n",
              "      <td>0.276193</td>\n",
              "      <td>0.026912</td>\n",
              "      <td>0.565675</td>\n",
              "      <td>1</td>\n",
              "      <td>0.074266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>0</td>\n",
              "      <td>0.503193</td>\n",
              "      <td>0.559693</td>\n",
              "      <td>0.550672</td>\n",
              "      <td>0.603590</td>\n",
              "      <td>0.603072</td>\n",
              "      <td>0.999001</td>\n",
              "      <td>0.797513</td>\n",
              "      <td>0.809398</td>\n",
              "      <td>0.303672</td>\n",
              "      <td>...</td>\n",
              "      <td>0.809661</td>\n",
              "      <td>0.001210</td>\n",
              "      <td>0.623755</td>\n",
              "      <td>0.603586</td>\n",
              "      <td>0.840973</td>\n",
              "      <td>0.277869</td>\n",
              "      <td>0.026991</td>\n",
              "      <td>0.565961</td>\n",
              "      <td>1</td>\n",
              "      <td>0.040601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>1</td>\n",
              "      <td>0.453030</td>\n",
              "      <td>0.516572</td>\n",
              "      <td>0.505595</td>\n",
              "      <td>0.596946</td>\n",
              "      <td>0.596953</td>\n",
              "      <td>0.998940</td>\n",
              "      <td>0.797084</td>\n",
              "      <td>0.809040</td>\n",
              "      <td>0.303052</td>\n",
              "      <td>...</td>\n",
              "      <td>0.777094</td>\n",
              "      <td>0.040161</td>\n",
              "      <td>0.617349</td>\n",
              "      <td>0.596946</td>\n",
              "      <td>0.837053</td>\n",
              "      <td>0.290904</td>\n",
              "      <td>0.026050</td>\n",
              "      <td>0.553328</td>\n",
              "      <td>1</td>\n",
              "      <td>0.016199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>1</td>\n",
              "      <td>0.472091</td>\n",
              "      <td>0.538814</td>\n",
              "      <td>0.517051</td>\n",
              "      <td>0.597393</td>\n",
              "      <td>0.597393</td>\n",
              "      <td>0.998960</td>\n",
              "      <td>0.797366</td>\n",
              "      <td>0.809290</td>\n",
              "      <td>0.303501</td>\n",
              "      <td>...</td>\n",
              "      <td>0.791982</td>\n",
              "      <td>0.007233</td>\n",
              "      <td>0.622323</td>\n",
              "      <td>0.597394</td>\n",
              "      <td>0.839162</td>\n",
              "      <td>0.300005</td>\n",
              "      <td>0.022762</td>\n",
              "      <td>0.571989</td>\n",
              "      <td>1</td>\n",
              "      <td>0.014054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>1</td>\n",
              "      <td>0.066933</td>\n",
              "      <td>0.057185</td>\n",
              "      <td>0.054821</td>\n",
              "      <td>0.601861</td>\n",
              "      <td>0.601861</td>\n",
              "      <td>0.998825</td>\n",
              "      <td>0.796779</td>\n",
              "      <td>0.808717</td>\n",
              "      <td>0.302760</td>\n",
              "      <td>...</td>\n",
              "      <td>0.525651</td>\n",
              "      <td>0.005803</td>\n",
              "      <td>0.623648</td>\n",
              "      <td>0.601857</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.182790</td>\n",
              "      <td>0.026763</td>\n",
              "      <td>0.565021</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>1</td>\n",
              "      <td>0.406669</td>\n",
              "      <td>0.467292</td>\n",
              "      <td>0.453450</td>\n",
              "      <td>0.595994</td>\n",
              "      <td>0.595994</td>\n",
              "      <td>0.998831</td>\n",
              "      <td>0.797191</td>\n",
              "      <td>0.809128</td>\n",
              "      <td>0.303466</td>\n",
              "      <td>...</td>\n",
              "      <td>0.746586</td>\n",
              "      <td>0.001053</td>\n",
              "      <td>0.623323</td>\n",
              "      <td>0.595994</td>\n",
              "      <td>0.831082</td>\n",
              "      <td>0.294222</td>\n",
              "      <td>0.026590</td>\n",
              "      <td>0.563976</td>\n",
              "      <td>1</td>\n",
              "      <td>0.015186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>0</td>\n",
              "      <td>0.482231</td>\n",
              "      <td>0.544320</td>\n",
              "      <td>0.536431</td>\n",
              "      <td>0.618083</td>\n",
              "      <td>0.618083</td>\n",
              "      <td>0.999004</td>\n",
              "      <td>0.797389</td>\n",
              "      <td>0.809315</td>\n",
              "      <td>0.303451</td>\n",
              "      <td>...</td>\n",
              "      <td>0.797585</td>\n",
              "      <td>0.014207</td>\n",
              "      <td>0.623644</td>\n",
              "      <td>0.618084</td>\n",
              "      <td>0.840280</td>\n",
              "      <td>0.289105</td>\n",
              "      <td>0.032353</td>\n",
              "      <td>0.569393</td>\n",
              "      <td>1</td>\n",
              "      <td>0.016943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>0</td>\n",
              "      <td>0.488519</td>\n",
              "      <td>0.550153</td>\n",
              "      <td>0.540339</td>\n",
              "      <td>0.600737</td>\n",
              "      <td>0.600758</td>\n",
              "      <td>0.998997</td>\n",
              "      <td>0.797409</td>\n",
              "      <td>0.809330</td>\n",
              "      <td>0.303502</td>\n",
              "      <td>...</td>\n",
              "      <td>0.804706</td>\n",
              "      <td>0.011073</td>\n",
              "      <td>0.623561</td>\n",
              "      <td>0.600737</td>\n",
              "      <td>0.840818</td>\n",
              "      <td>0.280855</td>\n",
              "      <td>0.027157</td>\n",
              "      <td>0.566455</td>\n",
              "      <td>1</td>\n",
              "      <td>0.025913</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60 rows × 96 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-46a19330-515a-4e13-81ea-9342ffd757a4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-46a19330-515a-4e13-81ea-9342ffd757a4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-46a19330-515a-4e13-81ea-9342ffd757a4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# open file\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "# drop any duplicate records\n",
        "data.drop_duplicates(inplace=True)\n",
        "# fill in missing values with mean values\n",
        "data.fillna(data.mean(), inplace=True)\n",
        "\n",
        "(data.head(60))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use decision tree to determine the best number of features\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# features = data.iloc[:,1:96]\n",
        "# labels = data.iloc[:, 0:1]\n",
        "# labels = labels.values.ravel()\n",
        "\n",
        "trainFeatures = []\n",
        "trainLabels = []\n",
        "testFeatures = []\n",
        "testLabels = []\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "#We shouldn't actually split these right here\n",
        "trainFeatures, testFeatures, trainLabels, testLabels = sk.model_selection.train_test_split(data.iloc[:, 1:96], data.iloc[:, 0:1], train_size = 0.8, test_size = 0.2, random_state = 42)\n",
        "features = data.iloc[:, 1:96]\n",
        "labels = data.iloc[:, 0:1]\n",
        "# we want to find which features are consistently the most important\n",
        "bestParams = 0\n",
        "cumulative_best_features = np.array([0.0 for i in range(95)])\n",
        "for i in range(1):\n",
        "    # decTreeTest = decTreeTest.fit(trainFeatures, trainLabels)\n",
        "\n",
        "    Hyperparameters={'max_features':np.arange(1,95,5)}\n",
        "    dectree= tree.DecisionTreeClassifier(random_state=42)\n",
        "    cv_grid = GridSearchCV(estimator= dectree ,param_grid = Hyperparameters, \n",
        "                           scoring ='accuracy',cv = 5)\n",
        "    cv_grid= cv_grid.fit(features, labels)\n",
        "    bestParams = cv_grid\n",
        "    print(cv_grid.best_params_)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLv6MPUyNCaK",
        "outputId": "62cae99f-82de-4616-d3d9-9b8cbbdfbbc8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'max_features': 21}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "featImport = cv_grid.best_estimator_.feature_importances_\n",
        "\n",
        "best_max_features = cv_grid.best_params_['max_features']\n",
        "\n",
        "cumulative_feature_importances = pd.DataFrame(cv_grid.best_estimator_.feature_importances_,\n",
        "                                index = trainFeatures.columns,\n",
        "                                columns = ['importance']).sort_values('importance', \n",
        "                                                                    ascending=False)\n",
        "cumulative_feature_importances.head(best_max_features)\n",
        "\n",
        "reduced_feature_names = cumulative_feature_importances.head(best_max_features).index.values\n",
        "print(reduced_feature_names)\n",
        "\n",
        "reduced_train_features = trainFeatures.loc[:,reduced_feature_names]\n",
        "reduced_test_features = testFeatures.loc[:, reduced_feature_names]\n",
        "reduced_train_features.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "DCdWHzJ5NCJ7",
        "outputId": "e3123058-08e7-43c6-bd13-fe4d28c32e9c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' Borrowing dependency' ' Net Value Growth Rate'\n",
            " ' ROA(B) before interest and depreciation after tax'\n",
            " ' Interest Coverage Ratio (Interest expense to EBIT)'\n",
            " ' Total Asset Return Growth Rate Ratio' ' No-credit Interval'\n",
            " ' Average Collection Days' ' Retained Earnings to Total Assets'\n",
            " ' Current Liability to Equity' ' Cash/Total Assets'\n",
            " ' Persistent EPS in the Last Four Seasons' ' Realized Sales Gross Margin'\n",
            " ' Fixed Assets Turnover Frequency' ' Quick Ratio'\n",
            " ' Current Liability to Current Assets' ' Revenue per person'\n",
            " ' Per Share Net profit before tax (Yuan ¥)'\n",
            " ' Accounts Receivable Turnover' ' Degree of Financial Leverage (DFL)'\n",
            " ' Net Income to Total Assets' ' Current Liabilities/Equity']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Borrowing dependency   Net Value Growth Rate  \\\n",
              "3759               0.378194                0.000433   \n",
              "1782               0.374378                0.000469   \n",
              "5013               0.373293                0.000536   \n",
              "5412               0.379327                0.000478   \n",
              "3066               0.372543                0.000442   \n",
              "3204               0.374816                0.000428   \n",
              "2499               0.371236                0.000474   \n",
              "3195               0.372124                0.000414   \n",
              "4461               0.370367                0.000448   \n",
              "3927               0.369637                0.000531   \n",
              "\n",
              "       ROA(B) before interest and depreciation after tax  \\\n",
              "3759                                           0.544622    \n",
              "1782                                           0.558863    \n",
              "5013                                           0.554687    \n",
              "5412                                           0.546764    \n",
              "3066                                           0.529150    \n",
              "3204                                           0.517426    \n",
              "2499                                           0.619573    \n",
              "3195                                           0.532523    \n",
              "4461                                           0.527384    \n",
              "3927                                           0.546121    \n",
              "\n",
              "       Interest Coverage Ratio (Interest expense to EBIT)  \\\n",
              "3759                                           0.566658     \n",
              "1782                                           0.565395     \n",
              "5013                                           0.565484     \n",
              "5412                                           0.565820     \n",
              "3066                                           0.565848     \n",
              "3204                                           0.564066     \n",
              "2499                                           0.565194     \n",
              "3195                                           0.564712     \n",
              "4461                                           0.565744     \n",
              "3927                                           0.565159     \n",
              "\n",
              "       Total Asset Return Growth Rate Ratio   No-credit Interval  \\\n",
              "3759                               0.263600             0.623649   \n",
              "1782                               0.264168             0.623932   \n",
              "5013                               0.263670             0.623714   \n",
              "5412                               0.264225             0.623986   \n",
              "3066                               0.263924             0.623845   \n",
              "3204                               0.263718             0.623591   \n",
              "2499                               0.263661             0.632213   \n",
              "3195                               0.263237             0.623663   \n",
              "4461                               0.263851             0.623788   \n",
              "3927                               0.264125             0.624892   \n",
              "\n",
              "       Average Collection Days   Retained Earnings to Total Assets  \\\n",
              "3759                  0.007710                            0.940112   \n",
              "1782                  0.010125                            0.937921   \n",
              "5013                  0.007109                            0.943937   \n",
              "5412                  0.006349                            0.936050   \n",
              "3066                  0.009149                            0.936134   \n",
              "3204                  0.007685                            0.927333   \n",
              "2499                  0.000408                            0.967305   \n",
              "3195                  0.007190                            0.923992   \n",
              "4461                  0.004183                            0.943536   \n",
              "3927                  0.009070                            0.935300   \n",
              "\n",
              "       Current Liability to Equity   Cash/Total Assets  ...  \\\n",
              "3759                      0.330333            0.032719  ...   \n",
              "1782                      0.333230            0.207601  ...   \n",
              "5013                      0.328602            0.061019  ...   \n",
              "5412                      0.338004            0.018261  ...   \n",
              "3066                      0.332926            0.014452  ...   \n",
              "3204                      0.328254            0.036013  ...   \n",
              "2499                      0.327421            0.001967  ...   \n",
              "3195                      0.328290            0.074751  ...   \n",
              "4461                      0.330262            0.150690  ...   \n",
              "3927                      0.327279            0.263469  ...   \n",
              "\n",
              "       Realized Sales Gross Margin   Fixed Assets Turnover Frequency  \\\n",
              "3759                      0.599036                      1.913939e-04   \n",
              "1782                      0.609334                      5.545128e-03   \n",
              "5013                      0.614055                      3.154596e-04   \n",
              "5412                      0.597825                      1.985807e-03   \n",
              "3066                      0.600362                      8.745114e-04   \n",
              "3204                      0.595267                      3.560000e+09   \n",
              "2499                      0.642824                      1.127182e-04   \n",
              "3195                      0.598466                      8.090000e+09   \n",
              "4461                      0.600578                      3.517715e-04   \n",
              "3927                      0.609183                      7.269943e-04   \n",
              "\n",
              "       Quick Ratio   Current Liability to Current Assets   Revenue per person  \\\n",
              "3759      0.005780                              0.031490             0.020366   \n",
              "1782      0.007728                              0.024709             0.026597   \n",
              "5013      0.005343                              0.033789             0.006357   \n",
              "5412      0.007319                              0.031170             0.141551   \n",
              "3066      0.007495                              0.033229             0.046103   \n",
              "3204      0.006751                              0.034430             0.005508   \n",
              "2499      0.000147                              0.916814             0.049525   \n",
              "3195      0.007264                              0.033287             0.006237   \n",
              "4461      0.006545                              0.034859             0.021553   \n",
              "3927      0.036499                              0.007222             0.024206   \n",
              "\n",
              "       Per Share Net profit before tax (Yuan ¥)  \\\n",
              "3759                                   0.175943   \n",
              "1782                                   0.177299   \n",
              "5013                                   0.183776   \n",
              "5412                                   0.186563   \n",
              "3066                                   0.178203   \n",
              "3204                                   0.162537   \n",
              "2499                                   0.210891   \n",
              "3195                                   0.159072   \n",
              "4461                                   0.174060   \n",
              "3927                                   0.174964   \n",
              "\n",
              "       Accounts Receivable Turnover   Degree of Financial Leverage (DFL)  \\\n",
              "3759                       0.000820                             0.027239   \n",
              "1782                       0.000625                             0.026843   \n",
              "5013                       0.000889                             0.026864   \n",
              "5412                       0.000996                             0.026951   \n",
              "3066                       0.000692                             0.026959   \n",
              "3204                       0.000824                             0.026603   \n",
              "2499                       0.015503                             0.026799   \n",
              "3195                       0.000881                             0.026705   \n",
              "4461                       0.001512                             0.026930   \n",
              "3927                       0.000698                             0.026791   \n",
              "\n",
              "       Net Income to Total Assets   Current Liabilities/Equity  \n",
              "3759                     0.801313                     0.330333  \n",
              "1782                     0.810914                     0.333230  \n",
              "5013                     0.809740                     0.328602  \n",
              "5412                     0.810082                     0.338004  \n",
              "3066                     0.804638                     0.332926  \n",
              "3204                     0.781277                     0.328254  \n",
              "2499                     0.857531                     0.327421  \n",
              "3195                     0.771614                     0.328290  \n",
              "4461                     0.799975                     0.330262  \n",
              "3927                     0.808044                     0.327279  \n",
              "\n",
              "[10 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2f3d0fae-823d-4807-b385-fead19be4e24\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Borrowing dependency</th>\n",
              "      <th>Net Value Growth Rate</th>\n",
              "      <th>ROA(B) before interest and depreciation after tax</th>\n",
              "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
              "      <th>Total Asset Return Growth Rate Ratio</th>\n",
              "      <th>No-credit Interval</th>\n",
              "      <th>Average Collection Days</th>\n",
              "      <th>Retained Earnings to Total Assets</th>\n",
              "      <th>Current Liability to Equity</th>\n",
              "      <th>Cash/Total Assets</th>\n",
              "      <th>...</th>\n",
              "      <th>Realized Sales Gross Margin</th>\n",
              "      <th>Fixed Assets Turnover Frequency</th>\n",
              "      <th>Quick Ratio</th>\n",
              "      <th>Current Liability to Current Assets</th>\n",
              "      <th>Revenue per person</th>\n",
              "      <th>Per Share Net profit before tax (Yuan ¥)</th>\n",
              "      <th>Accounts Receivable Turnover</th>\n",
              "      <th>Degree of Financial Leverage (DFL)</th>\n",
              "      <th>Net Income to Total Assets</th>\n",
              "      <th>Current Liabilities/Equity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3759</th>\n",
              "      <td>0.378194</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.544622</td>\n",
              "      <td>0.566658</td>\n",
              "      <td>0.263600</td>\n",
              "      <td>0.623649</td>\n",
              "      <td>0.007710</td>\n",
              "      <td>0.940112</td>\n",
              "      <td>0.330333</td>\n",
              "      <td>0.032719</td>\n",
              "      <td>...</td>\n",
              "      <td>0.599036</td>\n",
              "      <td>1.913939e-04</td>\n",
              "      <td>0.005780</td>\n",
              "      <td>0.031490</td>\n",
              "      <td>0.020366</td>\n",
              "      <td>0.175943</td>\n",
              "      <td>0.000820</td>\n",
              "      <td>0.027239</td>\n",
              "      <td>0.801313</td>\n",
              "      <td>0.330333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1782</th>\n",
              "      <td>0.374378</td>\n",
              "      <td>0.000469</td>\n",
              "      <td>0.558863</td>\n",
              "      <td>0.565395</td>\n",
              "      <td>0.264168</td>\n",
              "      <td>0.623932</td>\n",
              "      <td>0.010125</td>\n",
              "      <td>0.937921</td>\n",
              "      <td>0.333230</td>\n",
              "      <td>0.207601</td>\n",
              "      <td>...</td>\n",
              "      <td>0.609334</td>\n",
              "      <td>5.545128e-03</td>\n",
              "      <td>0.007728</td>\n",
              "      <td>0.024709</td>\n",
              "      <td>0.026597</td>\n",
              "      <td>0.177299</td>\n",
              "      <td>0.000625</td>\n",
              "      <td>0.026843</td>\n",
              "      <td>0.810914</td>\n",
              "      <td>0.333230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5013</th>\n",
              "      <td>0.373293</td>\n",
              "      <td>0.000536</td>\n",
              "      <td>0.554687</td>\n",
              "      <td>0.565484</td>\n",
              "      <td>0.263670</td>\n",
              "      <td>0.623714</td>\n",
              "      <td>0.007109</td>\n",
              "      <td>0.943937</td>\n",
              "      <td>0.328602</td>\n",
              "      <td>0.061019</td>\n",
              "      <td>...</td>\n",
              "      <td>0.614055</td>\n",
              "      <td>3.154596e-04</td>\n",
              "      <td>0.005343</td>\n",
              "      <td>0.033789</td>\n",
              "      <td>0.006357</td>\n",
              "      <td>0.183776</td>\n",
              "      <td>0.000889</td>\n",
              "      <td>0.026864</td>\n",
              "      <td>0.809740</td>\n",
              "      <td>0.328602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5412</th>\n",
              "      <td>0.379327</td>\n",
              "      <td>0.000478</td>\n",
              "      <td>0.546764</td>\n",
              "      <td>0.565820</td>\n",
              "      <td>0.264225</td>\n",
              "      <td>0.623986</td>\n",
              "      <td>0.006349</td>\n",
              "      <td>0.936050</td>\n",
              "      <td>0.338004</td>\n",
              "      <td>0.018261</td>\n",
              "      <td>...</td>\n",
              "      <td>0.597825</td>\n",
              "      <td>1.985807e-03</td>\n",
              "      <td>0.007319</td>\n",
              "      <td>0.031170</td>\n",
              "      <td>0.141551</td>\n",
              "      <td>0.186563</td>\n",
              "      <td>0.000996</td>\n",
              "      <td>0.026951</td>\n",
              "      <td>0.810082</td>\n",
              "      <td>0.338004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3066</th>\n",
              "      <td>0.372543</td>\n",
              "      <td>0.000442</td>\n",
              "      <td>0.529150</td>\n",
              "      <td>0.565848</td>\n",
              "      <td>0.263924</td>\n",
              "      <td>0.623845</td>\n",
              "      <td>0.009149</td>\n",
              "      <td>0.936134</td>\n",
              "      <td>0.332926</td>\n",
              "      <td>0.014452</td>\n",
              "      <td>...</td>\n",
              "      <td>0.600362</td>\n",
              "      <td>8.745114e-04</td>\n",
              "      <td>0.007495</td>\n",
              "      <td>0.033229</td>\n",
              "      <td>0.046103</td>\n",
              "      <td>0.178203</td>\n",
              "      <td>0.000692</td>\n",
              "      <td>0.026959</td>\n",
              "      <td>0.804638</td>\n",
              "      <td>0.332926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3204</th>\n",
              "      <td>0.374816</td>\n",
              "      <td>0.000428</td>\n",
              "      <td>0.517426</td>\n",
              "      <td>0.564066</td>\n",
              "      <td>0.263718</td>\n",
              "      <td>0.623591</td>\n",
              "      <td>0.007685</td>\n",
              "      <td>0.927333</td>\n",
              "      <td>0.328254</td>\n",
              "      <td>0.036013</td>\n",
              "      <td>...</td>\n",
              "      <td>0.595267</td>\n",
              "      <td>3.560000e+09</td>\n",
              "      <td>0.006751</td>\n",
              "      <td>0.034430</td>\n",
              "      <td>0.005508</td>\n",
              "      <td>0.162537</td>\n",
              "      <td>0.000824</td>\n",
              "      <td>0.026603</td>\n",
              "      <td>0.781277</td>\n",
              "      <td>0.328254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2499</th>\n",
              "      <td>0.371236</td>\n",
              "      <td>0.000474</td>\n",
              "      <td>0.619573</td>\n",
              "      <td>0.565194</td>\n",
              "      <td>0.263661</td>\n",
              "      <td>0.632213</td>\n",
              "      <td>0.000408</td>\n",
              "      <td>0.967305</td>\n",
              "      <td>0.327421</td>\n",
              "      <td>0.001967</td>\n",
              "      <td>...</td>\n",
              "      <td>0.642824</td>\n",
              "      <td>1.127182e-04</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>0.916814</td>\n",
              "      <td>0.049525</td>\n",
              "      <td>0.210891</td>\n",
              "      <td>0.015503</td>\n",
              "      <td>0.026799</td>\n",
              "      <td>0.857531</td>\n",
              "      <td>0.327421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3195</th>\n",
              "      <td>0.372124</td>\n",
              "      <td>0.000414</td>\n",
              "      <td>0.532523</td>\n",
              "      <td>0.564712</td>\n",
              "      <td>0.263237</td>\n",
              "      <td>0.623663</td>\n",
              "      <td>0.007190</td>\n",
              "      <td>0.923992</td>\n",
              "      <td>0.328290</td>\n",
              "      <td>0.074751</td>\n",
              "      <td>...</td>\n",
              "      <td>0.598466</td>\n",
              "      <td>8.090000e+09</td>\n",
              "      <td>0.007264</td>\n",
              "      <td>0.033287</td>\n",
              "      <td>0.006237</td>\n",
              "      <td>0.159072</td>\n",
              "      <td>0.000881</td>\n",
              "      <td>0.026705</td>\n",
              "      <td>0.771614</td>\n",
              "      <td>0.328290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4461</th>\n",
              "      <td>0.370367</td>\n",
              "      <td>0.000448</td>\n",
              "      <td>0.527384</td>\n",
              "      <td>0.565744</td>\n",
              "      <td>0.263851</td>\n",
              "      <td>0.623788</td>\n",
              "      <td>0.004183</td>\n",
              "      <td>0.943536</td>\n",
              "      <td>0.330262</td>\n",
              "      <td>0.150690</td>\n",
              "      <td>...</td>\n",
              "      <td>0.600578</td>\n",
              "      <td>3.517715e-04</td>\n",
              "      <td>0.006545</td>\n",
              "      <td>0.034859</td>\n",
              "      <td>0.021553</td>\n",
              "      <td>0.174060</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.026930</td>\n",
              "      <td>0.799975</td>\n",
              "      <td>0.330262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3927</th>\n",
              "      <td>0.369637</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>0.546121</td>\n",
              "      <td>0.565159</td>\n",
              "      <td>0.264125</td>\n",
              "      <td>0.624892</td>\n",
              "      <td>0.009070</td>\n",
              "      <td>0.935300</td>\n",
              "      <td>0.327279</td>\n",
              "      <td>0.263469</td>\n",
              "      <td>...</td>\n",
              "      <td>0.609183</td>\n",
              "      <td>7.269943e-04</td>\n",
              "      <td>0.036499</td>\n",
              "      <td>0.007222</td>\n",
              "      <td>0.024206</td>\n",
              "      <td>0.174964</td>\n",
              "      <td>0.000698</td>\n",
              "      <td>0.026791</td>\n",
              "      <td>0.808044</td>\n",
              "      <td>0.327279</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f3d0fae-823d-4807-b385-fead19be4e24')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2f3d0fae-823d-4807-b385-fead19be4e24 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2f3d0fae-823d-4807-b385-fead19be4e24');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI-PFuN97f4m"
      },
      "source": [
        "### Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "scrolled": true,
        "id": "Kk4DCbz27f4n",
        "outputId": "82846dcd-e66a-47e7-d22f-f2702e9f26b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of bankrupt companies: 220\n",
            "Number of non-bankrupt companies: 6599\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total bankruptcies of companies with a borrowing dependency (BD) of greather than or equal to .35:  24 \n",
            "\n",
            "Total bankruptcies of companies with a borrowing dependency (BD) of less than .35:  196 \n",
            "\n",
            "Percentage of companies that go bankrupt with a BD of greather than or equal to .35:  0.3429 \n",
            "\n",
            "Percentage of companies that go bankrupt with a BD of less than .35:  0.3429 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "dataTest = data.copy(deep = True)\n",
        "\n",
        "num_bankrupt = sum(data['Bankrupt?'])\n",
        "num_nonBankrupt = len(labels) - num_bankrupt\n",
        "\n",
        "print(\"Number of bankrupt companies:\", num_bankrupt)\n",
        "print(\"Number of non-bankrupt companies:\", num_nonBankrupt)\n",
        "\n",
        "denomination = 'Non-Bankrupt', 'Bankrupt'\n",
        "sizes = [num_nonBankrupt, num_bankrupt]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.pie(sizes, labels=denomination, autopct='%1.1f%%')\n",
        "\n",
        "bdScatter = plt.figure(\"Borrowing Dependency & Bankruptcy\")\n",
        "plt.scatter(data[' Borrowing dependency'], data['Bankrupt?'], alpha=0.5)\n",
        "plt.xlabel('Borrowing Dependency')\n",
        "plt.ylabel('Bankruptcy')\n",
        "plt.show()\n",
        "\n",
        "# dataTest[' Borrowing dependcy >= 0.2'] = dataTest[' Borrowing dependency'].apply(lambda x: 1 if x >= 0.2 else 0)\n",
        "# dataTest[' Borrowing dependcy < 0.2'] = dataTest[' Borrowing dependency'].apply(lambda x: 1 if x < 0.2 else 0)\n",
        "\n",
        "bdlt_bankruptcies = 0\n",
        "bgt_bankruptcies = 0\n",
        "bdlt_total = 0\n",
        "bgt_total = 0\n",
        "for i in range(len(data)):\n",
        "  if(data.iloc[i][' Borrowing dependency'] >= 0.4):\n",
        "    bgt_total +=1\n",
        "    if(data.iloc[i]['Bankrupt?'] == 1):\n",
        "      bgt_bankruptcies += 1\n",
        "  elif(data.iloc[i][' Borrowing dependency'] < 0.4):\n",
        "    bdlt_total +=1\n",
        "    if(data.iloc[i]['Bankrupt?'] == 1):\n",
        "      bdlt_bankruptcies += 1\n",
        " \n",
        "\n",
        "bgt_bankruptcies_pct = float(bgt_bankruptcies)/bgt_total\n",
        "bdlt_bankruptcies_pct = float(bgt_bankruptcies)/bgt_total\n",
        "bdlt_bankruptcies_pct = round(bdlt_bankruptcies_pct, 4)\n",
        "bgt_bankruptcies_pct = round(bgt_bankruptcies_pct, 4)\n",
        "print(\"Total bankruptcies of companies with a borrowing dependency (BD) of greather than or equal to .35: \", bgt_bankruptcies, \"\\n\")\n",
        "print(\"Total bankruptcies of companies with a borrowing dependency (BD) of less than .35: \",bdlt_bankruptcies, \"\\n\")\n",
        "print(\"Percentage of companies that go bankrupt with a BD of greather than or equal to .35: \", bgt_bankruptcies_pct, \"\\n\")\n",
        "print(\"Percentage of companies that go bankrupt with a BD of less than .35: \", bdlt_bankruptcies_pct, \"\\n\")\n",
        "\n",
        "\n",
        "# Separate the target variable (bankruptcy) from the features\n",
        "feat = reduced_train_features[[' Borrowing dependency', ' Net Value Growth Rate']]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eudijX_g7f4o"
      },
      "source": [
        "### Ensemble + Random Forests\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOcM224l7f4o"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "gHXn3BqK7f4o",
        "outputId": "14858be1-1890-425f-b8d7-477f0b202033",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98      1313\n",
            "           1       0.69      0.18      0.28        51\n",
            "\n",
            "    accuracy                           0.97      1364\n",
            "   macro avg       0.83      0.59      0.63      1364\n",
            "weighted avg       0.96      0.97      0.96      1364\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "rfc = RandomForestClassifier()\n",
        "\n",
        "param_grid = {'n_estimators': [50, 100, 150]}\n",
        "\n",
        "grid_search = GridSearchCV(rfc, param_grid, cv = 5)\n",
        "rfc_scores = cross_val_score(grid_search, reduced_train_features, trainLabels, cv = 5)\n",
        "\n",
        "\n",
        "grid_search.fit(reduced_train_features, trainLabels)\n",
        "print(classification_report(testLabels, grid_search.predict(reduced_test_features)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F2saIlo0w2s"
      },
      "source": [
        "### ADA Boost Classifier\n",
        "- Random Forests builds its base estimators independently, using bagging. There is another method of training ensemble classifiers called boosting. Here the classifiers are trained sequentially and each time the sampling of the training set depends on the performance of previously generated models.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "cETc8uxK7f4p",
        "outputId": "d61a0264-46df-4c74-f7a8-aa2277785016",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98      1313\n",
            "           1       0.57      0.41      0.48        51\n",
            "\n",
            "    accuracy                           0.97      1364\n",
            "   macro avg       0.77      0.70      0.73      1364\n",
            "weighted avg       0.96      0.97      0.96      1364\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_boost_classifier = AdaBoostClassifier()\n",
        "\n",
        "param_grid = {'n_estimators': [50, 100, 150]}\n",
        "\n",
        "grid_search = GridSearchCV(ada_boost_classifier, param_grid, cv=5)\n",
        "\n",
        "abc_scores = cross_val_score(grid_search, reduced_train_features, trainLabels, cv=5)\n",
        "\n",
        "grid_search.fit(reduced_train_features, trainLabels)\n",
        "print(classification_report(testLabels, grid_search.predict(reduced_test_features)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFhluJzO7f4q"
      },
      "source": [
        "**Cross Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "scrolled": true,
        "id": "0hweuAef7f4q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Q0F9_J7f4q"
      },
      "source": [
        "**Nested Cross Validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQq0TAli7f4r"
      },
      "source": [
        "Q5. Now we want to tune our model to use the best parameters to avoid overfitting to our training data. Grid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters (hyperparameters) specified in a grid. \n",
        "* Use `sklearn.model_selection.GridSearchCV` to find the best `max_depth`, `max_features`, and `min_samples_leaf` for your tree. Use a 5-fold-CV and 'accuracy' for the scoring criteria.\n",
        "* Try the values [5,10,15,20] for `max_depth` and `min_samples_leaf`. Try [5,10,15] for `max_features`. \n",
        "* Print out the best value for each of the tested parameters (`best_params_`).\n",
        "* Print out the accuracy of the model with these best values (`best_score_`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "NMFgq1kh7f4r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izMZ0lA-7f4r"
      },
      "source": [
        "Q6. What you did in Q5 performed the _inner_ loop of a nested CV (no test set was held out). What you did in Q4 performed an _outer_ loop of CV (holds out a test set). Now we need to combine them to perform the nested cross-validation that we discussed in class. To do this, you'll need to pass the a `GridSearchCV` into a `cross_val_score`. \n",
        "\n",
        "What this does is: the `cross_val_score` splits the data in to train and test sets for the first outer fold, and it passes the train set into `GridSearchCV`. `GridSearchCV` then splits that set into train and validation sets for k number of folds (the inner CV loop). The hyper-parameters for which the average score over all inner iterations is best, is reported as the `best_params_`, `best_score_`, and `best_estimator_`(best decision tree). This best decision tree is then evaluated with the test set from the `cross_val_score` (the outer CV loop). And this whole thing is repeated for the remaining k folds of the `cross_val_score` (the outer CV loop). \n",
        "\n",
        "That is a lot of explanation for a very complex (but IMPORTANT) process, which can all be performed with a single line of code!\n",
        "\n",
        "Be patient for this one to run. The nested cross-validation loop can take some time. A [ * ] next to the cell indicates that it is still running.\n",
        "\n",
        "Print the accuracy of your tuned, cross-validated model. This is the official accuracy that you would report for your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ZBT2kSAW7f4r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEukPPgq7f4s"
      },
      "source": [
        "### C. Naive Bayes (NB) & Evaluation Metrics\n",
        "\n",
        "`sklearn.naive_bayes.GaussianNB` implements the Gaussian Naive Bayes algorithm for classification. This means that the liklihood of continuous features is estimated using a Gaussian distribution. (Refer to slide 13 of the Naive Bayes powerpoint notes.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEyo34kr7f4s"
      },
      "source": [
        "Q7. Create a `sklearn.naive_bayes.GaussianNB` classifier. Use `sklearn.model_selection.cross_val_score` to do a 10-fold cross validation on the classifier. Display the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY8tkteY7f4s",
        "outputId": "c2b07c1e-b37f-4951-8ebc-64c7854220b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy:  0.5947826086956522\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "nb_clf = naive_bayes.GaussianNB()\n",
        "# nb_clf.fit(feature_vals, class_labels)\n",
        "accs = model_selection.cross_val_score(\n",
        "    estimator=nb_clf,\n",
        "    X=feature_vals,\n",
        "    y=class_labels,\n",
        "    cv=10\n",
        "    \n",
        ")\n",
        "\n",
        "acc = np.mean(accs)\n",
        "print(\"accuracy: \", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ6O9h-F7f4t"
      },
      "source": [
        "Q8. `cross_val_score` returns the scores of every test fold. There is another function called `cross_val_predict` that returns predicted y values for every record in the test fold. In other words, for each element in the input, `cross_val_predict` returns the prediction that was obtained for that element when it was in the test set. \n",
        "\n",
        "* Use `cross_val_predict` and `sklearn.metrics.confusion_matrix` to print the confusion matrix for the classifier.\n",
        "\n",
        "* Sckit-learn also provides a useful function `sklearn.metrics.classification_report` for evaluating the classifier on a per-class basis. It is a summary of the precision, recall, and F1 score for each class (and support is just the actual class count). Display the classification report for your Naive Bayes classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWoYN4QB7f4t",
        "outputId": "cc75aa57-9543-42c9-f787-b9b4cb056025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix:\n",
            " [[500  39]\n",
            " [427 184]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.93      0.68       539\n",
            "           1       0.83      0.30      0.44       611\n",
            "\n",
            "    accuracy                           0.59      1150\n",
            "   macro avg       0.68      0.61      0.56      1150\n",
            "weighted avg       0.69      0.59      0.55      1150\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "cross_val_preds = model_selection.cross_val_predict(\n",
        "    estimator=nb_clf, \n",
        "    X=feature_vals, \n",
        "    y=class_labels, \n",
        "    cv=10\n",
        ")\n",
        "confusion = metrics.confusion_matrix(y_true=class_labels, y_pred=cross_val_preds)\n",
        "\n",
        "report = metrics.classification_report(\n",
        "    y_true=class_labels,\n",
        "    y_pred=cross_val_preds,\n",
        ")\n",
        "\n",
        "print(\"confusion matrix:\\n\", confusion)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W1aQQdy7f4t"
      },
      "source": [
        "Q9. Using `sklearn.metrics.roc_curve` plot a ROC curve for the Naive Bayes classifier. Also calculate the area under the curve (AUC) using `sklearn.metrics.roc_auc_score`.\n",
        "\n",
        "* We will just do this on a single holdout test set (because it gets more complicated to put this inside of a cross-validation). So, split your data into training and test sets using `sklearn.model_selection.train_test_split`. Do an 80/20 split.\n",
        "* Fit the Naive Bayes classifier to the training data by calling the `fit` method on the trainng data.\n",
        "* Now call the `predict_proba` method on your classifier and pass in the test data. This will return a 2D numpy array with one row for each datapoint in the test set and 2 columns. Column index 0 is the probability that this datapoint is in class 0, and column index 1 is the probability that this datapoint is in class 1.\n",
        "* We are going to say that class 1 (having the disease) is the rare/positive class. To create a ROC curve, pass the actual Y labels and the probabilites of class 1 (column index 1 out of your predict_proba result) into `sklearn.metrics.roc_curve`\n",
        "* Pass the FPR and TPR that `roc_curve` returns into the plotting code that we have provided you.\n",
        "* Print the AUC (area under the curve) by using `sklearn.metrics.roc_auc_score`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqzdNCxN7f4u",
        "outputId": "fed18b4d-d1b9-4ac7-9fe7-433c9479905f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "auc score:  0.6464153228859112\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# your code goes here\n",
        "# using features_{train, test} and labels_{train, test} from previous part\n",
        "nb_clf.fit(X=features_train, y=labels_train)\n",
        "\n",
        "probas = nb_clf.predict_proba(features_test)\n",
        "\n",
        "#replace these fpr and tpr with the results of your roc_curve\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_true=labels_test, y_score=probas[:,1])\n",
        "\n",
        "auc_score = metrics.roc_auc_score(y_true=labels_test, y_score=probas[:,1])\n",
        "print(\"auc score: \",  auc_score)\n",
        "\n",
        "# Do not change this code! This plots the ROC curve.\n",
        "# Just replace the fpr and tpr above with the values from your roc_curve\n",
        "plt.plot([0,1],[0,1],'k--') #plot the diagonal line\n",
        "plt.plot(fpr, tpr, label='NB') #plot the ROC curve\n",
        "plt.xlabel('fpr')\n",
        "plt.ylabel('tpr')\n",
        "plt.title('ROC Curve Naive Bayes')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqnIKNX77f4u"
      },
      "source": [
        "### D. k-Nearest Neighbor (KNN) & Pipelines "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwW3ewrB7f4u"
      },
      "source": [
        "For some classification algorithms, scaling of the data is critical (like KNN, SVM, Neural Nets). For other classification algorithms, data scaling is not necessary (like Naive Bayes and Decision Trees). _Take a minute to think about why this is the case!!_ But using scaled data with an algorithm that doesn't explicitly need it to be scaled does not hurt the results of that algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVX-sGHI7f4u"
      },
      "source": [
        "Q10. The distance calculation method is central to the KNN algorithm. By default, `KNeighborsClassifier` uses  Euclidean distance as its metric (but this can be changed). Because of the distance calculations, it is critical to scale the data before running Nearest Neighbor!\n",
        "\n",
        "We discussed why dimensionality reduction may also be needed with KNN because of the curse of dimensionality. So we may want to also perform a dimensionality reduction with PCA before running KNN. PCA should only be performed on scaled data! (Remember that you can also reduce dimensionality by performing feature selection and feature engineering.) \n",
        "\n",
        "An important note about scaling data and dimensionality reduction is that they should only be performed on the **training** data, then you transform the test data into the scaled, PCA space that was found on the training data. (Refer to the concept of [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/).)\n",
        "\n",
        "So when you are doing cross-validation, the scaling and PCA needs to happen *inside of your CV loop*. This way, it is performed on the training set for the first fold, then the test set is put into that space. On the second fold, it is performed on the trainng set for the second fold, and the test set is put into that space. And so on for the remaining folds. \n",
        "\n",
        "In order to do this with scikit-learn, you must create what's called a `Pipeline` and pass that in to the cross validation. This is a very important concept for Data Mining and Machine Learning, so let's practice it here.\n",
        "\n",
        "Do the following:\n",
        "* Create a `sklearn.preprocessing.StandardScaler` object to standardize the dataset’s features (mean = 0 and variance = 1). (Do not call `fit` on it yet. Just create the `StandardScaler` object.)\n",
        "* Create a `sklearn.decomposition.PCA` object to perform PCA dimensionality reduction. (Do not call `fit` on it yet. Just create the `PCA` object.)\n",
        "* Create a `sklearn.neighbors.KNeighborsClassifier`. The number of neighbors defaults to 5 (k=5). Go ahead and change it to 7. (Do not call `fit` on it yet. Just create the `KNeighborsClassifier` object.)\n",
        "* Create a `sklearn.pipeline.Pipeline` object and set the `steps` to the scaler, the PCA, and the KNN objects that you just created. \n",
        "* Pass the `pipeline` object in to a `cross_val_score` as the estimator, along with the features and the labels, and use a 5-fold-CV. \n",
        "\n",
        "In each fold of the cross validation, the training phase will use _only_ the training data for scaling, PCA, and training the model. Then the testing phase will scale & transform the test data into the PCA space (found on the training data) and run the test data through the trained classifier, to return an accuracy measurement for each fold. Print the average accuracy across all 5 folds. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljGI2QSw7f4v",
        "outputId": "2e7fd7c9-c60f-4229-c12f-af297b240190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.62608696 0.64782609 0.64347826 0.59130435 0.5826087 ]\n",
            "average accuracy:  0.6182608695652174\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "scaler = preprocessing.StandardScaler()\n",
        "pca = decomposition.PCA()\n",
        "knn_clf = neighbors.KNeighborsClassifier(n_neighbors=7)\n",
        "pipe = pipeline.Pipeline(steps=[('scaler', scaler), ('pca', pca), ('knn_classifier', knn_clf)])\n",
        "accs = model_selection.cross_val_score(pipe, feature_vals, class_labels, cv=5)\n",
        "\n",
        "print(accs)\n",
        "print(\"average accuracy: \", np.mean(accs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1ow7eM57f4v"
      },
      "source": [
        "Q11. Another important part of KNN is choosing the best number of neighbors (tuning the hyperparameter, k). We can use nested cross validation to do this. Let's try k values from 1-25 to find the best one. \n",
        "\n",
        "We _also_ want to find the best number of dimensions to project down onto using PCA. We can use nested cross validation to do this as well. Let's try from 5-19 dimensions.\n",
        "\n",
        "* Starter code is provided to create the \"parameter grid\" to search. You will need to change this code! Where I have \"knn__n_neighbors\", this indicates that I want to tune the \"n_neighbors\" parameter in the \"knn\" part of the pipeline. When you created your pipeline above, you named the KNN part of the pipeline with a string. You should replace \"knn\" in the param_grid below with whatever you named your KNN part of the pipeline: **<replace_this>__n_neighbors.** Do the same for the PCA part of the pipeline.\n",
        "* Create a `sklearn.model_selection.GridSearchCV` and pass in the pipeline, the param_grid, and set it to a 5-fold-CV.\n",
        "* Now, on that `GridSearchCV` object, call `fit` and pass in the features and labels.\n",
        "* Show the best number of dimensions and best number of neighbors for this dataset by printing the `best_params_` from the `GridSearchCV`.\n",
        "* Also print the accuracy when using this best number of dimensions and neighbors by printing the `best_score_` from the `GridSearchCV`.\n",
        "\n",
        "Be patient, this can take some time to run. It is trying every combination of dimensions from 5-19 with every k from 1-25! A [ * ] next to the cell indicates that it is still running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bNTI7aN7f4v",
        "outputId": "d3f473bb-fa73-4e4b-ed63-ee5f7d6dc795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best params:  {'knn_classifier__n_neighbors': 23, 'pca__n_components': 14}\n",
            "best score:  0.6617391304347826\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "On the \"pca\" part of the pipeline, \n",
        "tune the n_components parameter,\n",
        "by trying the values 1-19.\n",
        "\n",
        "On the \"knn\" part of the pipeline, \n",
        "tune the n_neighbors parameter,\n",
        "by trying the values 1-25.\n",
        "'''\n",
        "param_grid = {\n",
        "    'pca__n_components': list(range(5, 19)),\n",
        "    'knn_classifier__n_neighbors': list(range(1, 25))\n",
        "}\n",
        "\n",
        "# your code goes here\n",
        "grid = model_selection.GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=5\n",
        ")\n",
        "grid.fit(feature_vals, class_labels)\n",
        "\n",
        "print(\"best params: \", grid.best_params_)\n",
        "print(\"best score: \", grid.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc6d_8147f4w"
      },
      "source": [
        "Q12. In Q11, we did not hold out a test set. The accuracy reported out is on the _validation_ set. So now we need to wrap the whole process in another cross-validation to perform a nested cross-validation and report the accuarcy of this KNN model on unseen test data. This is the official accuracy you would report on this model.\n",
        "\n",
        "You'll need to pass the `GridSearchCV` into a `cross_val_score`, just as you did with the decision tree. Use a 5-fold-CV for the outer loop. \n",
        "\n",
        "Again, be patient for this one to run. The nested cross-validation loop can take some time. It is doing what it did above in Q11 five times. A [ * ] next to the cell indicates that it is still running. (Just for comparison, mine takes about 2 mins to run and the fan revs up so it sounds like my computer is going to explode. All computers are different, so yours could take shorter or longer...)\n",
        "\n",
        "<img src=\"https://github.com/attruong00/ml-final/blob/main/model_is_training.png?raw=1\" width=\"250\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm9HU0437f4w",
        "outputId": "74908952-d51b-4992-fc60-e9eff8041751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.59130435 0.68695652 0.66521739 0.61304348 0.65217391]\n",
            "average accuracy on test data:  0.6417391304347827\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "official_accuracy = model_selection.cross_val_score(grid, feature_vals, class_labels, cv=5)\n",
        "print(official_accuracy)\n",
        "print(\"average accuracy on test data: \", np.mean(official_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAexCNdW7f4w"
      },
      "source": [
        "### E. Support Vector Machines (SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbkUKT2A7f4w"
      },
      "source": [
        "Q13. Now put it all together with an SVM. \n",
        "* Create a `pipeline` that includes scaling, PCA, and an `sklearn.svm.SVC`.\n",
        "* Create a parameter grid that tries number of dimensions from 5-19 and SVM kernels `linear`, `rbf` and `poly`.\n",
        "* Create a `GridSearchCV` for the inner CV loop. Use a 5-fold CV.\n",
        "* Run a `cross_val_predict` with a 10-fold CV for the outer loop. \n",
        "* Print out the accuracy and the classification report of using an SVM classifier on this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGsrEuvl7f4x",
        "outputId": "946ea778-be74-475b-ae7e-696a08f87394"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy:  0.72\n",
            "report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.84      0.74       539\n",
            "           1       0.81      0.62      0.70       611\n",
            "\n",
            "    accuracy                           0.72      1150\n",
            "   macro avg       0.74      0.73      0.72      1150\n",
            "weighted avg       0.74      0.72      0.72      1150\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "pipe = pipeline.Pipeline(steps=[\n",
        "    (\"scaler\", preprocessing.StandardScaler()), \n",
        "    (\"pca\", decomposition.PCA()),\n",
        "    (\"svc\", svm.SVC())\n",
        "])\n",
        "param_grid = {\n",
        "    \"pca__n_components\": list(range(5, 19)),\n",
        "    \"svc__kernel\": [\"linear\", \"rbf\", \"poly\"]\n",
        "}\n",
        "\n",
        "grid = model_selection.GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "cross_val_preds = model_selection.cross_val_predict(grid, feature_vals, class_labels,cv=10)\n",
        "accuracy = metrics.accuracy_score(y_true=class_labels, y_pred=cross_val_preds)\n",
        "print(\"accuracy: \", accuracy)\n",
        "\n",
        "report = report = metrics.classification_report(\n",
        "    y_true=class_labels,\n",
        "    y_pred=cross_val_preds,\n",
        ")\n",
        "print(\"report:\\n\", report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qosyoO5a7f4x"
      },
      "source": [
        "### F. Neural Networks (NN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cih316ki7f4x"
      },
      "source": [
        "Q14. Train a multi-layer perceptron with a single hidden layer using `sklearn.neural_network.MLPClassifier`. \n",
        "* Create a pipeline with scaling and a neural net. (No PCA on this one. But scaling is critical to neural nets.)\n",
        "* Use `GridSearchCV` with 5 fold cross validation to find the best hidden layer size and the best activation function. \n",
        "* Try values of `hidden_layer_sizes` ranging from `(30,)` to `(60,)` by increments of 10.\n",
        "* Try activation functions `logistic`, `tanh`, `relu`.\n",
        "* Wrap your `GridSearchCV` in a 5-fold `cross_val_score` and report the accuracy of your neural net.\n",
        "\n",
        "Be patient, as this can take a few minutes to run. You may get ConvergenceWarnings as it runs - that is fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "et6NXbz17f4x"
      },
      "outputs": [],
      "source": [
        "# your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe8U5AlP7f4y"
      },
      "source": [
        "### G. Ensemble Classifiers\n",
        "\n",
        "Ensemble classifiers combine the predictions of multiple base estimators to improve the accuracy of the predictions. One of the key assumptions that ensemble classifiers make is that the base estimators are built independently (so they are diverse)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_LeUwU67f4y"
      },
      "source": [
        "**Random Forests**\n",
        "\n",
        "Q15. Use `sklearn.ensemble.RandomForestClassifier` to classify the data. Scaling the data is not necessary for Decision Trees (take a minute to think about why). So, no need for a pipeline here.\n",
        "\n",
        "The default for RandomForest is to use 100 fully-grown decision trees. Let's use a `GridSearchCV` with a 5-fold CV to try various numbers of base classifiers and select the one with the best results. \n",
        "\n",
        "Try `n_estimators` of 50, 100, and 150 - this is the number of base classifiers in the ensemble. Wrap your GridSearchCV in a cross_val_score with 5-fold CV. Display the classification report. \n",
        "\n",
        "Note that this does get a higher accuracy than a single decision tree!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRq62ewJ7f4y"
      },
      "outputs": [],
      "source": [
        "# your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txjepgms7f4y"
      },
      "source": [
        "**AdaBoost**\n",
        "\n",
        "Random Forests builds its base estimators independently, using bagging. There is another method of training ensemble classifiers called boosting. Here the classifiers are trained sequentially and each time the sampling of the training set depends on the performance of previously generated models.\n",
        "\n",
        "Q16. Evaluate a `sklearn.ensemble.AdaBoostClassifier` classifier on the data. By default, `AdaBoostClassifier` uses 50 decision stumps as the base classifiers. Let's again use a `GridSearchCV` with a 5-fold CV to try various numbers of base classifiers.\n",
        "\n",
        "Try `n_estimators` of 50, 100, and 150 - this is the number of base classifiers in the ensemble. Wrap your GridSearchCV in a cross_val_score with 5-fold CV. Display the classification report.\n",
        "\n",
        "Note that even when using decision stumps as the base classifier, this gets a higher accuracy than a single decision tree!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6hDdCHz7f4y"
      },
      "outputs": [],
      "source": [
        "# your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14fIzV8K7f4y"
      },
      "source": [
        "### H. Build your final model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h423gikj7f4z"
      },
      "source": [
        "Now you have tested all kinds of classifiers on this data. Some have performed better than others. \n",
        "\n",
        "Q17. We may not want to deploy any of these models in the real world to actually diagnose patients because the accuracies are not high enough. What can we do to improve the accuracy rates? Answer as a comment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfiNUb1T7f4z"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Answer here as a comment.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83-hbXfz7f4z"
      },
      "source": [
        "Q18. Let's say we *did* get to the point where we had a model with high enough accuracy and we want to deploy that model and use it for real-world predictions.\n",
        "\n",
        "* Let's say we're going to deploy our SVM classifier.\n",
        "* We need to make one final version of this model, where we use ALL of our available data for training (we do not hold out a test set this time, so no outer cross-validation loop). \n",
        "* We need to tune the parameters of the model on the FULL dataset, so copy the code you entered for Q13, but remove the outer cross validation loop (remove `cross_val_score`). Just run the `GridSearchCV` by calling `fit` on it and passing in the full dataset. This results in the final trained model with the best parameters for the full dataset. You can print out `best_params_` to see what they are.\n",
        "* The accuracy of this model is what you assessed and reported in Q13.\n",
        "\n",
        "\n",
        "* Use the `pickle` package to save your model. We have provided the lines of code for you, just make sure your final model gets passed in to `pickle.dump()`. This will save your model to a file called finalized_model.sav in your current working directory. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DTnEiwH7f4z"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# your code goes here\n",
        "\n",
        "#replace this final_model with your final model\n",
        "final_model = None\n",
        "\n",
        "filename = 'finalized_model.sav'\n",
        "pickle.dump(final_model, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmbORF2a7f4z"
      },
      "source": [
        "Q19. Now if someone wants to use your trained, saved classifier to classify a new record, they can load the saved model and just call `predict` on it. \n",
        "* Given this new record, classify it with your saved model and print out either \"Negative for disease\" or \"Positive for disease.\"\n",
        "* Note that `predict` is expecting a list of lists (a list of records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM0oXqZ47f40"
      },
      "outputs": [],
      "source": [
        "# some time later...\n",
        "\n",
        "# use this as the new record to classify\n",
        "record = [ 0.05905386, 0.2982129, 0.68613149, 0.75078865, 0.87119216, 0.88615694,\n",
        "  0.93600623, 0.98369184, -0.47426472, -0.57642756, -0.53115361, -0.42789774,\n",
        " -0.21907738, -0.20090532, -0.21496782, -0.2080998, 0.06692373, -2.81681183,\n",
        " -0.7117194 ]\n",
        "\n",
        " \n",
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "# your code goes here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}